{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62447e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e5d91",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b86c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = pd.read_csv('./dataSet/Dataset_spine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f11319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
       "       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis',\n",
       "       'pelvic_slope', 'Direct_tilt', 'thoracic_slope', 'cervical_tilt',\n",
       "       'sacrum_angle', 'scoliosis_slope', 'class', 'Unnamed: 13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c6942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.drop([dataSet.keys()[-1]], axis=1, inplace=True)\n",
    "dataSet['target'] = dataSet['class'].apply(lambda x: 0 if x == 'Abnormal' else 1)\n",
    "#dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29322d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
       "       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis',\n",
       "       'pelvic_slope', 'Direct_tilt', 'thoracic_slope', 'cervical_tilt',\n",
       "       'sacrum_angle', 'scoliosis_slope', 'class', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.drop('class', axis=1, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce9d92ef",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74d5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "scale.fit(dataSet)\n",
    "\n",
    "dataSet = scale.transform(dataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018a907",
   "metadata": {},
   "source": [
    "## Creating the train and test arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433f0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataSet[:,0:12]\n",
    "y = dataSet[:,-1]\n",
    "y = y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43f28d4a",
   "metadata": {},
   "source": [
    "## Creating the MLP classifier #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218f2aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71153536\n",
      "Iteration 2, loss = 0.68845424\n",
      "Iteration 3, loss = 0.66856531\n",
      "Iteration 4, loss = 0.65018338\n",
      "Iteration 5, loss = 0.63342480\n",
      "Iteration 6, loss = 0.61783184\n",
      "Iteration 7, loss = 0.60353344\n",
      "Iteration 8, loss = 0.59042990\n",
      "Iteration 9, loss = 0.57857981\n",
      "Iteration 10, loss = 0.56739669\n",
      "Iteration 11, loss = 0.55756424\n",
      "Iteration 12, loss = 0.54811682\n",
      "Iteration 13, loss = 0.53973998\n",
      "Iteration 14, loss = 0.53192211\n",
      "Iteration 15, loss = 0.52487241\n",
      "Iteration 16, loss = 0.51823523\n",
      "Iteration 17, loss = 0.51244824\n",
      "Iteration 18, loss = 0.50686210\n",
      "Iteration 19, loss = 0.50172321\n",
      "Iteration 20, loss = 0.49696793\n",
      "Iteration 21, loss = 0.49237381\n",
      "Iteration 22, loss = 0.48803407\n",
      "Iteration 23, loss = 0.48388570\n",
      "Iteration 24, loss = 0.48004852\n",
      "Iteration 25, loss = 0.47620351\n",
      "Iteration 26, loss = 0.47268420\n",
      "Iteration 27, loss = 0.46931103\n",
      "Iteration 28, loss = 0.46603049\n",
      "Iteration 29, loss = 0.46290410\n",
      "Iteration 30, loss = 0.45986680\n",
      "Iteration 31, loss = 0.45694571\n",
      "Iteration 32, loss = 0.45400973\n",
      "Iteration 33, loss = 0.45107804\n",
      "Iteration 34, loss = 0.44826807\n",
      "Iteration 35, loss = 0.44536067\n",
      "Iteration 36, loss = 0.44265803\n",
      "Iteration 37, loss = 0.44000569\n",
      "Iteration 38, loss = 0.43743227\n",
      "Iteration 39, loss = 0.43474011\n",
      "Iteration 40, loss = 0.43224501\n",
      "Iteration 41, loss = 0.42974105\n",
      "Iteration 42, loss = 0.42729218\n",
      "Iteration 43, loss = 0.42497799\n",
      "Iteration 44, loss = 0.42281601\n",
      "Iteration 45, loss = 0.42075221\n",
      "Iteration 46, loss = 0.41862007\n",
      "Iteration 47, loss = 0.41666242\n",
      "Iteration 48, loss = 0.41456757\n",
      "Iteration 49, loss = 0.41255195\n",
      "Iteration 50, loss = 0.41045337\n",
      "Iteration 51, loss = 0.40841952\n",
      "Iteration 52, loss = 0.40642849\n",
      "Iteration 53, loss = 0.40450931\n",
      "Iteration 54, loss = 0.40267121\n",
      "Iteration 55, loss = 0.40077398\n",
      "Iteration 56, loss = 0.39894357\n",
      "Iteration 57, loss = 0.39715764\n",
      "Iteration 58, loss = 0.39527763\n",
      "Iteration 59, loss = 0.39350323\n",
      "Iteration 60, loss = 0.39165653\n",
      "Iteration 61, loss = 0.38986080\n",
      "Iteration 62, loss = 0.38817233\n",
      "Iteration 63, loss = 0.38653305\n",
      "Iteration 64, loss = 0.38487299\n",
      "Iteration 65, loss = 0.38333646\n",
      "Iteration 66, loss = 0.38164543\n",
      "Iteration 67, loss = 0.37988665\n",
      "Iteration 68, loss = 0.37807779\n",
      "Iteration 69, loss = 0.37603729\n",
      "Iteration 70, loss = 0.37433264\n",
      "Iteration 71, loss = 0.37251012\n",
      "Iteration 72, loss = 0.37082299\n",
      "Iteration 73, loss = 0.36924917\n",
      "Iteration 74, loss = 0.36766331\n",
      "Iteration 75, loss = 0.36610107\n",
      "Iteration 76, loss = 0.36446147\n",
      "Iteration 77, loss = 0.36282146\n",
      "Iteration 78, loss = 0.36108654\n",
      "Iteration 79, loss = 0.35950770\n",
      "Iteration 80, loss = 0.35793722\n",
      "Iteration 81, loss = 0.35646747\n",
      "Iteration 82, loss = 0.35503422\n",
      "Iteration 83, loss = 0.35345454\n",
      "Iteration 84, loss = 0.35204930\n",
      "Iteration 85, loss = 0.35052120\n",
      "Iteration 86, loss = 0.34903255\n",
      "Iteration 87, loss = 0.34778934\n",
      "Iteration 88, loss = 0.34625079\n",
      "Iteration 89, loss = 0.34473499\n",
      "Iteration 90, loss = 0.34331464\n",
      "Iteration 91, loss = 0.34184972\n",
      "Iteration 92, loss = 0.34040344\n",
      "Iteration 93, loss = 0.33899005\n",
      "Iteration 94, loss = 0.33781048\n",
      "Iteration 95, loss = 0.33668176\n",
      "Iteration 96, loss = 0.33569490\n",
      "Iteration 97, loss = 0.33473592\n",
      "Iteration 98, loss = 0.33369601\n",
      "Iteration 99, loss = 0.33275390\n",
      "Iteration 100, loss = 0.33175949\n",
      "Iteration 101, loss = 0.33087483\n",
      "Iteration 102, loss = 0.33007062\n",
      "Iteration 103, loss = 0.32876635\n",
      "Iteration 104, loss = 0.32721587\n",
      "Iteration 105, loss = 0.32563482\n",
      "Iteration 106, loss = 0.32408011\n",
      "Iteration 107, loss = 0.32247618\n",
      "Iteration 108, loss = 0.32104038\n",
      "Iteration 109, loss = 0.31955667\n",
      "Iteration 110, loss = 0.31817469\n",
      "Iteration 111, loss = 0.31679765\n",
      "Iteration 112, loss = 0.31555160\n",
      "Iteration 113, loss = 0.31430222\n",
      "Iteration 114, loss = 0.31305613\n",
      "Iteration 115, loss = 0.31172307\n",
      "Iteration 116, loss = 0.31049712\n",
      "Iteration 117, loss = 0.30923624\n",
      "Iteration 118, loss = 0.30812403\n",
      "Iteration 119, loss = 0.30691843\n",
      "Iteration 120, loss = 0.30578185\n",
      "Iteration 121, loss = 0.30467687\n",
      "Iteration 122, loss = 0.30355814\n",
      "Iteration 123, loss = 0.30253739\n",
      "Iteration 124, loss = 0.30137627\n",
      "Iteration 125, loss = 0.30026277\n",
      "Iteration 126, loss = 0.29920603\n",
      "Iteration 127, loss = 0.29834244\n",
      "Iteration 128, loss = 0.29735180\n",
      "Iteration 129, loss = 0.29641643\n",
      "Iteration 130, loss = 0.29535727\n",
      "Iteration 131, loss = 0.29426354\n",
      "Iteration 132, loss = 0.29318754\n",
      "Iteration 133, loss = 0.29204236\n",
      "Iteration 134, loss = 0.29075753\n",
      "Iteration 135, loss = 0.28969522\n",
      "Iteration 136, loss = 0.28874620\n",
      "Iteration 137, loss = 0.28773167\n",
      "Iteration 138, loss = 0.28675775\n",
      "Iteration 139, loss = 0.28575449\n",
      "Iteration 140, loss = 0.28496784\n",
      "Iteration 141, loss = 0.28395707\n",
      "Iteration 142, loss = 0.28292668\n",
      "Iteration 143, loss = 0.28205161\n",
      "Iteration 144, loss = 0.28113034\n",
      "Iteration 145, loss = 0.28024510\n",
      "Iteration 146, loss = 0.27934550\n",
      "Iteration 147, loss = 0.27860000\n",
      "Iteration 148, loss = 0.27772628\n",
      "Iteration 149, loss = 0.27687014\n",
      "Iteration 150, loss = 0.27589752\n",
      "Iteration 151, loss = 0.27496252\n",
      "Iteration 152, loss = 0.27402650\n",
      "Iteration 153, loss = 0.27324512\n",
      "Iteration 154, loss = 0.27252765\n",
      "Iteration 155, loss = 0.27171414\n",
      "Iteration 156, loss = 0.27105769\n",
      "Iteration 157, loss = 0.27040288\n",
      "Iteration 158, loss = 0.26959192\n",
      "Iteration 159, loss = 0.26872168\n",
      "Iteration 160, loss = 0.26784660\n",
      "Iteration 161, loss = 0.26699334\n",
      "Iteration 162, loss = 0.26613065\n",
      "Iteration 163, loss = 0.26522774\n",
      "Iteration 164, loss = 0.26427187\n",
      "Iteration 165, loss = 0.26328085\n",
      "Iteration 166, loss = 0.26241451\n",
      "Iteration 167, loss = 0.26149548\n",
      "Iteration 168, loss = 0.26083931\n",
      "Iteration 169, loss = 0.26011192\n",
      "Iteration 170, loss = 0.25936198\n",
      "Iteration 171, loss = 0.25876789\n",
      "Iteration 172, loss = 0.25808622\n",
      "Iteration 173, loss = 0.25743099\n",
      "Iteration 174, loss = 0.25671440\n",
      "Iteration 175, loss = 0.25612094\n",
      "Iteration 176, loss = 0.25537244\n",
      "Iteration 177, loss = 0.25470938\n",
      "Iteration 178, loss = 0.25390245\n",
      "Iteration 179, loss = 0.25322806\n",
      "Iteration 180, loss = 0.25235671\n",
      "Iteration 181, loss = 0.25155012\n",
      "Iteration 182, loss = 0.25082097\n",
      "Iteration 183, loss = 0.25001442\n",
      "Iteration 184, loss = 0.24943397\n",
      "Iteration 185, loss = 0.24890272\n",
      "Iteration 186, loss = 0.24829144\n",
      "Iteration 187, loss = 0.24749875\n",
      "Iteration 188, loss = 0.24672983\n",
      "Iteration 189, loss = 0.24585914\n",
      "Iteration 190, loss = 0.24488189\n",
      "Iteration 191, loss = 0.24396193\n",
      "Iteration 192, loss = 0.24341539\n",
      "Iteration 193, loss = 0.24285004\n",
      "Iteration 194, loss = 0.24262281\n",
      "Iteration 195, loss = 0.24231471\n",
      "Iteration 196, loss = 0.24210864\n",
      "Iteration 197, loss = 0.24176931\n",
      "Iteration 198, loss = 0.24102333\n",
      "Iteration 199, loss = 0.24018429\n",
      "Iteration 200, loss = 0.23934216\n",
      "Iteration 201, loss = 0.23840470\n",
      "Iteration 202, loss = 0.23750399\n",
      "Iteration 203, loss = 0.23670070\n",
      "Iteration 204, loss = 0.23574086\n",
      "Iteration 205, loss = 0.23512506\n",
      "Iteration 206, loss = 0.23456397\n",
      "Iteration 207, loss = 0.23407673\n",
      "Iteration 208, loss = 0.23346070\n",
      "Iteration 209, loss = 0.23291612\n",
      "Iteration 210, loss = 0.23235386\n",
      "Iteration 211, loss = 0.23181458\n",
      "Iteration 212, loss = 0.23124755\n",
      "Iteration 213, loss = 0.23063232\n",
      "Iteration 214, loss = 0.23009194\n",
      "Iteration 215, loss = 0.22935139\n",
      "Iteration 216, loss = 0.22874072\n",
      "Iteration 217, loss = 0.22813141\n",
      "Iteration 218, loss = 0.22748072\n",
      "Iteration 219, loss = 0.22678144\n",
      "Iteration 220, loss = 0.22604413\n",
      "Iteration 221, loss = 0.22532997\n",
      "Iteration 222, loss = 0.22497230\n",
      "Iteration 223, loss = 0.22432393\n",
      "Iteration 224, loss = 0.22382493\n",
      "Iteration 225, loss = 0.22344210\n",
      "Iteration 226, loss = 0.22301269\n",
      "Iteration 227, loss = 0.22271495\n",
      "Iteration 228, loss = 0.22238070\n",
      "Iteration 229, loss = 0.22207550\n",
      "Iteration 230, loss = 0.22184309\n",
      "Iteration 231, loss = 0.22182458\n",
      "Iteration 232, loss = 0.22171402\n",
      "Iteration 233, loss = 0.22141159\n",
      "Iteration 234, loss = 0.22065167\n",
      "Iteration 235, loss = 0.21976921\n",
      "Iteration 236, loss = 0.21868953\n",
      "Iteration 237, loss = 0.21792092\n",
      "Iteration 238, loss = 0.21713660\n",
      "Iteration 239, loss = 0.21666322\n",
      "Iteration 240, loss = 0.21603334\n",
      "Iteration 241, loss = 0.21529889\n",
      "Iteration 242, loss = 0.21475444\n",
      "Iteration 243, loss = 0.21420974\n",
      "Iteration 244, loss = 0.21358791\n",
      "Iteration 245, loss = 0.21301314\n",
      "Iteration 246, loss = 0.21246327\n",
      "Iteration 247, loss = 0.21171868\n",
      "Iteration 248, loss = 0.21141553\n",
      "Iteration 249, loss = 0.21102601\n",
      "Iteration 250, loss = 0.21054538\n",
      "Iteration 251, loss = 0.21030543\n",
      "Iteration 252, loss = 0.21003557\n",
      "Iteration 253, loss = 0.20973354\n",
      "Iteration 254, loss = 0.20925098\n",
      "Iteration 255, loss = 0.20867512\n",
      "Iteration 256, loss = 0.20797703\n",
      "Iteration 257, loss = 0.20725489\n",
      "Iteration 258, loss = 0.20655728\n",
      "Iteration 259, loss = 0.20606922\n",
      "Iteration 260, loss = 0.20570858\n",
      "Iteration 261, loss = 0.20564686\n",
      "Iteration 262, loss = 0.20567829\n",
      "Iteration 263, loss = 0.20577847\n",
      "Iteration 264, loss = 0.20584452\n",
      "Iteration 265, loss = 0.20604007\n",
      "Iteration 266, loss = 0.20638534\n",
      "Iteration 267, loss = 0.20670746\n",
      "Iteration 268, loss = 0.20655506\n",
      "Iteration 269, loss = 0.20583348\n",
      "Iteration 270, loss = 0.20477609\n",
      "Iteration 271, loss = 0.20359241\n",
      "Iteration 272, loss = 0.20213946\n",
      "Iteration 273, loss = 0.20086539\n",
      "Iteration 274, loss = 0.19957159\n",
      "Iteration 275, loss = 0.19858829\n",
      "Iteration 276, loss = 0.19766177\n",
      "Iteration 277, loss = 0.19744928\n",
      "Iteration 278, loss = 0.19682957\n",
      "Iteration 279, loss = 0.19640831\n",
      "Iteration 280, loss = 0.19599041\n",
      "Iteration 281, loss = 0.19563164\n",
      "Iteration 282, loss = 0.19528934\n",
      "Iteration 283, loss = 0.19499648\n",
      "Iteration 284, loss = 0.19502315\n",
      "Iteration 285, loss = 0.19445175\n",
      "Iteration 286, loss = 0.19426571\n",
      "Iteration 287, loss = 0.19363745\n",
      "Iteration 288, loss = 0.19340945\n",
      "Iteration 289, loss = 0.19305732\n",
      "Iteration 290, loss = 0.19269677\n",
      "Iteration 291, loss = 0.19247948\n",
      "Iteration 292, loss = 0.19254054\n",
      "Iteration 293, loss = 0.19258016\n",
      "Iteration 294, loss = 0.19250682\n",
      "Iteration 295, loss = 0.19187261\n",
      "Iteration 296, loss = 0.19100815\n",
      "Iteration 297, loss = 0.19010719\n",
      "Iteration 298, loss = 0.18927141\n",
      "Iteration 299, loss = 0.18890759\n",
      "Iteration 300, loss = 0.18877612\n",
      "Iteration 301, loss = 0.18864894\n",
      "Iteration 302, loss = 0.18867723\n",
      "Iteration 303, loss = 0.18865156\n",
      "Iteration 304, loss = 0.18827707\n",
      "Iteration 305, loss = 0.18773667\n",
      "Iteration 306, loss = 0.18724065\n",
      "Iteration 307, loss = 0.18675266\n",
      "Iteration 308, loss = 0.18630320\n",
      "Iteration 309, loss = 0.18576125\n",
      "Iteration 310, loss = 0.18513844\n",
      "Iteration 311, loss = 0.18431248\n",
      "Iteration 312, loss = 0.18363143\n",
      "Iteration 313, loss = 0.18289038\n",
      "Iteration 314, loss = 0.18216501\n",
      "Iteration 315, loss = 0.18165340\n",
      "Iteration 316, loss = 0.18099441\n",
      "Iteration 317, loss = 0.18042611\n",
      "Iteration 318, loss = 0.17968455\n",
      "Iteration 319, loss = 0.17935780\n",
      "Iteration 320, loss = 0.17904545\n",
      "Iteration 321, loss = 0.17877256\n",
      "Iteration 322, loss = 0.17857579\n",
      "Iteration 323, loss = 0.17829288\n",
      "Iteration 324, loss = 0.17806272\n",
      "Iteration 325, loss = 0.17775917\n",
      "Iteration 326, loss = 0.17763020\n",
      "Iteration 327, loss = 0.17738365\n",
      "Iteration 328, loss = 0.17707086\n",
      "Iteration 329, loss = 0.17708517\n",
      "Iteration 330, loss = 0.17709574\n",
      "Iteration 331, loss = 0.17723745\n",
      "Iteration 332, loss = 0.17742856\n",
      "Iteration 333, loss = 0.17735023\n",
      "Iteration 334, loss = 0.17691769\n",
      "Iteration 335, loss = 0.17603781\n",
      "Iteration 336, loss = 0.17484801\n",
      "Iteration 337, loss = 0.17363101\n",
      "Iteration 338, loss = 0.17271967\n",
      "Iteration 339, loss = 0.17204874\n",
      "Iteration 340, loss = 0.17139743\n",
      "Iteration 341, loss = 0.17090541\n",
      "Iteration 342, loss = 0.17034802\n",
      "Iteration 343, loss = 0.16978946\n",
      "Iteration 344, loss = 0.16928069\n",
      "Iteration 345, loss = 0.16856047\n",
      "Iteration 346, loss = 0.16812683\n",
      "Iteration 347, loss = 0.16794231\n",
      "Iteration 348, loss = 0.16753859\n",
      "Iteration 349, loss = 0.16720831\n",
      "Iteration 350, loss = 0.16692252\n",
      "Iteration 351, loss = 0.16661912\n",
      "Iteration 352, loss = 0.16639877\n",
      "Iteration 353, loss = 0.16615929\n",
      "Iteration 354, loss = 0.16605605\n",
      "Iteration 355, loss = 0.16604133\n",
      "Iteration 356, loss = 0.16674391\n",
      "Iteration 357, loss = 0.16768422\n",
      "Iteration 358, loss = 0.16837865\n",
      "Iteration 359, loss = 0.16843930\n",
      "Iteration 360, loss = 0.16776359\n",
      "Iteration 361, loss = 0.16688972\n",
      "Iteration 362, loss = 0.16597394\n",
      "Iteration 363, loss = 0.16493724\n",
      "Iteration 364, loss = 0.16402931\n",
      "Iteration 365, loss = 0.16305733\n",
      "Iteration 366, loss = 0.16216233\n",
      "Iteration 367, loss = 0.16168736\n",
      "Iteration 368, loss = 0.16143011\n",
      "Iteration 369, loss = 0.16098726\n",
      "Iteration 370, loss = 0.16067728\n",
      "Iteration 371, loss = 0.16026604\n",
      "Iteration 372, loss = 0.16006411\n",
      "Iteration 373, loss = 0.15972787\n",
      "Iteration 374, loss = 0.15952921\n",
      "Iteration 375, loss = 0.15940193\n",
      "Iteration 376, loss = 0.15918591\n",
      "Iteration 377, loss = 0.15909092\n",
      "Iteration 378, loss = 0.15896474\n",
      "Iteration 379, loss = 0.15874598\n",
      "Iteration 380, loss = 0.15870400\n",
      "Iteration 381, loss = 0.15821994\n",
      "Iteration 382, loss = 0.15780021\n",
      "Iteration 383, loss = 0.15724917\n",
      "Iteration 384, loss = 0.15696659\n",
      "Iteration 385, loss = 0.15656635\n",
      "Iteration 386, loss = 0.15613199\n",
      "Iteration 387, loss = 0.15539980\n",
      "Iteration 388, loss = 0.15485709\n",
      "Iteration 389, loss = 0.15457339\n",
      "Iteration 390, loss = 0.15399524\n",
      "Iteration 391, loss = 0.15349348\n",
      "Iteration 392, loss = 0.15311323\n",
      "Iteration 393, loss = 0.15271459\n",
      "Iteration 394, loss = 0.15254733\n",
      "Iteration 395, loss = 0.15205358\n",
      "Iteration 396, loss = 0.15159664\n",
      "Iteration 397, loss = 0.15108201\n",
      "Iteration 398, loss = 0.15072669\n",
      "Iteration 399, loss = 0.15025286\n",
      "Iteration 400, loss = 0.15004977\n",
      "Iteration 401, loss = 0.15021868\n",
      "Iteration 402, loss = 0.14991919\n",
      "Iteration 403, loss = 0.14972229\n",
      "Iteration 404, loss = 0.14927658\n",
      "Iteration 405, loss = 0.14886670\n",
      "Iteration 406, loss = 0.14828478\n",
      "Iteration 407, loss = 0.14767672\n",
      "Iteration 408, loss = 0.14728748\n",
      "Iteration 409, loss = 0.14690489\n",
      "Iteration 410, loss = 0.14664047\n",
      "Iteration 411, loss = 0.14629804\n",
      "Iteration 412, loss = 0.14598381\n",
      "Iteration 413, loss = 0.14555729\n",
      "Iteration 414, loss = 0.14517489\n",
      "Iteration 415, loss = 0.14488976\n",
      "Iteration 416, loss = 0.14466601\n",
      "Iteration 417, loss = 0.14446339\n",
      "Iteration 418, loss = 0.14426898\n",
      "Iteration 419, loss = 0.14415245\n",
      "Iteration 420, loss = 0.14396603\n",
      "Iteration 421, loss = 0.14370404\n",
      "Iteration 422, loss = 0.14336990\n",
      "Iteration 423, loss = 0.14301833\n",
      "Iteration 424, loss = 0.14259820\n",
      "Iteration 425, loss = 0.14226521\n",
      "Iteration 426, loss = 0.14196236\n",
      "Iteration 427, loss = 0.14164971\n",
      "Iteration 428, loss = 0.14130468\n",
      "Iteration 429, loss = 0.14101467\n",
      "Iteration 430, loss = 0.14064321\n",
      "Iteration 431, loss = 0.14034884\n",
      "Iteration 432, loss = 0.14003048\n",
      "Iteration 433, loss = 0.13980329\n",
      "Iteration 434, loss = 0.13952053\n",
      "Iteration 435, loss = 0.13931461\n",
      "Iteration 436, loss = 0.13919340\n",
      "Iteration 437, loss = 0.13915250\n",
      "Iteration 438, loss = 0.13905231\n",
      "Iteration 439, loss = 0.13891758\n",
      "Iteration 440, loss = 0.13873906\n",
      "Iteration 441, loss = 0.13857565\n",
      "Iteration 442, loss = 0.13826412\n",
      "Iteration 443, loss = 0.13776238\n",
      "Iteration 444, loss = 0.13718662\n",
      "Iteration 445, loss = 0.13645942\n",
      "Iteration 446, loss = 0.13644827\n",
      "Iteration 447, loss = 0.13603022\n",
      "Iteration 448, loss = 0.13607702\n",
      "Iteration 449, loss = 0.13628648\n",
      "Iteration 450, loss = 0.13640286\n",
      "Iteration 451, loss = 0.13638120\n",
      "Iteration 452, loss = 0.13628007\n",
      "Iteration 453, loss = 0.13629471\n",
      "Iteration 454, loss = 0.13561169\n",
      "Iteration 455, loss = 0.13466081\n",
      "Iteration 456, loss = 0.13401838\n",
      "Iteration 457, loss = 0.13337235\n",
      "Iteration 458, loss = 0.13296018\n",
      "Iteration 459, loss = 0.13268723\n",
      "Iteration 460, loss = 0.13290549\n",
      "Iteration 461, loss = 0.13279194\n",
      "Iteration 462, loss = 0.13253431\n",
      "Iteration 463, loss = 0.13229156\n",
      "Iteration 464, loss = 0.13204079\n",
      "Iteration 465, loss = 0.13177321\n",
      "Iteration 466, loss = 0.13182215\n",
      "Iteration 467, loss = 0.13182651\n",
      "Iteration 468, loss = 0.13166603\n",
      "Iteration 469, loss = 0.13106686\n",
      "Iteration 470, loss = 0.13046536\n",
      "Iteration 471, loss = 0.12984814\n",
      "Iteration 472, loss = 0.12944519\n",
      "Iteration 473, loss = 0.12919662\n",
      "Iteration 474, loss = 0.12882249\n",
      "Iteration 475, loss = 0.12854734\n",
      "Iteration 476, loss = 0.12820952\n",
      "Iteration 477, loss = 0.12776042\n",
      "Iteration 478, loss = 0.12755070\n",
      "Iteration 479, loss = 0.12718404\n",
      "Iteration 480, loss = 0.12692920\n",
      "Iteration 481, loss = 0.12670732\n",
      "Iteration 482, loss = 0.12648114\n",
      "Iteration 483, loss = 0.12616790\n",
      "Iteration 484, loss = 0.12591897\n",
      "Iteration 485, loss = 0.12570204\n",
      "Iteration 486, loss = 0.12548223\n",
      "Iteration 487, loss = 0.12525718\n",
      "Iteration 488, loss = 0.12505087\n",
      "Iteration 489, loss = 0.12482915\n",
      "Iteration 490, loss = 0.12453351\n",
      "Iteration 491, loss = 0.12407718\n",
      "Iteration 492, loss = 0.12369257\n",
      "Iteration 493, loss = 0.12361784\n",
      "Iteration 494, loss = 0.12376764\n",
      "Iteration 495, loss = 0.12369690\n",
      "Iteration 496, loss = 0.12333893\n",
      "Iteration 497, loss = 0.12291691\n",
      "Iteration 498, loss = 0.12252632\n",
      "Iteration 499, loss = 0.12213301\n",
      "Iteration 500, loss = 0.12183545\n",
      "Iteration 501, loss = 0.12149614\n",
      "Iteration 502, loss = 0.12121025\n",
      "Iteration 503, loss = 0.12094728\n",
      "Iteration 504, loss = 0.12065065\n",
      "Iteration 505, loss = 0.12050754\n",
      "Iteration 506, loss = 0.12049691\n",
      "Iteration 507, loss = 0.12101536\n",
      "Iteration 508, loss = 0.12129709\n",
      "Iteration 509, loss = 0.12148320\n",
      "Iteration 510, loss = 0.12137264\n",
      "Iteration 511, loss = 0.12096839\n",
      "Iteration 512, loss = 0.12025226\n",
      "Iteration 513, loss = 0.11972689\n",
      "Iteration 514, loss = 0.11887525\n",
      "Iteration 515, loss = 0.11835258\n",
      "Iteration 516, loss = 0.11794006\n",
      "Iteration 517, loss = 0.11789117\n",
      "Iteration 518, loss = 0.11776944\n",
      "Iteration 519, loss = 0.11790751\n",
      "Iteration 520, loss = 0.11811608\n",
      "Iteration 521, loss = 0.11825393\n",
      "Iteration 522, loss = 0.11813840\n",
      "Iteration 523, loss = 0.11800333\n",
      "Iteration 524, loss = 0.11757736\n",
      "Iteration 525, loss = 0.11698632\n",
      "Iteration 526, loss = 0.11621390\n",
      "Iteration 527, loss = 0.11582704\n",
      "Iteration 528, loss = 0.11575453\n",
      "Iteration 529, loss = 0.11549166\n",
      "Iteration 530, loss = 0.11551653\n",
      "Iteration 531, loss = 0.11536262\n",
      "Iteration 532, loss = 0.11508692\n",
      "Iteration 533, loss = 0.11479681\n",
      "Iteration 534, loss = 0.11456436\n",
      "Iteration 535, loss = 0.11431311\n",
      "Iteration 536, loss = 0.11419656\n",
      "Iteration 537, loss = 0.11401800\n",
      "Iteration 538, loss = 0.11376272\n",
      "Iteration 539, loss = 0.11339494\n",
      "Iteration 540, loss = 0.11292093\n",
      "Iteration 541, loss = 0.11257521\n",
      "Iteration 542, loss = 0.11227647\n",
      "Iteration 543, loss = 0.11194061\n",
      "Iteration 544, loss = 0.11174529\n",
      "Iteration 545, loss = 0.11152823\n",
      "Iteration 546, loss = 0.11128254\n",
      "Iteration 547, loss = 0.11118976\n",
      "Iteration 548, loss = 0.11087279\n",
      "Iteration 549, loss = 0.11068501\n",
      "Iteration 550, loss = 0.11054320\n",
      "Iteration 551, loss = 0.11043181\n",
      "Iteration 552, loss = 0.11024052\n",
      "Iteration 553, loss = 0.11008190\n",
      "Iteration 554, loss = 0.10972904\n",
      "Iteration 555, loss = 0.10936463\n",
      "Iteration 556, loss = 0.10909679\n",
      "Iteration 557, loss = 0.10880513\n",
      "Iteration 558, loss = 0.10868208\n",
      "Iteration 559, loss = 0.10846337\n",
      "Iteration 560, loss = 0.10816224\n",
      "Iteration 561, loss = 0.10785508\n",
      "Iteration 562, loss = 0.10753703\n",
      "Iteration 563, loss = 0.10741478\n",
      "Iteration 564, loss = 0.10716541\n",
      "Iteration 565, loss = 0.10700931\n",
      "Iteration 566, loss = 0.10682819\n",
      "Iteration 567, loss = 0.10717306\n",
      "Iteration 568, loss = 0.10700017\n",
      "Iteration 569, loss = 0.10680202\n",
      "Iteration 570, loss = 0.10673115\n",
      "Iteration 571, loss = 0.10651254\n",
      "Iteration 572, loss = 0.10638779\n",
      "Iteration 573, loss = 0.10617511\n",
      "Iteration 574, loss = 0.10584188\n",
      "Iteration 575, loss = 0.10549743\n",
      "Iteration 576, loss = 0.10527358\n",
      "Iteration 577, loss = 0.10512188\n",
      "Iteration 578, loss = 0.10534494\n",
      "Iteration 579, loss = 0.10491708\n",
      "Iteration 580, loss = 0.10422390\n",
      "Iteration 581, loss = 0.10378165\n",
      "Iteration 582, loss = 0.10342753\n",
      "Iteration 583, loss = 0.10309206\n",
      "Iteration 584, loss = 0.10308507\n",
      "Iteration 585, loss = 0.10288609\n",
      "Iteration 586, loss = 0.10249623\n",
      "Iteration 587, loss = 0.10217541\n",
      "Iteration 588, loss = 0.10181231\n",
      "Iteration 589, loss = 0.10155495\n",
      "Iteration 590, loss = 0.10128423\n",
      "Iteration 591, loss = 0.10109668\n",
      "Iteration 592, loss = 0.10097419\n",
      "Iteration 593, loss = 0.10075954\n",
      "Iteration 594, loss = 0.10048095\n",
      "Iteration 595, loss = 0.10023983\n",
      "Iteration 596, loss = 0.10000971\n",
      "Iteration 597, loss = 0.09975959\n",
      "Iteration 598, loss = 0.09957796\n",
      "Iteration 599, loss = 0.09976831\n",
      "Iteration 600, loss = 0.09970175\n",
      "Iteration 601, loss = 0.09965216\n",
      "Iteration 602, loss = 0.09966304\n",
      "Iteration 603, loss = 0.09972766\n",
      "Iteration 604, loss = 0.09940698\n",
      "Iteration 605, loss = 0.09890879\n",
      "Iteration 606, loss = 0.09853144\n",
      "Iteration 607, loss = 0.09805636\n",
      "Iteration 608, loss = 0.09763177\n",
      "Iteration 609, loss = 0.09726498\n",
      "Iteration 610, loss = 0.09688455\n",
      "Iteration 611, loss = 0.09675796\n",
      "Iteration 612, loss = 0.09648647\n",
      "Iteration 613, loss = 0.09643182\n",
      "Iteration 614, loss = 0.09620926\n",
      "Iteration 615, loss = 0.09599080\n",
      "Iteration 616, loss = 0.09574467\n",
      "Iteration 617, loss = 0.09549460\n",
      "Iteration 618, loss = 0.09531677\n",
      "Iteration 619, loss = 0.09506674\n",
      "Iteration 620, loss = 0.09495824\n",
      "Iteration 621, loss = 0.09499189\n",
      "Iteration 622, loss = 0.09495393\n",
      "Iteration 623, loss = 0.09492299\n",
      "Iteration 624, loss = 0.09495001\n",
      "Iteration 625, loss = 0.09455065\n",
      "Iteration 626, loss = 0.09392580\n",
      "Iteration 627, loss = 0.09354591\n",
      "Iteration 628, loss = 0.09326670\n",
      "Iteration 629, loss = 0.09305444\n",
      "Iteration 630, loss = 0.09292609\n",
      "Iteration 631, loss = 0.09279275\n",
      "Iteration 632, loss = 0.09264573\n",
      "Iteration 633, loss = 0.09253975\n",
      "Iteration 634, loss = 0.09274181\n",
      "Iteration 635, loss = 0.09348410\n",
      "Iteration 636, loss = 0.09378204\n",
      "Iteration 637, loss = 0.09415179\n",
      "Iteration 638, loss = 0.09432032\n",
      "Iteration 639, loss = 0.09407714\n",
      "Iteration 640, loss = 0.09353449\n",
      "Iteration 641, loss = 0.09297602\n",
      "Iteration 642, loss = 0.09245779\n",
      "Iteration 643, loss = 0.09193113\n",
      "Iteration 644, loss = 0.09127966\n",
      "Iteration 645, loss = 0.09053291\n",
      "Iteration 646, loss = 0.08992127\n",
      "Iteration 647, loss = 0.08947151\n",
      "Iteration 648, loss = 0.08919947\n",
      "Iteration 649, loss = 0.08913765\n",
      "Iteration 650, loss = 0.08904070\n",
      "Iteration 651, loss = 0.08893255\n",
      "Iteration 652, loss = 0.08863551\n",
      "Iteration 653, loss = 0.08830951\n",
      "Iteration 654, loss = 0.08804835\n",
      "Iteration 655, loss = 0.08769287\n",
      "Iteration 656, loss = 0.08739547\n",
      "Iteration 657, loss = 0.08728491\n",
      "Iteration 658, loss = 0.08723826\n",
      "Iteration 659, loss = 0.08725690\n",
      "Iteration 660, loss = 0.08740260\n",
      "Iteration 661, loss = 0.08747598\n",
      "Iteration 662, loss = 0.08760528\n",
      "Iteration 663, loss = 0.08745819\n",
      "Iteration 664, loss = 0.08717154\n",
      "Iteration 665, loss = 0.08671518\n",
      "Iteration 666, loss = 0.08606218\n",
      "Iteration 667, loss = 0.08568498\n",
      "Iteration 668, loss = 0.08537704\n",
      "Iteration 669, loss = 0.08514183\n",
      "Iteration 670, loss = 0.08490211\n",
      "Iteration 671, loss = 0.08467101\n",
      "Iteration 672, loss = 0.08446131\n",
      "Iteration 673, loss = 0.08432512\n",
      "Iteration 674, loss = 0.08422806\n",
      "Iteration 675, loss = 0.08422263\n",
      "Iteration 676, loss = 0.08421402\n",
      "Iteration 677, loss = 0.08415713\n",
      "Iteration 678, loss = 0.08416239\n",
      "Iteration 679, loss = 0.08390385\n",
      "Iteration 680, loss = 0.08387082\n",
      "Iteration 681, loss = 0.08361489\n",
      "Iteration 682, loss = 0.08359700\n",
      "Iteration 683, loss = 0.08379650\n",
      "Iteration 684, loss = 0.08365676\n",
      "Iteration 685, loss = 0.08335861\n",
      "Iteration 686, loss = 0.08311072\n",
      "Iteration 687, loss = 0.08283845\n",
      "Iteration 688, loss = 0.08267106\n",
      "Iteration 689, loss = 0.08236916\n",
      "Iteration 690, loss = 0.08223357\n",
      "Iteration 691, loss = 0.08205656\n",
      "Iteration 692, loss = 0.08183292\n",
      "Iteration 693, loss = 0.08153102\n",
      "Iteration 694, loss = 0.08127929\n",
      "Iteration 695, loss = 0.08097625\n",
      "Iteration 696, loss = 0.08074981\n",
      "Iteration 697, loss = 0.08053348\n",
      "Iteration 698, loss = 0.08039376\n",
      "Iteration 699, loss = 0.08016420\n",
      "Iteration 700, loss = 0.08011098\n",
      "Iteration 701, loss = 0.08045896\n",
      "Iteration 702, loss = 0.08043635\n",
      "Iteration 703, loss = 0.08042163\n",
      "Iteration 704, loss = 0.08024141\n",
      "Iteration 705, loss = 0.07969607\n",
      "Iteration 706, loss = 0.07909729\n",
      "Iteration 707, loss = 0.07863031\n",
      "Iteration 708, loss = 0.07819789\n",
      "Iteration 709, loss = 0.07806726\n",
      "Iteration 710, loss = 0.07813540\n",
      "Iteration 711, loss = 0.07784680\n",
      "Iteration 712, loss = 0.07761068\n",
      "Iteration 713, loss = 0.07735950\n",
      "Iteration 714, loss = 0.07720630\n",
      "Iteration 715, loss = 0.07701829\n",
      "Iteration 716, loss = 0.07683318\n",
      "Iteration 717, loss = 0.07673134\n",
      "Iteration 718, loss = 0.07670179\n",
      "Iteration 719, loss = 0.07661093\n",
      "Iteration 720, loss = 0.07658353\n",
      "Iteration 721, loss = 0.07636135\n",
      "Iteration 722, loss = 0.07623705\n",
      "Iteration 723, loss = 0.07610766\n",
      "Iteration 724, loss = 0.07573780\n",
      "Iteration 725, loss = 0.07558445\n",
      "Iteration 726, loss = 0.07536871\n",
      "Iteration 727, loss = 0.07515372\n",
      "Iteration 728, loss = 0.07493512\n",
      "Iteration 729, loss = 0.07478480\n",
      "Iteration 730, loss = 0.07477556\n",
      "Iteration 731, loss = 0.07457714\n",
      "Iteration 732, loss = 0.07415938\n",
      "Iteration 733, loss = 0.07386052\n",
      "Iteration 734, loss = 0.07369031\n",
      "Iteration 735, loss = 0.07363408\n",
      "Iteration 736, loss = 0.07352390\n",
      "Iteration 737, loss = 0.07344461\n",
      "Iteration 738, loss = 0.07325387\n",
      "Iteration 739, loss = 0.07310073\n",
      "Iteration 740, loss = 0.07304456\n",
      "Iteration 741, loss = 0.07294030\n",
      "Iteration 742, loss = 0.07277889\n",
      "Iteration 743, loss = 0.07268690\n",
      "Iteration 744, loss = 0.07254435\n",
      "Iteration 745, loss = 0.07242283\n",
      "Iteration 746, loss = 0.07220287\n",
      "Iteration 747, loss = 0.07205615\n",
      "Iteration 748, loss = 0.07189405\n",
      "Iteration 749, loss = 0.07176490\n",
      "Iteration 750, loss = 0.07163429\n",
      "Iteration 751, loss = 0.07149004\n",
      "Iteration 752, loss = 0.07136615\n",
      "Iteration 753, loss = 0.07121224\n",
      "Iteration 754, loss = 0.07103314\n",
      "Iteration 755, loss = 0.07086844\n",
      "Iteration 756, loss = 0.07076031\n",
      "Iteration 757, loss = 0.07051051\n",
      "Iteration 758, loss = 0.07032617\n",
      "Iteration 759, loss = 0.07015509\n",
      "Iteration 760, loss = 0.06998264\n",
      "Iteration 761, loss = 0.06990039\n",
      "Iteration 762, loss = 0.06985790\n",
      "Iteration 763, loss = 0.06962566\n",
      "Iteration 764, loss = 0.06945048\n",
      "Iteration 765, loss = 0.06926883\n",
      "Iteration 766, loss = 0.06928944\n",
      "Iteration 767, loss = 0.06903262\n",
      "Iteration 768, loss = 0.06876541\n",
      "Iteration 769, loss = 0.06853677\n",
      "Iteration 770, loss = 0.06835166\n",
      "Iteration 771, loss = 0.06818705\n",
      "Iteration 772, loss = 0.06806470\n",
      "Iteration 773, loss = 0.06795346\n",
      "Iteration 774, loss = 0.06786869\n",
      "Iteration 775, loss = 0.06772106\n",
      "Iteration 776, loss = 0.06751838\n",
      "Iteration 777, loss = 0.06743603\n",
      "Iteration 778, loss = 0.06733292\n",
      "Iteration 779, loss = 0.06719518\n",
      "Iteration 780, loss = 0.06697036\n",
      "Iteration 781, loss = 0.06676769\n",
      "Iteration 782, loss = 0.06650290\n",
      "Iteration 783, loss = 0.06653134\n",
      "Iteration 784, loss = 0.06681880\n",
      "Iteration 785, loss = 0.06680739\n",
      "Iteration 786, loss = 0.06671545\n",
      "Iteration 787, loss = 0.06662752\n",
      "Iteration 788, loss = 0.06642359\n",
      "Iteration 789, loss = 0.06608259\n",
      "Iteration 790, loss = 0.06568180\n",
      "Iteration 791, loss = 0.06521134\n",
      "Iteration 792, loss = 0.06483217\n",
      "Iteration 793, loss = 0.06481017\n",
      "Iteration 794, loss = 0.06488022\n",
      "Iteration 795, loss = 0.06537815\n",
      "Iteration 796, loss = 0.06639945\n",
      "Iteration 797, loss = 0.06675208\n",
      "Iteration 798, loss = 0.06644466\n",
      "Iteration 799, loss = 0.06560023\n",
      "Iteration 800, loss = 0.06461356\n",
      "Iteration 801, loss = 0.06403819\n",
      "Iteration 802, loss = 0.06354481\n",
      "Iteration 803, loss = 0.06371715\n",
      "Iteration 804, loss = 0.06415375\n",
      "Iteration 805, loss = 0.06452510\n",
      "Iteration 806, loss = 0.06490562\n",
      "Iteration 807, loss = 0.06487743\n",
      "Iteration 808, loss = 0.06465144\n",
      "Iteration 809, loss = 0.06440218\n",
      "Iteration 810, loss = 0.06416713\n",
      "Iteration 811, loss = 0.06394650\n",
      "Iteration 812, loss = 0.06382614\n",
      "Iteration 813, loss = 0.06359513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier = MLPClassifier(max_iter=1000,activation='relu', verbose=True, random_state=1)\n",
    "mlpClassifier.fit(X_train, y_train)\n",
    "y_pred = mlpClassifier.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0fc32ea",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7c1744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56989247311827"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze(targets,preds):\n",
    "    hit = [True if preds[i] == targets[i] else False for i in range(len(targets))]\n",
    "    hitRate = (hit.count(True)/len(hit))*100\n",
    "    return hitRate\n",
    "\n",
    "analyze(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741acce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde349ca190>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmaklEQVR4nO3deXxW5Zn/8c+VJ/sOSQgQlgCyCIqgiGvdtWBbsLtoO9W249SpbZ12OtW2P6e1s9R22tpWu9iOHacdRWs3Wm211t0qEizIDiFsCYQEyEZC9uv3x3PAEANESDjP8+T7fr2eF+fc53DORXL45s59NnN3REQk/iWFXYCIiAwMBbqISIJQoIuIJAgFuohIglCgi4gkCAW6iEiCUKCLiCQIBbrEJDPbamZXhLTvuWb2uJnVm9k+M3vVzG4MoxaRt0KBLtKDmZ0HPA08B5wCFAA3A/OPc3uRgatO5OgU6BJXzCzNzO42s53B524zSwuWFZrZH3r0rF8ws6Rg2RfMrMrMmsxsg5ldfoRdfBN4wN3vcvc9HrXc3T8QbOcGM3uxV01uZqcE0/9jZj8MevjNwD+bWXXPYDezd5vZ68F0kpndZmabzWyvmT1iZsMH/AsnQ4ICXeLNl4BzgVnAGcBc4MvBss8BlUARUAx8EXAzmwrcApzt7jnA24GtvTdsZpnAecCjJ1jjdcC/AznAd4Fm4LJeyx8Mpj8FXANcDIwG6oB7T3D/MkQp0CXeXA/c6e417l4LfBX4cLCsAxgFjHf3Dnd/waMPK+oC0oDpZpbi7lvdfXMf2x5G9P/ErhOs8Xfu/pK7d7t7K/AQsAjAzHKAq4M2gE8AX3L3SndvA74CvM/Mkk+wBhmCFOgSb0YD23rMbwvaIDpcUg48aWYVZnYbgLuXA7cSDcsaM1tsZqN5szqgm+gPhROxo9f8g8B7gqGh9wCvufvBf8N44DfBMFE9sI7oD6DiE6xBhiAFusSbnURD8KBxQRvu3uTun3P3icAC4LMHx8rd/UF3vzD4uw7c1XvD7t4CvAy89yj7bwYyD86Y2cg+1jnsEabuvpboD575HD7cAtHwn+/u+T0+6e5edZQaRPqkQJdYlmJm6T0+yUSHKr5sZkVmVgjcAfwCwMzeaWanmJkBDUR7ut1mNtXMLgt6yK3AAaI98b78C3CDmX3ezAqC7Z5hZouD5SuBGWY2y8zSifb6++NB4DPARcAve7T/CPh3Mxsf7KvIzBb2c5sih1GgSyx7nGj4Hvx8Bfg3oAx4HVgFvBa0AUwGngL2E+1p/8DdnyE6fv51YA9QDYwAbu9rh+7+V6InMC8DKsxsH3BfUAvuvhG4M9jPJuDFvrbTh4eInvh82t339Gj/LrCE6DBRE/AKcE4/tylyGNMLLkREEoN66CIiCUKBLiKSIBToIiIJQoEuIpIgQrsbrbCw0EtLS8PavYhIXFq+fPkedy/qa1logV5aWkpZWVlYuxcRiUtmtu1IyzTkIiKSIBToIiIJQoEuIpIgFOgiIglCgS4ikiAU6CIiCUKBLiKSIOIu0Jdt3cc3n1hPV7eeEiki0lPcBfqK7fXc+8xmmts7wy5FRCSm9CvQzWyemW0ws/KD72nstfw7ZrYi+GwM3o04KHLSoze3NrUq0EVEejrmrf9mFgHuBa4EKoFlZrYkeE8iAO7+Tz3W/xQwexBqBSAnPQWA/Qp0EZHD9KeHPhcod/cKd28HFgNHe+fhIqKv2xoU2Yd66B2DtQsRkbjUn0AvIfpm8oMqg7Y3CV50OwF4+gjLbzKzMjMrq62tfau1AhpyERE5koE+KXot8Ki7d/W10N3vc/c57j6nqKjPpz8eU05aEOhtCnQRkZ76E+hVwNge82OCtr5cyyAOt8AbY+gachEROVx/An0ZMNnMJphZKtHQXtJ7JTObBgwDXh7YEg+nIRcRkb4dM9DdvRO4BXgCWAc84u5rzOxOM1vQY9VrgcXuPqh3/GSmRkgyXeUiItJbv95Y5O6PA4/3aruj1/xXBq6sIzMzstOSNeQiItJL3N0pCtFxdA25iIgcLk4DPVlXuYiI9BK/ga4hFxGRw8RloOemp9B4QD10EZGe4jLQ8zJTaDigHrqISE9xGej5GanUtbSHXYaISEyJy0AflplCS3sXbZ19PmFARGRIistAz8+M3v7f0KJhFxGRg+I00FMBqNc4uojIIXEZ6MOCQK9r1ji6iMhBcRnoB4dc1EMXEXlDfAe6rnQRETkkTgM9GEPXSVERkUPiMtCzUiOkRIw6BbqIyCFxGehmRn5mqoZcRER6iMtAB8jPSNGQi4hID3Eb6MMydfu/iEhPcRvoekCXiMjh4jbQh2WmqIcuItJD3AZ6QXYa+5rb6e4e1HdSi4jEjbgN9KLsNDq6XMMuIiKBfgW6mc0zsw1mVm5mtx1hnQ+Y2VozW2NmDw5smW9WlJMGQO3+tsHelYhIXEg+1gpmFgHuBa4EKoFlZrbE3df2WGcycDtwgbvXmdmIwSr4oEOB3tTGlOKcwd6diEjM608PfS5Q7u4V7t4OLAYW9lrn74F73b0OwN1rBrbMNxsRBHpNU+tg70pEJC70J9BLgB095iuDtp6mAFPM7CUze8XM5vW1ITO7yczKzKystrb2+CoO9Oyhi4jIwJ0UTQYmA5cAi4CfmFl+75Xc/T53n+Puc4qKik5oh9lpyaSnJCnQRUQC/Qn0KmBsj/kxQVtPlcASd+9w9y3ARqIBP2jMjKKcNAW6iEigP4G+DJhsZhPMLBW4FljSa53fEu2dY2aFRIdgKgauzL4VZafpKhcRkcAxA93dO4FbgCeAdcAj7r7GzO40swXBak8Ae81sLfAM8Hl33ztYRR+kHrqIyBuOedkigLs/Djzeq+2OHtMOfDb4nDRFOWm8umXfydyliEjMits7RQGKstOpa+mgvbM77FJEREIX14E+Ki8dgOoGXYsuIhLXgV4yLAOAyvqWkCsREQlffAd6fjTQq+oOhFyJiEj44jrQR+VHh1yq6hXoIiJxHehpyRFG5KSphy4iQpwHOkTH0dVDFxFJhEDPV6CLiEAiBPqwDHbVt+pVdCIy5MV/oOdn0N7VrWe6iMiQF/eBPm54JgDb9upadBEZ2uI+0CcVZQNQXrM/5EpERMIV94Fekp9BekoSm2sV6CIytMV9oCclGRMLs9VDF5EhL+4DHeCUEdnqoYvIkJcQgT6pKJuq+gMcaO8KuxQRkdAkRKCfMiIbd6jYo166iAxdCRHoU4qjV7qs29UUciUiIuFJiECfWJRNZmqEVZX1YZciIhKahAj0SJJxWkker1c1hF2KiEhoEiLQAWaW5LF2ZyMdXXq/qIgMTYkT6GPzaevsZkO1xtFFZGjqV6Cb2Twz22Bm5WZ2Wx/LbzCzWjNbEXw+PvClHt3MkjwAVmocXUSGqGMGuplFgHuB+cB0YJGZTe9j1YfdfVbw+ekA13lM4wsyKcpJY2nFvpO9axGRmNCfHvpcoNzdK9y9HVgMLBzcst46M+O8iQW8XLEXdz0bXUSGnv4Eegmwo8d8ZdDW23vN7HUze9TMxva1ITO7yczKzKystrb2OMo9uvMmFVDb1Mbm2uYB37aISKwbqJOivwdK3X0m8Gfggb5Wcvf73H2Ou88pKioaoF2/4byJBQC8XLF3wLctIhLr+hPoVUDPHveYoO0Qd9/r7gdfGfRT4KyBKe+tGV+QSUl+Bs9tGPjev4hIrOtPoC8DJpvZBDNLBa4FlvRcwcxG9ZhdAKwbuBL7z8y4cnoxL5bX6kFdIjLkHDPQ3b0TuAV4gmhQP+Lua8zsTjNbEKz2aTNbY2YrgU8DNwxWwcdyxanFtHZ081L5nrBKEBEJRXJ/VnL3x4HHe7Xd0WP6duD2gS3t+MydMJyctGSeWrebK6YXh12OiMhJkzB3ih6UmpzERVOLeGpdDd3dunxRRIaOhAt0gHkzRrJnf5uudhGRISUhA/3K6cXkpCfzq9cqwy5FROSkSchAT0+J8M6Zo/jT6mqa2zrDLkdE5KRIyEAHeM+ZY2hp7+KPq6vDLkVE5KRI2ECfM34YEwqzeHDptrBLERE5KRI20M2MD507nte217NabzISkSEgYQMd4H1njSEjJcL/vrw17FJERAZdQgd6XkYK18wu4XcrdlLX3B52OSIigyqhAx3g784bT1tnN79cvuPYK4uIxLGED/RTR+Uyd8JwHvjrNr1AWkQSWsIHOsAnLp5IVf0BfrdiZ9iliIgMmiER6JdOHcG0kTn84NlyuvR8FxFJUEMi0M2MT156ChW1zTy5RjcaiUhiGhKBDnD16aMoLcjk3mfL9RJpEUlIQybQI0nGzZdMYnVVI89u1CvqRCTxDJlAB3j37DGMHZ7B1x9fT6eueBGRBDOkAj01OYnb55/Kht1NPFym69JFJLEMqUAHmH/aSOaWDudbT25kn+4eFZEEMuQC3cz46sIZNB7o4N8eWxt2OSIiA2bIBTpE7x79xMWT+PVrVTyvE6QikiD6FehmNs/MNphZuZnddpT13mtmbmZzBq7EwXHLZacwsSiLL/5mFS3tequRiMS/Ywa6mUWAe4H5wHRgkZlN72O9HOAzwNKBLnIwpKdE+Pp7ZlJZd4BvP7kx7HJERE5Yf3roc4Fyd69w93ZgMbCwj/W+BtwFtA5gfYNq7oThXH/OOO5/aQsrd9SHXY6IyAnpT6CXAD2v8asM2g4xszOBse7+2NE2ZGY3mVmZmZXV1sbG2PUX5k+jKCeNL/zqdT2NUUTi2gmfFDWzJODbwOeOta673+fuc9x9TlFR0YnuekDkpqfwtYWnsb66ifuerwi7HBGR49afQK8CxvaYHxO0HZQDnAY8a2ZbgXOBJfFwYvSgq2aM5OrTR/Ldv2xic+3+sMsRETku/Qn0ZcBkM5tgZqnAtcCSgwvdvcHdC9291N1LgVeABe5eNigVD5KvLJhBenISn314Be2dGnoRkfhzzEB3907gFuAJYB3wiLuvMbM7zWzBYBd4sozISeeu985kZWUD33xifdjliIi8Zcn9WcndHwce79V2xxHWveTEywrH/NNH8aFzx/GTF7Zw/qRCLp02IuySRET6bUjeKXo0X37HdKaNzOFzv1zJ7sa4uQJTRESB3lt6SoR7rjuTA+1d3Lp4hV5ZJyJxQ4Heh1NGZHPnwhm8XLGXe58pD7scEZF+UaAfwfvOGsM1s0Zz91MbeXXLvrDLERE5JgX6EZgZ//bu0xk3PJPPLP4bdXp2uojEOAX6UWSnJXPPdWeyZ38bn390pV4uLSIxTYF+DKeV5HH7/FN5al0NP3tpa9jliIgckQK9H268oJQrTh3Bf/5xHS9v3ht2OSIifVKg94OZ8a0PzKK0IIt/+HkZ5TVNYZckIvImCvR+ystI4f4bziY1OcJH7l9GTZNuOhKR2KJAfwvGDs/k/hvmsK+5nY8/UKZX14lITFGgv0Uzx+Tz/UWzWV3VwKcf+pvuJBWRmKFAPw5XTC/mX981g6fW1XDn79fockYRiQn9etqivNlHzi9lx74WfvriFjq7nX991wxSk/XzUUTCo0A/AV+8+lQiEePHz1WwcXcTP7j+LIpy0sIuS0SGKHUpT0BSknH7/FP53qLZrKpqYME9L7JlT3PYZYnIEKVAHwALzhjNo584n7bObj7+wDIaWzvCLklEhiAF+gA5rSSPH1x/Jtv2tujqFxEJhQJ9AJ07sYCvLpzBsxtq+caf9F5SETm5dFJ0gF1/znjW72rix89XMCI3nY9dOCHskkRkiFCgD4I73jWdPfvb+Nof1tLZ1c0/XDwp7JJEZAjQkMsgSIkk8b1Fs3nnzFH85x/X6zV2InJS9CvQzWyemW0ws3Izu62P5Z8ws1VmtsLMXjSz6QNfanxJiSRx9wdncc2s0XzziQ3c/dRG3VEqIoPqmEMuZhYB7gWuBCqBZWa2xN3X9ljtQXf/UbD+AuDbwLxBqDeuJEeS+NYHZpEcSeLupzbRcKCDL79jOpEkC7s0EUlA/RlDnwuUu3sFgJktBhYChwLd3Rt7rJ8FqCsaiCQZ33jvTHLTU7j/pS1s39vC9xbNJitNpy9EZGD1Z8ilBNjRY74yaDuMmX3SzDYD3wA+3deGzOwmMyszs7La2trjqTcuJSUZd7xrOl9bOINnNtTw/h+9zK6GA2GXJSIJZsBOirr7ve4+CfgC8OUjrHOfu89x9zlFRUUDteu48eHzSrn/hrPZvq+Fa+59idVVDWGXJCIJpD+BXgWM7TE/Jmg7ksXANSdQU0K7ZOoIfnXz+UTMWPSTV3hmQ03YJYlIguhPoC8DJpvZBDNLBa4FlvRcwcwm95h9B7Bp4EpMPFNH5vDLm8+nJD+DG3+2jLv+tJ7Oru6wyxKROHfMQHf3TuAW4AlgHfCIu68xszuDK1oAbjGzNWa2Avgs8JHBKjhRlORn8NtPXsCiueP44bObWfSTVzSuLiInxMK6NnrOnDleVlYWyr5jze9WVPHFX68iNTmJb39wFpdOHRF2SSISo8xsubvP6WuZ7hSNAQtnlfD7T11IcW46N/5sGf/6u9UcaO8KuywRiTMK9BgxsSib337yAj56wQQeeHkbV3/vBZZvqwu7LBGJIwr0GJKeEuGOd03nob8/l/bObt7/o79y15/W09ap3rqIHJsCPQadN6mAP936Nt5/1lh++OxmFt7zEmt26pp1ETk6BXqMyklP4a73zeT+G+awt7mda+59iXue3qTLG0XkiBToMe6yacU8eetFzDttFP/15Ebe88O/snzbvrDLEpEYpECPA8OyUvn+otncc91sqhtaee8PX+bmXyynonZ/2KWJSAxRoMeRd84czbOfv4Rbr5jMcxtrueo7z/ONP63XJY4iAijQ405majK3XjGFZz9/CQtnlfCDZzdz5Xee40+rd+kFGiJDnAI9To3ISedbHziDxTedS0ZKhE/84jU++ONX2LGvJezSRCQkuvU/AXR2dfPL5ZX8x2Pr6HLnurnjmH/6KGaPzSdJb0cSSShHu/VfgZ5AKuta+M/H1/Pk2mo6upyJRVnc9d6ZnF06POzSRGSAKNCHmMbWDp5au5tv/3kjVfUH+Mh5pXz+7VP12juRBHC0QNf/8ASUm57Ce84cw9tnjOSbT2zggZe38viqXdxwQSnXzx1PXmZK2CWKyCBQD30IKNu6j7uf2sSL5XvISUvmxgsn8LELJ5CXoWAXiTcachEA1u5s5PtPb+KPq6vJSU/m4xdO5KMXlpKTrmAXiRcKdDnM2p2N3P3URp5cu5u8jBSuPn0UV5w6gumjcynKTiM5oqtZRWKVAl36tKqygfteqOAv63bTEtxtWpSTxn+8+3SunF4ccnUi0hcFuhxVa0cXf9tez+ba/Ty4dDtrdzWycNZoPnP5ZCYWZYddnoj0oECXfmvr7OJ7f9nEf7+4hfbObq6ZVcKnLp/MhMKssEsTERTochxqm9r48XOb+cXSbbR1djNzTD6XTi1i3mkjmTYyN+zyRIasEw50M5sHfBeIAD9196/3Wv5Z4ONAJ1ALfNTdtx1tmwr0+FDT1MpDS3fwzIYaVlbW4w6XTi3ihgsmkJESobxmP93ufGDOWFKTdTJVZLCdUKCbWQTYCFwJVALLgEXuvrbHOpcCS929xcxuBi5x9w8ebbsK9Pizd38bD726nf9+cQt1LR2HLZs1Np//ufFs8jNTQ6pOZGg40TtF5wLl7l4RbGwxsBA4FOju/kyP9V8BPnT85UqsKshO45bLJvPRCyewbGsdSQZjhmWydmcj//TwChb9ZCm/+NhcCrLTwi5VZEjqT6CXADt6zFcC5xxl/Y8Bf+xrgZndBNwEMG7cuH6WKLEmMzWZi6cUHZqfUJhFTnoyN/28jAX3vMSNF5Sy4IzRjMhND7FKkaGnP0Mu7wPmufvHg/kPA+e4+y19rPsh4BbgYndvO9p2NeSSeJZW7OVrj61ldVUjAMW5aVw1fSSnleRSWpDFlOIchmVpSEbkRJzokEsVMLbH/JigrfdOrgC+RD/CXBLTORML+MOn3sbG3U08s76Gv22v55GyHfz8lW4AMlIifO6qKVx3zjgyU/VcOJGB1p8eejLRk6KXEw3yZcB17r6mxzqzgUeJ9uQ39WfH6qEPDZ1d3eysb2Xznv38/OVtPL2+BoD8zBQumFTIzZdM4rSSvJCrFIkfJ9RDd/dOM7sFeILoZYv3u/saM7sTKHP3JcA3gWzgl2YGsN3dFwzYv0DiVnIkiXEFmYwryOSSKUW8sGkPr1fWU1l3gMdW7eKxVbt42+RCPnP5ZOboRRwiJ0Q3FkloGls7+MUr27j/xS3s2d/OZdNGcOMFpZw/qZCIXp0n0ifdKSoxraW9k5+9tJWfvlBBXUsHWakRZozO47SSPIpz00iJJDFjdC7nTCwIu1SR0CnQJS60dXbx1NoaXt2yl1VVDazZ2UhbZ/eh5f981RQ+eekpBMN6IkOSXkEncSEtOcI7Zo7iHTNHAdDV7bR2dNHW2c2dv1/Dfz0ZfYb7Jy6exJXTi0nRc9tFDqNAl5gVSTKy0pLJSoPvfHAW500q4N5nNvOP//cahdmpvOP0UZx/SiGlBVm8sKmWZzfUcv0545h/+qiwSxcJhYZcJK50dTvPbaxh8as7eH5TLa0dbwzJFGansmd/O++eXcJXF84gV6/WkwSkIRdJGJEk47JpxVw2rZi2zi5WVzVSWdfC2OGZzCzJ455nyvn+0+UsrdjLNbNLOH9SITPH5incZUhQD10Szmvb67jrj+tZvq2Ozu7o8T06L52pI3OYPW4Y18wqYVxBZshVihwfXeUiQ9L+tk7Ktu5j7a5GNlQ3RT+7mwC4eEoRH5wzlkunjSA9JRJypSL9p0AXCVQ3tPLg0m08XLaD3Y1tZKREuGhKIVecWsyU4hxmjM4lWVfPSAxToIv00tnVzcsVe3lyzW7+vHY31Y2tAOSkJTP/9JG8f85Yzhw3THesSsxRoIschbuzdlcjFbXNPLexlsdX7aKlvYvc9GTOmVjArLH5zBidy2kleRTq5R0SMgW6yFvQ1NrBMxtqeXFTLa9u2cfWvS2Hlk0tzuHCyYWcM2E450woIC9TV8/IyaVAFzkBDQc6WLuzkRU76nlhUy1l2+poDx5JkJeRwqi8dKYU53D5qSO4avpI0lOS6Op2jcXLoFCgiwyg1o4uVu6oZ/n2OqobWtmxr4U1OxupaWojOclwojdAXTZtBP8ybyrTRuaGXbIkEAW6yCDr7naWbtnH85tqSTJo7+zm4WU7aGztJDstmRE5aUwsymLs8EwWzR3HlOKcsEuWOKVAFwlBfUs7D766ndqmNqobWtmyp5mKPc20d3Zzekke500q4MJTCpk1Ll93skq/KdBFYkRdczuLl+3g2Q3Rd662d0XH4icWZXHGmHxmjslj5pg88jJSmViYRZIum5ReFOgiMailvZOyrXW8XlnPih0NvF5ZT03TG+9XH5aZwoWTi7hociHFuelsqG5iZF46b58xktRknXAdqvRwLpEYlJmazEVTirhoStGhtuqGVtbsbGBvczuvVOzlhU17+P3KnYf9vcLsVBacUcLo/HTaOruZOSaPCyYVqjcvCnSRWDIyL52ReekAfGDOWNydDbubqG1qY/qoXF6vauDhV3fwvy9vPfTgMYDxBZlMKc5hQmF06OaMsXmU5Gfo7U5DjIZcROJQc1snnV1OJGI8vb6GX79WSXVD66GTrhDtyUfDPfqZNjKHETlpCvk4pyEXkQSTlfbGf90FZ4xmwRmjgejlkuurG1m5Izouv7Kynqc31HCw35aTlszs8cOYWZLHjNG5zBidR0qyUZSdphuhEkC/At3M5gHfBSLAT939672WXwTcDcwErnX3Rwe4ThHph9TkJGaOyWfmmHw+fF60rbG1g9VVDZTX7Gd9dRNlW/fxUvkeunoM2aREjHHDM5lYlM3EoiwumFTIBacU6uFkceaYQy5mFgE2AlcClcAyYJG7r+2xTimQC/wzsKQ/ga4hF5HwtHZ0saG6iXW7Gmnt6GJ3UxsVtfupqG1m294W2ru6SU+J/nCYWzr80JDNmGEalw/biQ65zAXK3b0i2NhiYCFwKNDdfWuwrLuvDYhIbElPiRwaW++trbOLJ9fs5rXtdZRtreOHz20+1JvPSUumMCeNlIhRnJvOVTNGMv+0kXoKZYzoT6CXADt6zFcC5xzPzszsJuAmgHHjxh3PJkRkkKUlR3jXGaN5VzAu39LeydqdjWzY3cT6XU3UtbTT2eVsqmni//12Nf/vt6sZMyyDCYVZTCrKZtzwTFIixqQR2Zw1fhhpyRHcnZ0NrRgwOj8j3H9gAjupJ0Xd/T7gPogOuZzMfYvI8clMTWZO6XDmlA4/rN3dWberiec31bJ2ZyNb9zbzSNkOWtq7Dq2TkRJhysgc6prb2b4v+hjicyYM52MXTuDyU4s1Rj/A+hPoVcDYHvNjgjYRGcLMjOmjc5k++o2nSXZ3O/UHOujs6ub1ygae31TLlj3N5KYn85HzS2nr7OL/XtnOTT9fzviCTC48pZBxwzM5Y2w+Ewuz6Oh2UpKMEbnpIf7L4ld/An0ZMNnMJhAN8muB6wa1KhGJS0lJxvCsVACumJ7OFdOL37TOTW+byBNrdvO/L2/lsVW7qG/peNM6U4tzmH/6SGaOyWNkbgY56ckMy0olKzWik7JH0a8bi8zsaqKXJUaA+939383sTqDM3ZeY2dnAb4BhQCtQ7e4zjrZNXeUiIgD7mttZuaOeyroWUpOTaGrt5Pev7+L1ynp6x1NqJIlhWSlMLMzm1FHR3w5OHZXD5BE5Q+b5Nno4l4jEnf1tnazb1cje/W00tnZS39LOvuYO9uxvY1PNftbvaqQtuCs2JWJkpibT0dXNuRMLeP9ZY5hQlMWovAxy05MTqlevO0VFJO5kpyVzdq8TsT11dTtb9jSzdlcj63Y1sr+1EzN4fFU1T6+vObReVmqEUfkZjMpLZ1JRNqeOyoneQFWYxfCs1MQKe/XQRSSRtHd2s6qqgV0NB9hV38rOHn9u2r2fAx1vXIWTl5HC6PwMRuSkUZybxsi8DMbkZ1AyLIOS/AxG52fE3FCOeugiMmSkJidx1vhhRE/pHa6r26mqO8DmPfvZUttMxZ79VDe0sruxjXW7Gqnd33bYuH2SwbjhmYzOz6AoJ42S/AwmF2dzSlEOJcMyGJaZElM9fAW6iAwZkSRjXEEm4woyuXTqm5e3d3ZT3dBKZX0LVXUH2LGvhc17mqluaOW17XU89vquwx5bnJ6SxOj8DHLTU3BgQkEmZ40fRkZqMo+U7WD2uHw+c/lk9jW3s3d/O6WFWeRlDN7rBjXkIiLST+2d3Wzd20xF7X521reys/4AOxsO0HigE4Dymv1UN7YCMCInjZqmNlKTkw490hhgZG46t82fxjWzS46rBg25iIgMgNTkJKYU5zClOKfP5e7O9n0tVDe0cub4YazYUc+vlldyyohsxg7PZMueZjZWN1E8SDdOKdBFRAaImTG+IIvxBVkAnF06/KhX6gy02Dp9KyIix02BLiKSIBToIiIJQoEuIpIgFOgiIglCgS4ikiAU6CIiCUKBLiKSIEK79d/MaoFtx/nXC4E9A1jOQInFumKxJojNumKxJojNumKxJojNuga6pvHuXtTXgtAC/USYWdmRnmUQplisKxZrgtisKxZrgtisKxZrgtis62TWpCEXEZEEoUAXEUkQ8Rro94VdwBHEYl2xWBPEZl2xWBPEZl2xWBPEZl0nraa4HEMXEZE3i9ceuoiI9KJAFxFJEHEX6GY2z8w2mFm5md12Evd7v5nVmNnqHm3DzezPZrYp+HNY0G5m9r2gxtfN7MxBrGusmT1jZmvNbI2ZfSbs2sws3cxeNbOVQU1fDdonmNnSYN8Pm1lq0J4WzJcHy0sHuqYetUXM7G9m9ocYqmmrma0ysxVmVha0xcKxlW9mj5rZejNbZ2bnhXxcTQ2+Rgc/jWZ2a9hfKzP7p+A4X21mDwXHfzjHlbvHzQeIAJuBiUAqsBKYfpL2fRFwJrC6R9s3gNuC6duAu4Lpq4E/AgacCywdxLpGAWcG0znARmB6mLUF284OplOApcG+HgGuDdp/BNwcTP8j8KNg+lrg4UH8en0WeBD4QzAfCzVtBQp7tcXCsfUA8PFgOhXIj4W6gv1FgGpgfMjHegmwBcjocTzdENZxNWhf8EH6Jp4HPNFj/nbg9pO4/1IOD/QNwKhgehSwIZj+MbCor/VOQo2/A66MldqATOA14Byid8sl9/5eAk8A5wXTycF6Ngi1jAH+AlwG/CH4jx5qTcH2t/LmQA/1+wfkBUFlsVRXj+1fBbwUdk1EA30HMDw4Tv4AvD2s4yrehlwOfvEOqgzawlLs7ruC6WqgOJgOpc7g17fZRHvEodYWDG2sAGqAPxP9zare3Tv72O+hmoLlDUDBQNcE3A38C3DwFewFMVATgANPmtlyM7spaAv72JoA1AI/C4aofmpmWTFQ10HXAg8F06HV5O5VwH8B24FdRI+T5YR0XMVboMcsj/7IDe0aUDPLBn4F3OrujT2XhVGbu3e5+yyiveK5wLSTuf/ezOydQI27Lw+zjiO40N3PBOYDnzSzi3ouDOnYSiY6xPhDd58NNBMdzgi7LoLx6AXAL3svO9k1BeP1C4n+ABwNZAHzTtb+e4u3QK8CxvaYHxO0hWW3mY0CCP6sCdpPap1mlkI0zP/P3X8dS7W5ez3wDNFfO/PNLLmP/R6qKVieB+wd4FIuABaY2VZgMdFhl++GXBNwqJeHu9cAvyH6AzDs718lUOnuS4P5R4kGfNh1QfQH32vuvjuYD7OmK4At7l7r7h3Ar4kea6EcV/EW6MuAycEZ5FSiv3YtCbGeJcBHgumPEB2/Ptj+d8FZ9nOBhh6/Eg4oMzPgv4F17v7tWKjNzIrMLD+YziA6pr+OaLC/7wg1Haz1fcDTQU9rwLj77e4+xt1LiR43T7v79WHWBGBmWWaWc3Ca6NjwakI+tty9GthhZlODpsuBtWHXFVjEG8MtB/cdVk3bgXPNLDP4v3jw6xTOcTVYJy0G60P0zPVGomOyXzqJ+32I6BhZB9Hey8eIjn39BdgEPAUMD9Y14N6gxlXAnEGs60Kiv2K+DqwIPleHWRswE/hbUNNq4I6gfSLwKlBO9NfltKA9PZgvD5ZPHOTv5SW8cZVLqDUF+18ZfNYcPKZj5NiaBZQF38ffAsPCrovokMZeIK9HW9g1fRVYHxzrPwfSwjqudOu/iEiCiLchFxEROQIFuohIglCgi4gkCAW6iEiCUKCLiCQIBbqISIJQoIuIJIj/D7YSxFRvYvh0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve')\n",
    "plt.plot(range(len(mlpClassifier.loss_curve_)),mlpClassifier.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230bf4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(pd.DataFrame(confMatrix),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14692430",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a70f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04413811\n",
      "Iteration 2, loss = 1.03103132\n",
      "Iteration 3, loss = 1.01869998\n",
      "Iteration 4, loss = 1.00663400\n",
      "Iteration 5, loss = 0.99494172\n",
      "Iteration 6, loss = 0.98332969\n",
      "Iteration 7, loss = 0.97231050\n",
      "Iteration 8, loss = 0.96151232\n",
      "Iteration 9, loss = 0.95075399\n",
      "Iteration 10, loss = 0.94039067\n",
      "Iteration 11, loss = 0.92991170\n",
      "Iteration 12, loss = 0.91993123\n",
      "Iteration 13, loss = 0.90992657\n",
      "Iteration 14, loss = 0.90040501\n",
      "Iteration 15, loss = 0.89084888\n",
      "Iteration 16, loss = 0.88173495\n",
      "Iteration 17, loss = 0.87270159\n",
      "Iteration 18, loss = 0.86406929\n",
      "Iteration 19, loss = 0.85548709\n",
      "Iteration 20, loss = 0.84697643\n",
      "Iteration 21, loss = 0.83893465\n",
      "Iteration 22, loss = 0.83056830\n",
      "Iteration 23, loss = 0.82269022\n",
      "Iteration 24, loss = 0.81469552\n",
      "Iteration 25, loss = 0.80695304\n",
      "Iteration 26, loss = 0.79941992\n",
      "Iteration 27, loss = 0.79219324\n",
      "Iteration 28, loss = 0.78504727\n",
      "Iteration 29, loss = 0.77801535\n",
      "Iteration 30, loss = 0.77109799\n",
      "Iteration 31, loss = 0.76446092\n",
      "Iteration 32, loss = 0.75755139\n",
      "Iteration 33, loss = 0.75133075\n",
      "Iteration 34, loss = 0.74522786\n",
      "Iteration 35, loss = 0.73942968\n",
      "Iteration 36, loss = 0.73381133\n",
      "Iteration 37, loss = 0.72854403\n",
      "Iteration 38, loss = 0.72324233\n",
      "Iteration 39, loss = 0.71829853\n",
      "Iteration 40, loss = 0.71319193\n",
      "Iteration 41, loss = 0.70815346\n",
      "Iteration 42, loss = 0.70347739\n",
      "Iteration 43, loss = 0.69863151\n",
      "Iteration 44, loss = 0.69424932\n",
      "Iteration 45, loss = 0.68985073\n",
      "Iteration 46, loss = 0.68573506\n",
      "Iteration 47, loss = 0.68169639\n",
      "Iteration 48, loss = 0.67777333\n",
      "Iteration 49, loss = 0.67393390\n",
      "Iteration 50, loss = 0.67026835\n",
      "Iteration 51, loss = 0.66673165\n",
      "Iteration 52, loss = 0.66319840\n",
      "Iteration 53, loss = 0.65963657\n",
      "Iteration 54, loss = 0.65593753\n",
      "Iteration 55, loss = 0.65235112\n",
      "Iteration 56, loss = 0.64868017\n",
      "Iteration 57, loss = 0.64518624\n",
      "Iteration 58, loss = 0.64166608\n",
      "Iteration 59, loss = 0.63825409\n",
      "Iteration 60, loss = 0.63502326\n",
      "Iteration 61, loss = 0.63172809\n",
      "Iteration 62, loss = 0.62858623\n",
      "Iteration 63, loss = 0.62542330\n",
      "Iteration 64, loss = 0.62218807\n",
      "Iteration 65, loss = 0.61897260\n",
      "Iteration 66, loss = 0.61578821\n",
      "Iteration 67, loss = 0.61266514\n",
      "Iteration 68, loss = 0.60963761\n",
      "Iteration 69, loss = 0.60673266\n",
      "Iteration 70, loss = 0.60398261\n",
      "Iteration 71, loss = 0.60123848\n",
      "Iteration 72, loss = 0.59858833\n",
      "Iteration 73, loss = 0.59606141\n",
      "Iteration 74, loss = 0.59351256\n",
      "Iteration 75, loss = 0.59102942\n",
      "Iteration 76, loss = 0.58875570\n",
      "Iteration 77, loss = 0.58637554\n",
      "Iteration 78, loss = 0.58414020\n",
      "Iteration 79, loss = 0.58189289\n",
      "Iteration 80, loss = 0.57964092\n",
      "Iteration 81, loss = 0.57754067\n",
      "Iteration 82, loss = 0.57540124\n",
      "Iteration 83, loss = 0.57338592\n",
      "Iteration 84, loss = 0.57122477\n",
      "Iteration 85, loss = 0.56918114\n",
      "Iteration 86, loss = 0.56708426\n",
      "Iteration 87, loss = 0.56500789\n",
      "Iteration 88, loss = 0.56298307\n",
      "Iteration 89, loss = 0.56104285\n",
      "Iteration 90, loss = 0.55907935\n",
      "Iteration 91, loss = 0.55726979\n",
      "Iteration 92, loss = 0.55540499\n",
      "Iteration 93, loss = 0.55356998\n",
      "Iteration 94, loss = 0.55173578\n",
      "Iteration 95, loss = 0.54990206\n",
      "Iteration 96, loss = 0.54808739\n",
      "Iteration 97, loss = 0.54623161\n",
      "Iteration 98, loss = 0.54442123\n",
      "Iteration 99, loss = 0.54259502\n",
      "Iteration 100, loss = 0.54077832\n",
      "Iteration 101, loss = 0.53891650\n",
      "Iteration 102, loss = 0.53711159\n",
      "Iteration 103, loss = 0.53525446\n",
      "Iteration 104, loss = 0.53343365\n",
      "Iteration 105, loss = 0.53153764\n",
      "Iteration 106, loss = 0.52972211\n",
      "Iteration 107, loss = 0.52794455\n",
      "Iteration 108, loss = 0.52618193\n",
      "Iteration 109, loss = 0.52445156\n",
      "Iteration 110, loss = 0.52282581\n",
      "Iteration 111, loss = 0.52115292\n",
      "Iteration 112, loss = 0.51960123\n",
      "Iteration 113, loss = 0.51801359\n",
      "Iteration 114, loss = 0.51655833\n",
      "Iteration 115, loss = 0.51501233\n",
      "Iteration 116, loss = 0.51356829\n",
      "Iteration 117, loss = 0.51210320\n",
      "Iteration 118, loss = 0.51065990\n",
      "Iteration 119, loss = 0.50923246\n",
      "Iteration 120, loss = 0.50784716\n",
      "Iteration 121, loss = 0.50652062\n",
      "Iteration 122, loss = 0.50518556\n",
      "Iteration 123, loss = 0.50384939\n",
      "Iteration 124, loss = 0.50259394\n",
      "Iteration 125, loss = 0.50135713\n",
      "Iteration 126, loss = 0.50010765\n",
      "Iteration 127, loss = 0.49886549\n",
      "Iteration 128, loss = 0.49765062\n",
      "Iteration 129, loss = 0.49638909\n",
      "Iteration 130, loss = 0.49517984\n",
      "Iteration 131, loss = 0.49402926\n",
      "Iteration 132, loss = 0.49283931\n",
      "Iteration 133, loss = 0.49167783\n",
      "Iteration 134, loss = 0.49052017\n",
      "Iteration 135, loss = 0.48941016\n",
      "Iteration 136, loss = 0.48837189\n",
      "Iteration 137, loss = 0.48732650\n",
      "Iteration 138, loss = 0.48631465\n",
      "Iteration 139, loss = 0.48525648\n",
      "Iteration 140, loss = 0.48418897\n",
      "Iteration 141, loss = 0.48314798\n",
      "Iteration 142, loss = 0.48203744\n",
      "Iteration 143, loss = 0.48085978\n",
      "Iteration 144, loss = 0.47971007\n",
      "Iteration 145, loss = 0.47860172\n",
      "Iteration 146, loss = 0.47740649\n",
      "Iteration 147, loss = 0.47633246\n",
      "Iteration 148, loss = 0.47520063\n",
      "Iteration 149, loss = 0.47416616\n",
      "Iteration 150, loss = 0.47312207\n",
      "Iteration 151, loss = 0.47204949\n",
      "Iteration 152, loss = 0.47100915\n",
      "Iteration 153, loss = 0.46999875\n",
      "Iteration 154, loss = 0.46896576\n",
      "Iteration 155, loss = 0.46798240\n",
      "Iteration 156, loss = 0.46695593\n",
      "Iteration 157, loss = 0.46592405\n",
      "Iteration 158, loss = 0.46492240\n",
      "Iteration 159, loss = 0.46390591\n",
      "Iteration 160, loss = 0.46289421\n",
      "Iteration 161, loss = 0.46194594\n",
      "Iteration 162, loss = 0.46097513\n",
      "Iteration 163, loss = 0.46007008\n",
      "Iteration 164, loss = 0.45915884\n",
      "Iteration 165, loss = 0.45825314\n",
      "Iteration 166, loss = 0.45732971\n",
      "Iteration 167, loss = 0.45637920\n",
      "Iteration 168, loss = 0.45540081\n",
      "Iteration 169, loss = 0.45443426\n",
      "Iteration 170, loss = 0.45350281\n",
      "Iteration 171, loss = 0.45258482\n",
      "Iteration 172, loss = 0.45172020\n",
      "Iteration 173, loss = 0.45082705\n",
      "Iteration 174, loss = 0.44990659\n",
      "Iteration 175, loss = 0.44899913\n",
      "Iteration 176, loss = 0.44812602\n",
      "Iteration 177, loss = 0.44717259\n",
      "Iteration 178, loss = 0.44630118\n",
      "Iteration 179, loss = 0.44541684\n",
      "Iteration 180, loss = 0.44455581\n",
      "Iteration 181, loss = 0.44368131\n",
      "Iteration 182, loss = 0.44282139\n",
      "Iteration 183, loss = 0.44195525\n",
      "Iteration 184, loss = 0.44109302\n",
      "Iteration 185, loss = 0.44019623\n",
      "Iteration 186, loss = 0.43933055\n",
      "Iteration 187, loss = 0.43848376\n",
      "Iteration 188, loss = 0.43758799\n",
      "Iteration 189, loss = 0.43672857\n",
      "Iteration 190, loss = 0.43584162\n",
      "Iteration 191, loss = 0.43498136\n",
      "Iteration 192, loss = 0.43412893\n",
      "Iteration 193, loss = 0.43325760\n",
      "Iteration 194, loss = 0.43244315\n",
      "Iteration 195, loss = 0.43161339\n",
      "Iteration 196, loss = 0.43077717\n",
      "Iteration 197, loss = 0.42994547\n",
      "Iteration 198, loss = 0.42910711\n",
      "Iteration 199, loss = 0.42831363\n",
      "Iteration 200, loss = 0.42754974\n",
      "Iteration 201, loss = 0.42674037\n",
      "Iteration 202, loss = 0.42596797\n",
      "Iteration 203, loss = 0.42519047\n",
      "Iteration 204, loss = 0.42444253\n",
      "Iteration 205, loss = 0.42368052\n",
      "Iteration 206, loss = 0.42291605\n",
      "Iteration 207, loss = 0.42218743\n",
      "Iteration 208, loss = 0.42140113\n",
      "Iteration 209, loss = 0.42060484\n",
      "Iteration 210, loss = 0.41979703\n",
      "Iteration 211, loss = 0.41898458\n",
      "Iteration 212, loss = 0.41825862\n",
      "Iteration 213, loss = 0.41746580\n",
      "Iteration 214, loss = 0.41666035\n",
      "Iteration 215, loss = 0.41583397\n",
      "Iteration 216, loss = 0.41508289\n",
      "Iteration 217, loss = 0.41428190\n",
      "Iteration 218, loss = 0.41348972\n",
      "Iteration 219, loss = 0.41274403\n",
      "Iteration 220, loss = 0.41197087\n",
      "Iteration 221, loss = 0.41119507\n",
      "Iteration 222, loss = 0.41040640\n",
      "Iteration 223, loss = 0.40958609\n",
      "Iteration 224, loss = 0.40882145\n",
      "Iteration 225, loss = 0.40807058\n",
      "Iteration 226, loss = 0.40727186\n",
      "Iteration 227, loss = 0.40647852\n",
      "Iteration 228, loss = 0.40567712\n",
      "Iteration 229, loss = 0.40486214\n",
      "Iteration 230, loss = 0.40409042\n",
      "Iteration 231, loss = 0.40330185\n",
      "Iteration 232, loss = 0.40250854\n",
      "Iteration 233, loss = 0.40172373\n",
      "Iteration 234, loss = 0.40100952\n",
      "Iteration 235, loss = 0.40027134\n",
      "Iteration 236, loss = 0.39957987\n",
      "Iteration 237, loss = 0.39887804\n",
      "Iteration 238, loss = 0.39824053\n",
      "Iteration 239, loss = 0.39762529\n",
      "Iteration 240, loss = 0.39688689\n",
      "Iteration 241, loss = 0.39615874\n",
      "Iteration 242, loss = 0.39537649\n",
      "Iteration 243, loss = 0.39464139\n",
      "Iteration 244, loss = 0.39386639\n",
      "Iteration 245, loss = 0.39310522\n",
      "Iteration 246, loss = 0.39234644\n",
      "Iteration 247, loss = 0.39156755\n",
      "Iteration 248, loss = 0.39083356\n",
      "Iteration 249, loss = 0.39004741\n",
      "Iteration 250, loss = 0.38932453\n",
      "Iteration 251, loss = 0.38857346\n",
      "Iteration 252, loss = 0.38790191\n",
      "Iteration 253, loss = 0.38717170\n",
      "Iteration 254, loss = 0.38648055\n",
      "Iteration 255, loss = 0.38578145\n",
      "Iteration 256, loss = 0.38504847\n",
      "Iteration 257, loss = 0.38432281\n",
      "Iteration 258, loss = 0.38362155\n",
      "Iteration 259, loss = 0.38290476\n",
      "Iteration 260, loss = 0.38216748\n",
      "Iteration 261, loss = 0.38148659\n",
      "Iteration 262, loss = 0.38081177\n",
      "Iteration 263, loss = 0.38012095\n",
      "Iteration 264, loss = 0.37948531\n",
      "Iteration 265, loss = 0.37882576\n",
      "Iteration 266, loss = 0.37819796\n",
      "Iteration 267, loss = 0.37756563\n",
      "Iteration 268, loss = 0.37694196\n",
      "Iteration 269, loss = 0.37631076\n",
      "Iteration 270, loss = 0.37569019\n",
      "Iteration 271, loss = 0.37504547\n",
      "Iteration 272, loss = 0.37441080\n",
      "Iteration 273, loss = 0.37376330\n",
      "Iteration 274, loss = 0.37312396\n",
      "Iteration 275, loss = 0.37247312\n",
      "Iteration 276, loss = 0.37183539\n",
      "Iteration 277, loss = 0.37120360\n",
      "Iteration 278, loss = 0.37057281\n",
      "Iteration 279, loss = 0.36992417\n",
      "Iteration 280, loss = 0.36919821\n",
      "Iteration 281, loss = 0.36856217\n",
      "Iteration 282, loss = 0.36790510\n",
      "Iteration 283, loss = 0.36730167\n",
      "Iteration 284, loss = 0.36663850\n",
      "Iteration 285, loss = 0.36601718\n",
      "Iteration 286, loss = 0.36537104\n",
      "Iteration 287, loss = 0.36481053\n",
      "Iteration 288, loss = 0.36420321\n",
      "Iteration 289, loss = 0.36361580\n",
      "Iteration 290, loss = 0.36306186\n",
      "Iteration 291, loss = 0.36248836\n",
      "Iteration 292, loss = 0.36192387\n",
      "Iteration 293, loss = 0.36136213\n",
      "Iteration 294, loss = 0.36078365\n",
      "Iteration 295, loss = 0.36022273\n",
      "Iteration 296, loss = 0.35970464\n",
      "Iteration 297, loss = 0.35914167\n",
      "Iteration 298, loss = 0.35856636\n",
      "Iteration 299, loss = 0.35801937\n",
      "Iteration 300, loss = 0.35742651\n",
      "Iteration 301, loss = 0.35688192\n",
      "Iteration 302, loss = 0.35628363\n",
      "Iteration 303, loss = 0.35571017\n",
      "Iteration 304, loss = 0.35513293\n",
      "Iteration 305, loss = 0.35451656\n",
      "Iteration 306, loss = 0.35399798\n",
      "Iteration 307, loss = 0.35346286\n",
      "Iteration 308, loss = 0.35290318\n",
      "Iteration 309, loss = 0.35238240\n",
      "Iteration 310, loss = 0.35183059\n",
      "Iteration 311, loss = 0.35128039\n",
      "Iteration 312, loss = 0.35068234\n",
      "Iteration 313, loss = 0.35008561\n",
      "Iteration 314, loss = 0.34952582\n",
      "Iteration 315, loss = 0.34894324\n",
      "Iteration 316, loss = 0.34837519\n",
      "Iteration 317, loss = 0.34781974\n",
      "Iteration 318, loss = 0.34724646\n",
      "Iteration 319, loss = 0.34674310\n",
      "Iteration 320, loss = 0.34624773\n",
      "Iteration 321, loss = 0.34575828\n",
      "Iteration 322, loss = 0.34538621\n",
      "Iteration 323, loss = 0.34486894\n",
      "Iteration 324, loss = 0.34438776\n",
      "Iteration 325, loss = 0.34393244\n",
      "Iteration 326, loss = 0.34350733\n",
      "Iteration 327, loss = 0.34308542\n",
      "Iteration 328, loss = 0.34263190\n",
      "Iteration 329, loss = 0.34209419\n",
      "Iteration 330, loss = 0.34155530\n",
      "Iteration 331, loss = 0.34098420\n",
      "Iteration 332, loss = 0.34038598\n",
      "Iteration 333, loss = 0.33989244\n",
      "Iteration 334, loss = 0.33929193\n",
      "Iteration 335, loss = 0.33879565\n",
      "Iteration 336, loss = 0.33833017\n",
      "Iteration 337, loss = 0.33782698\n",
      "Iteration 338, loss = 0.33727999\n",
      "Iteration 339, loss = 0.33677711\n",
      "Iteration 340, loss = 0.33624394\n",
      "Iteration 341, loss = 0.33572685\n",
      "Iteration 342, loss = 0.33516522\n",
      "Iteration 343, loss = 0.33462144\n",
      "Iteration 344, loss = 0.33406124\n",
      "Iteration 345, loss = 0.33355002\n",
      "Iteration 346, loss = 0.33301828\n",
      "Iteration 347, loss = 0.33248983\n",
      "Iteration 348, loss = 0.33197322\n",
      "Iteration 349, loss = 0.33150541\n",
      "Iteration 350, loss = 0.33105068\n",
      "Iteration 351, loss = 0.33062483\n",
      "Iteration 352, loss = 0.33016275\n",
      "Iteration 353, loss = 0.32973052\n",
      "Iteration 354, loss = 0.32929261\n",
      "Iteration 355, loss = 0.32884398\n",
      "Iteration 356, loss = 0.32828563\n",
      "Iteration 357, loss = 0.32774644\n",
      "Iteration 358, loss = 0.32720678\n",
      "Iteration 359, loss = 0.32672016\n",
      "Iteration 360, loss = 0.32617896\n",
      "Iteration 361, loss = 0.32562692\n",
      "Iteration 362, loss = 0.32525104\n",
      "Iteration 363, loss = 0.32468914\n",
      "Iteration 364, loss = 0.32424382\n",
      "Iteration 365, loss = 0.32384788\n",
      "Iteration 366, loss = 0.32341985\n",
      "Iteration 367, loss = 0.32305855\n",
      "Iteration 368, loss = 0.32263203\n",
      "Iteration 369, loss = 0.32222082\n",
      "Iteration 370, loss = 0.32176456\n",
      "Iteration 371, loss = 0.32129509\n",
      "Iteration 372, loss = 0.32081750\n",
      "Iteration 373, loss = 0.32033119\n",
      "Iteration 374, loss = 0.31992593\n",
      "Iteration 375, loss = 0.31944882\n",
      "Iteration 376, loss = 0.31901635\n",
      "Iteration 377, loss = 0.31859910\n",
      "Iteration 378, loss = 0.31814277\n",
      "Iteration 379, loss = 0.31773458\n",
      "Iteration 380, loss = 0.31728958\n",
      "Iteration 381, loss = 0.31683155\n",
      "Iteration 382, loss = 0.31639803\n",
      "Iteration 383, loss = 0.31598532\n",
      "Iteration 384, loss = 0.31549077\n",
      "Iteration 385, loss = 0.31504701\n",
      "Iteration 386, loss = 0.31459937\n",
      "Iteration 387, loss = 0.31413220\n",
      "Iteration 388, loss = 0.31367678\n",
      "Iteration 389, loss = 0.31322110\n",
      "Iteration 390, loss = 0.31280564\n",
      "Iteration 391, loss = 0.31247283\n",
      "Iteration 392, loss = 0.31207352\n",
      "Iteration 393, loss = 0.31165995\n",
      "Iteration 394, loss = 0.31131254\n",
      "Iteration 395, loss = 0.31098867\n",
      "Iteration 396, loss = 0.31068239\n",
      "Iteration 397, loss = 0.31032664\n",
      "Iteration 398, loss = 0.30997427\n",
      "Iteration 399, loss = 0.30957908\n",
      "Iteration 400, loss = 0.30921171\n",
      "Iteration 401, loss = 0.30889621\n",
      "Iteration 402, loss = 0.30857988\n",
      "Iteration 403, loss = 0.30826891\n",
      "Iteration 404, loss = 0.30787081\n",
      "Iteration 405, loss = 0.30751619\n",
      "Iteration 406, loss = 0.30711878\n",
      "Iteration 407, loss = 0.30668222\n",
      "Iteration 408, loss = 0.30620949\n",
      "Iteration 409, loss = 0.30584859\n",
      "Iteration 410, loss = 0.30535672\n",
      "Iteration 411, loss = 0.30507537\n",
      "Iteration 412, loss = 0.30459601\n",
      "Iteration 413, loss = 0.30426950\n",
      "Iteration 414, loss = 0.30394035\n",
      "Iteration 415, loss = 0.30353390\n",
      "Iteration 416, loss = 0.30313115\n",
      "Iteration 417, loss = 0.30272172\n",
      "Iteration 418, loss = 0.30232180\n",
      "Iteration 419, loss = 0.30191157\n",
      "Iteration 420, loss = 0.30151589\n",
      "Iteration 421, loss = 0.30111378\n",
      "Iteration 422, loss = 0.30070689\n",
      "Iteration 423, loss = 0.30032857\n",
      "Iteration 424, loss = 0.29994343\n",
      "Iteration 425, loss = 0.29958812\n",
      "Iteration 426, loss = 0.29921029\n",
      "Iteration 427, loss = 0.29887855\n",
      "Iteration 428, loss = 0.29857021\n",
      "Iteration 429, loss = 0.29821831\n",
      "Iteration 430, loss = 0.29788956\n",
      "Iteration 431, loss = 0.29758072\n",
      "Iteration 432, loss = 0.29731037\n",
      "Iteration 433, loss = 0.29698870\n",
      "Iteration 434, loss = 0.29668509\n",
      "Iteration 435, loss = 0.29640981\n",
      "Iteration 436, loss = 0.29605399\n",
      "Iteration 437, loss = 0.29567741\n",
      "Iteration 438, loss = 0.29525953\n",
      "Iteration 439, loss = 0.29483615\n",
      "Iteration 440, loss = 0.29437439\n",
      "Iteration 441, loss = 0.29396206\n",
      "Iteration 442, loss = 0.29350250\n",
      "Iteration 443, loss = 0.29307557\n",
      "Iteration 444, loss = 0.29265689\n",
      "Iteration 445, loss = 0.29227571\n",
      "Iteration 446, loss = 0.29187749\n",
      "Iteration 447, loss = 0.29147954\n",
      "Iteration 448, loss = 0.29113050\n",
      "Iteration 449, loss = 0.29083228\n",
      "Iteration 450, loss = 0.29053941\n",
      "Iteration 451, loss = 0.29017559\n",
      "Iteration 452, loss = 0.28983588\n",
      "Iteration 453, loss = 0.28948555\n",
      "Iteration 454, loss = 0.28918932\n",
      "Iteration 455, loss = 0.28883295\n",
      "Iteration 456, loss = 0.28846387\n",
      "Iteration 457, loss = 0.28803153\n",
      "Iteration 458, loss = 0.28762538\n",
      "Iteration 459, loss = 0.28732707\n",
      "Iteration 460, loss = 0.28696462\n",
      "Iteration 461, loss = 0.28660673\n",
      "Iteration 462, loss = 0.28620809\n",
      "Iteration 463, loss = 0.28594279\n",
      "Iteration 464, loss = 0.28553977\n",
      "Iteration 465, loss = 0.28520031\n",
      "Iteration 466, loss = 0.28484765\n",
      "Iteration 467, loss = 0.28456290\n",
      "Iteration 468, loss = 0.28428239\n",
      "Iteration 469, loss = 0.28392082\n",
      "Iteration 470, loss = 0.28356648\n",
      "Iteration 471, loss = 0.28316265\n",
      "Iteration 472, loss = 0.28275640\n",
      "Iteration 473, loss = 0.28237095\n",
      "Iteration 474, loss = 0.28191179\n",
      "Iteration 475, loss = 0.28146368\n",
      "Iteration 476, loss = 0.28101816\n",
      "Iteration 477, loss = 0.28056842\n",
      "Iteration 478, loss = 0.28015723\n",
      "Iteration 479, loss = 0.27976361\n",
      "Iteration 480, loss = 0.27940105\n",
      "Iteration 481, loss = 0.27903463\n",
      "Iteration 482, loss = 0.27868662\n",
      "Iteration 483, loss = 0.27834008\n",
      "Iteration 484, loss = 0.27802921\n",
      "Iteration 485, loss = 0.27766647\n",
      "Iteration 486, loss = 0.27739555\n",
      "Iteration 487, loss = 0.27708975\n",
      "Iteration 488, loss = 0.27676524\n",
      "Iteration 489, loss = 0.27653611\n",
      "Iteration 490, loss = 0.27622560\n",
      "Iteration 491, loss = 0.27590285\n",
      "Iteration 492, loss = 0.27558974\n",
      "Iteration 493, loss = 0.27533267\n",
      "Iteration 494, loss = 0.27504337\n",
      "Iteration 495, loss = 0.27471998\n",
      "Iteration 496, loss = 0.27440953\n",
      "Iteration 497, loss = 0.27409963\n",
      "Iteration 498, loss = 0.27376535\n",
      "Iteration 499, loss = 0.27346506\n",
      "Iteration 500, loss = 0.27317274\n",
      "Iteration 501, loss = 0.27288192\n",
      "Iteration 502, loss = 0.27262325\n",
      "Iteration 503, loss = 0.27231085\n",
      "Iteration 504, loss = 0.27194849\n",
      "Iteration 505, loss = 0.27159324\n",
      "Iteration 506, loss = 0.27124500\n",
      "Iteration 507, loss = 0.27096194\n",
      "Iteration 508, loss = 0.27059459\n",
      "Iteration 509, loss = 0.27025865\n",
      "Iteration 510, loss = 0.26994563\n",
      "Iteration 511, loss = 0.26969815\n",
      "Iteration 512, loss = 0.26934075\n",
      "Iteration 513, loss = 0.26907655\n",
      "Iteration 514, loss = 0.26881285\n",
      "Iteration 515, loss = 0.26849107\n",
      "Iteration 516, loss = 0.26823060\n",
      "Iteration 517, loss = 0.26790902\n",
      "Iteration 518, loss = 0.26765802\n",
      "Iteration 519, loss = 0.26743514\n",
      "Iteration 520, loss = 0.26712595\n",
      "Iteration 521, loss = 0.26689175\n",
      "Iteration 522, loss = 0.26668814\n",
      "Iteration 523, loss = 0.26644837\n",
      "Iteration 524, loss = 0.26621110\n",
      "Iteration 525, loss = 0.26592254\n",
      "Iteration 526, loss = 0.26564157\n",
      "Iteration 527, loss = 0.26539207\n",
      "Iteration 528, loss = 0.26522254\n",
      "Iteration 529, loss = 0.26504844\n",
      "Iteration 530, loss = 0.26491008\n",
      "Iteration 531, loss = 0.26471536\n",
      "Iteration 532, loss = 0.26449606\n",
      "Iteration 533, loss = 0.26421155\n",
      "Iteration 534, loss = 0.26386931\n",
      "Iteration 535, loss = 0.26343802\n",
      "Iteration 536, loss = 0.26291105\n",
      "Iteration 537, loss = 0.26232305\n",
      "Iteration 538, loss = 0.26196154\n",
      "Iteration 539, loss = 0.26162516\n",
      "Iteration 540, loss = 0.26145989\n",
      "Iteration 541, loss = 0.26107222\n",
      "Iteration 542, loss = 0.26082507\n",
      "Iteration 543, loss = 0.26061862\n",
      "Iteration 544, loss = 0.26048961\n",
      "Iteration 545, loss = 0.26031511\n",
      "Iteration 546, loss = 0.26010142\n",
      "Iteration 547, loss = 0.25991135\n",
      "Iteration 548, loss = 0.25964301\n",
      "Iteration 549, loss = 0.25934314\n",
      "Iteration 550, loss = 0.25896346\n",
      "Iteration 551, loss = 0.25860910\n",
      "Iteration 552, loss = 0.25833392\n",
      "Iteration 553, loss = 0.25795296\n",
      "Iteration 554, loss = 0.25768340\n",
      "Iteration 555, loss = 0.25745608\n",
      "Iteration 556, loss = 0.25709556\n",
      "Iteration 557, loss = 0.25702571\n",
      "Iteration 558, loss = 0.25672951\n",
      "Iteration 559, loss = 0.25652420\n",
      "Iteration 560, loss = 0.25627002\n",
      "Iteration 561, loss = 0.25600643\n",
      "Iteration 562, loss = 0.25569979\n",
      "Iteration 563, loss = 0.25549408\n",
      "Iteration 564, loss = 0.25517846\n",
      "Iteration 565, loss = 0.25489125\n",
      "Iteration 566, loss = 0.25457108\n",
      "Iteration 567, loss = 0.25425216\n",
      "Iteration 568, loss = 0.25391626\n",
      "Iteration 569, loss = 0.25365962\n",
      "Iteration 570, loss = 0.25342122\n",
      "Iteration 571, loss = 0.25305751\n",
      "Iteration 572, loss = 0.25280443\n",
      "Iteration 573, loss = 0.25250806\n",
      "Iteration 574, loss = 0.25223517\n",
      "Iteration 575, loss = 0.25195210\n",
      "Iteration 576, loss = 0.25171549\n",
      "Iteration 577, loss = 0.25138172\n",
      "Iteration 578, loss = 0.25111357\n",
      "Iteration 579, loss = 0.25086466\n",
      "Iteration 580, loss = 0.25054041\n",
      "Iteration 581, loss = 0.25020647\n",
      "Iteration 582, loss = 0.24999810\n",
      "Iteration 583, loss = 0.24967840\n",
      "Iteration 584, loss = 0.24948248\n",
      "Iteration 585, loss = 0.24935272\n",
      "Iteration 586, loss = 0.24906412\n",
      "Iteration 587, loss = 0.24875215\n",
      "Iteration 588, loss = 0.24850319\n",
      "Iteration 589, loss = 0.24823646\n",
      "Iteration 590, loss = 0.24804325\n",
      "Iteration 591, loss = 0.24776762\n",
      "Iteration 592, loss = 0.24748523\n",
      "Iteration 593, loss = 0.24727747\n",
      "Iteration 594, loss = 0.24720437\n",
      "Iteration 595, loss = 0.24703575\n",
      "Iteration 596, loss = 0.24680189\n",
      "Iteration 597, loss = 0.24651449\n",
      "Iteration 598, loss = 0.24617829\n",
      "Iteration 599, loss = 0.24584392\n",
      "Iteration 600, loss = 0.24552365\n",
      "Iteration 601, loss = 0.24517125\n",
      "Iteration 602, loss = 0.24484017\n",
      "Iteration 603, loss = 0.24450795\n",
      "Iteration 604, loss = 0.24426737\n",
      "Iteration 605, loss = 0.24397132\n",
      "Iteration 606, loss = 0.24370974\n",
      "Iteration 607, loss = 0.24343801\n",
      "Iteration 608, loss = 0.24316452\n",
      "Iteration 609, loss = 0.24290092\n",
      "Iteration 610, loss = 0.24267372\n",
      "Iteration 611, loss = 0.24241192\n",
      "Iteration 612, loss = 0.24224619\n",
      "Iteration 613, loss = 0.24201874\n",
      "Iteration 614, loss = 0.24179166\n",
      "Iteration 615, loss = 0.24154820\n",
      "Iteration 616, loss = 0.24130314\n",
      "Iteration 617, loss = 0.24098014\n",
      "Iteration 618, loss = 0.24067824\n",
      "Iteration 619, loss = 0.24041860\n",
      "Iteration 620, loss = 0.24010687\n",
      "Iteration 621, loss = 0.23991610\n",
      "Iteration 622, loss = 0.23976959\n",
      "Iteration 623, loss = 0.23952759\n",
      "Iteration 624, loss = 0.23929516\n",
      "Iteration 625, loss = 0.23904280\n",
      "Iteration 626, loss = 0.23880590\n",
      "Iteration 627, loss = 0.23852342\n",
      "Iteration 628, loss = 0.23829515\n",
      "Iteration 629, loss = 0.23802168\n",
      "Iteration 630, loss = 0.23777392\n",
      "Iteration 631, loss = 0.23755602\n",
      "Iteration 632, loss = 0.23732815\n",
      "Iteration 633, loss = 0.23701693\n",
      "Iteration 634, loss = 0.23678396\n",
      "Iteration 635, loss = 0.23650040\n",
      "Iteration 636, loss = 0.23626036\n",
      "Iteration 637, loss = 0.23601021\n",
      "Iteration 638, loss = 0.23580007\n",
      "Iteration 639, loss = 0.23560878\n",
      "Iteration 640, loss = 0.23531191\n",
      "Iteration 641, loss = 0.23512616\n",
      "Iteration 642, loss = 0.23491067\n",
      "Iteration 643, loss = 0.23474462\n",
      "Iteration 644, loss = 0.23472909\n",
      "Iteration 645, loss = 0.23456632\n",
      "Iteration 646, loss = 0.23438426\n",
      "Iteration 647, loss = 0.23421626\n",
      "Iteration 648, loss = 0.23404808\n",
      "Iteration 649, loss = 0.23393424\n",
      "Iteration 650, loss = 0.23377959\n",
      "Iteration 651, loss = 0.23361531\n",
      "Iteration 652, loss = 0.23341253\n",
      "Iteration 653, loss = 0.23312164\n",
      "Iteration 654, loss = 0.23284886\n",
      "Iteration 655, loss = 0.23265399\n",
      "Iteration 656, loss = 0.23247451\n",
      "Iteration 657, loss = 0.23227067\n",
      "Iteration 658, loss = 0.23208725\n",
      "Iteration 659, loss = 0.23186506\n",
      "Iteration 660, loss = 0.23161775\n",
      "Iteration 661, loss = 0.23139755\n",
      "Iteration 662, loss = 0.23119943\n",
      "Iteration 663, loss = 0.23095913\n",
      "Iteration 664, loss = 0.23074918\n",
      "Iteration 665, loss = 0.23057380\n",
      "Iteration 666, loss = 0.23027369\n",
      "Iteration 667, loss = 0.23005406\n",
      "Iteration 668, loss = 0.22974876\n",
      "Iteration 669, loss = 0.22952755\n",
      "Iteration 670, loss = 0.22930215\n",
      "Iteration 671, loss = 0.22905405\n",
      "Iteration 672, loss = 0.22880963\n",
      "Iteration 673, loss = 0.22854174\n",
      "Iteration 674, loss = 0.22824562\n",
      "Iteration 675, loss = 0.22802159\n",
      "Iteration 676, loss = 0.22772378\n",
      "Iteration 677, loss = 0.22748850\n",
      "Iteration 678, loss = 0.22717938\n",
      "Iteration 679, loss = 0.22697278\n",
      "Iteration 680, loss = 0.22673378\n",
      "Iteration 681, loss = 0.22657898\n",
      "Iteration 682, loss = 0.22631268\n",
      "Iteration 683, loss = 0.22609852\n",
      "Iteration 684, loss = 0.22586606\n",
      "Iteration 685, loss = 0.22572065\n",
      "Iteration 686, loss = 0.22553927\n",
      "Iteration 687, loss = 0.22554289\n",
      "Iteration 688, loss = 0.22537117\n",
      "Iteration 689, loss = 0.22528882\n",
      "Iteration 690, loss = 0.22517551\n",
      "Iteration 691, loss = 0.22497166\n",
      "Iteration 692, loss = 0.22476723\n",
      "Iteration 693, loss = 0.22446179\n",
      "Iteration 694, loss = 0.22423136\n",
      "Iteration 695, loss = 0.22391872\n",
      "Iteration 696, loss = 0.22364384\n",
      "Iteration 697, loss = 0.22346479\n",
      "Iteration 698, loss = 0.22322112\n",
      "Iteration 699, loss = 0.22306642\n",
      "Iteration 700, loss = 0.22285619\n",
      "Iteration 701, loss = 0.22273824\n",
      "Iteration 702, loss = 0.22256370\n",
      "Iteration 703, loss = 0.22248239\n",
      "Iteration 704, loss = 0.22237206\n",
      "Iteration 705, loss = 0.22231317\n",
      "Iteration 706, loss = 0.22213279\n",
      "Iteration 707, loss = 0.22194930\n",
      "Iteration 708, loss = 0.22176124\n",
      "Iteration 709, loss = 0.22158593\n",
      "Iteration 710, loss = 0.22138737\n",
      "Iteration 711, loss = 0.22117930\n",
      "Iteration 712, loss = 0.22097183\n",
      "Iteration 713, loss = 0.22084357\n",
      "Iteration 714, loss = 0.22067413\n",
      "Iteration 715, loss = 0.22041734\n",
      "Iteration 716, loss = 0.22019069\n",
      "Iteration 717, loss = 0.21996027\n",
      "Iteration 718, loss = 0.21975103\n",
      "Iteration 719, loss = 0.21966270\n",
      "Iteration 720, loss = 0.21944335\n",
      "Iteration 721, loss = 0.21929512\n",
      "Iteration 722, loss = 0.21910915\n",
      "Iteration 723, loss = 0.21895431\n",
      "Iteration 724, loss = 0.21876727\n",
      "Iteration 725, loss = 0.21856736\n",
      "Iteration 726, loss = 0.21840667\n",
      "Iteration 727, loss = 0.21822273\n",
      "Iteration 728, loss = 0.21803137\n",
      "Iteration 729, loss = 0.21786582\n",
      "Iteration 730, loss = 0.21765948\n",
      "Iteration 731, loss = 0.21750523\n",
      "Iteration 732, loss = 0.21730074\n",
      "Iteration 733, loss = 0.21713080\n",
      "Iteration 734, loss = 0.21695500\n",
      "Iteration 735, loss = 0.21679858\n",
      "Iteration 736, loss = 0.21652341\n",
      "Iteration 737, loss = 0.21632581\n",
      "Iteration 738, loss = 0.21604276\n",
      "Iteration 739, loss = 0.21584917\n",
      "Iteration 740, loss = 0.21572386\n",
      "Iteration 741, loss = 0.21554258\n",
      "Iteration 742, loss = 0.21536780\n",
      "Iteration 743, loss = 0.21516532\n",
      "Iteration 744, loss = 0.21503558\n",
      "Iteration 745, loss = 0.21487792\n",
      "Iteration 746, loss = 0.21463480\n",
      "Iteration 747, loss = 0.21435514\n",
      "Iteration 748, loss = 0.21418190\n",
      "Iteration 749, loss = 0.21392046\n",
      "Iteration 750, loss = 0.21373031\n",
      "Iteration 751, loss = 0.21354451\n",
      "Iteration 752, loss = 0.21334668\n",
      "Iteration 753, loss = 0.21327830\n",
      "Iteration 754, loss = 0.21305725\n",
      "Iteration 755, loss = 0.21290752\n",
      "Iteration 756, loss = 0.21274030\n",
      "Iteration 757, loss = 0.21260434\n",
      "Iteration 758, loss = 0.21259100\n",
      "Iteration 759, loss = 0.21251921\n",
      "Iteration 760, loss = 0.21257916\n",
      "Iteration 761, loss = 0.21248221\n",
      "Iteration 762, loss = 0.21230807\n",
      "Iteration 763, loss = 0.21211190\n",
      "Iteration 764, loss = 0.21189177\n",
      "Iteration 765, loss = 0.21161810\n",
      "Iteration 766, loss = 0.21138397\n",
      "Iteration 767, loss = 0.21111474\n",
      "Iteration 768, loss = 0.21088731\n",
      "Iteration 769, loss = 0.21067969\n",
      "Iteration 770, loss = 0.21042816\n",
      "Iteration 771, loss = 0.21020509\n",
      "Iteration 772, loss = 0.20995349\n",
      "Iteration 773, loss = 0.20972947\n",
      "Iteration 774, loss = 0.20948390\n",
      "Iteration 775, loss = 0.20926683\n",
      "Iteration 776, loss = 0.20902902\n",
      "Iteration 777, loss = 0.20884974\n",
      "Iteration 778, loss = 0.20864642\n",
      "Iteration 779, loss = 0.20844477\n",
      "Iteration 780, loss = 0.20831353\n",
      "Iteration 781, loss = 0.20815971\n",
      "Iteration 782, loss = 0.20804520\n",
      "Iteration 783, loss = 0.20803796\n",
      "Iteration 784, loss = 0.20790928\n",
      "Iteration 785, loss = 0.20773149\n",
      "Iteration 786, loss = 0.20748217\n",
      "Iteration 787, loss = 0.20733135\n",
      "Iteration 788, loss = 0.20711161\n",
      "Iteration 789, loss = 0.20685971\n",
      "Iteration 790, loss = 0.20668865\n",
      "Iteration 791, loss = 0.20650710\n",
      "Iteration 792, loss = 0.20634051\n",
      "Iteration 793, loss = 0.20620741\n",
      "Iteration 794, loss = 0.20615001\n",
      "Iteration 795, loss = 0.20604587\n",
      "Iteration 796, loss = 0.20600077\n",
      "Iteration 797, loss = 0.20579369\n",
      "Iteration 798, loss = 0.20558968\n",
      "Iteration 799, loss = 0.20543943\n",
      "Iteration 800, loss = 0.20525710\n",
      "Iteration 801, loss = 0.20507007\n",
      "Iteration 802, loss = 0.20488507\n",
      "Iteration 803, loss = 0.20467798\n",
      "Iteration 804, loss = 0.20444659\n",
      "Iteration 805, loss = 0.20425715\n",
      "Iteration 806, loss = 0.20404786\n",
      "Iteration 807, loss = 0.20383914\n",
      "Iteration 808, loss = 0.20364516\n",
      "Iteration 809, loss = 0.20343200\n",
      "Iteration 810, loss = 0.20337170\n",
      "Iteration 811, loss = 0.20312661\n",
      "Iteration 812, loss = 0.20296274\n",
      "Iteration 813, loss = 0.20283275\n",
      "Iteration 814, loss = 0.20268966\n",
      "Iteration 815, loss = 0.20245809\n",
      "Iteration 816, loss = 0.20231240\n",
      "Iteration 817, loss = 0.20214664\n",
      "Iteration 818, loss = 0.20201363\n",
      "Iteration 819, loss = 0.20187186\n",
      "Iteration 820, loss = 0.20168905\n",
      "Iteration 821, loss = 0.20157932\n",
      "Iteration 822, loss = 0.20147423\n",
      "Iteration 823, loss = 0.20134106\n",
      "Iteration 824, loss = 0.20130752\n",
      "Iteration 825, loss = 0.20118067\n",
      "Iteration 826, loss = 0.20104720\n",
      "Iteration 827, loss = 0.20089458\n",
      "Iteration 828, loss = 0.20077135\n",
      "Iteration 829, loss = 0.20070351\n",
      "Iteration 830, loss = 0.20057761\n",
      "Iteration 831, loss = 0.20044123\n",
      "Iteration 832, loss = 0.20031929\n",
      "Iteration 833, loss = 0.20015609\n",
      "Iteration 834, loss = 0.19994732\n",
      "Iteration 835, loss = 0.19979972\n",
      "Iteration 836, loss = 0.19964237\n",
      "Iteration 837, loss = 0.19950513\n",
      "Iteration 838, loss = 0.19931000\n",
      "Iteration 839, loss = 0.19911706\n",
      "Iteration 840, loss = 0.19900474\n",
      "Iteration 841, loss = 0.19905104\n",
      "Iteration 842, loss = 0.19892949\n",
      "Iteration 843, loss = 0.19888824\n",
      "Iteration 844, loss = 0.19894215\n",
      "Iteration 845, loss = 0.19897072\n",
      "Iteration 846, loss = 0.19891580\n",
      "Iteration 847, loss = 0.19888577\n",
      "Iteration 848, loss = 0.19882040\n",
      "Iteration 849, loss = 0.19884294\n",
      "Iteration 850, loss = 0.19870337\n",
      "Iteration 851, loss = 0.19856050\n",
      "Iteration 852, loss = 0.19838033\n",
      "Iteration 853, loss = 0.19821833\n",
      "Iteration 854, loss = 0.19817515\n",
      "Iteration 855, loss = 0.19805161\n",
      "Iteration 856, loss = 0.19776115\n",
      "Iteration 857, loss = 0.19739482\n",
      "Iteration 858, loss = 0.19702211\n",
      "Iteration 859, loss = 0.19664180\n",
      "Iteration 860, loss = 0.19635207\n",
      "Iteration 861, loss = 0.19619925\n",
      "Iteration 862, loss = 0.19594452\n",
      "Iteration 863, loss = 0.19579208\n",
      "Iteration 864, loss = 0.19568380\n",
      "Iteration 865, loss = 0.19572085\n",
      "Iteration 866, loss = 0.19571435\n",
      "Iteration 867, loss = 0.19577041\n",
      "Iteration 868, loss = 0.19565867\n",
      "Iteration 869, loss = 0.19554943\n",
      "Iteration 870, loss = 0.19546264\n",
      "Iteration 871, loss = 0.19529067\n",
      "Iteration 872, loss = 0.19514629\n",
      "Iteration 873, loss = 0.19501341\n",
      "Iteration 874, loss = 0.19479355\n",
      "Iteration 875, loss = 0.19459219\n",
      "Iteration 876, loss = 0.19444068\n",
      "Iteration 877, loss = 0.19429758\n",
      "Iteration 878, loss = 0.19422254\n",
      "Iteration 879, loss = 0.19400727\n",
      "Iteration 880, loss = 0.19381077\n",
      "Iteration 881, loss = 0.19363831\n",
      "Iteration 882, loss = 0.19342078\n",
      "Iteration 883, loss = 0.19321534\n",
      "Iteration 884, loss = 0.19300969\n",
      "Iteration 885, loss = 0.19276212\n",
      "Iteration 886, loss = 0.19270800\n",
      "Iteration 887, loss = 0.19258418\n",
      "Iteration 888, loss = 0.19243243\n",
      "Iteration 889, loss = 0.19226385\n",
      "Iteration 890, loss = 0.19221339\n",
      "Iteration 891, loss = 0.19203782\n",
      "Iteration 892, loss = 0.19188046\n",
      "Iteration 893, loss = 0.19176068\n",
      "Iteration 894, loss = 0.19160237\n",
      "Iteration 895, loss = 0.19175003\n",
      "Iteration 896, loss = 0.19185657\n",
      "Iteration 897, loss = 0.19187060\n",
      "Iteration 898, loss = 0.19183369\n",
      "Iteration 899, loss = 0.19169618\n",
      "Iteration 900, loss = 0.19149894\n",
      "Iteration 901, loss = 0.19130472\n",
      "Iteration 902, loss = 0.19111334\n",
      "Iteration 903, loss = 0.19093191\n",
      "Iteration 904, loss = 0.19077013\n",
      "Iteration 905, loss = 0.19048016\n",
      "Iteration 906, loss = 0.19024856\n",
      "Iteration 907, loss = 0.19002097\n",
      "Iteration 908, loss = 0.18989339\n",
      "Iteration 909, loss = 0.18966273\n",
      "Iteration 910, loss = 0.18950186\n",
      "Iteration 911, loss = 0.18941066\n",
      "Iteration 912, loss = 0.18921648\n",
      "Iteration 913, loss = 0.18908410\n",
      "Iteration 914, loss = 0.18891988\n",
      "Iteration 915, loss = 0.18875786\n",
      "Iteration 916, loss = 0.18856678\n",
      "Iteration 917, loss = 0.18840193\n",
      "Iteration 918, loss = 0.18820656\n",
      "Iteration 919, loss = 0.18800889\n",
      "Iteration 920, loss = 0.18785935\n",
      "Iteration 921, loss = 0.18770039\n",
      "Iteration 922, loss = 0.18759460\n",
      "Iteration 923, loss = 0.18744923\n",
      "Iteration 924, loss = 0.18729103\n",
      "Iteration 925, loss = 0.18710068\n",
      "Iteration 926, loss = 0.18694186\n",
      "Iteration 927, loss = 0.18675879\n",
      "Iteration 928, loss = 0.18657808\n",
      "Iteration 929, loss = 0.18646170\n",
      "Iteration 930, loss = 0.18634236\n",
      "Iteration 931, loss = 0.18623276\n",
      "Iteration 932, loss = 0.18603368\n",
      "Iteration 933, loss = 0.18589587\n",
      "Iteration 934, loss = 0.18574415\n",
      "Iteration 935, loss = 0.18562828\n",
      "Iteration 936, loss = 0.18556752\n",
      "Iteration 937, loss = 0.18540814\n",
      "Iteration 938, loss = 0.18522531\n",
      "Iteration 939, loss = 0.18510407\n",
      "Iteration 940, loss = 0.18496294\n",
      "Iteration 941, loss = 0.18487537\n",
      "Iteration 942, loss = 0.18478873\n",
      "Iteration 943, loss = 0.18466672\n",
      "Iteration 944, loss = 0.18457691\n",
      "Iteration 945, loss = 0.18445112\n",
      "Iteration 946, loss = 0.18436754\n",
      "Iteration 947, loss = 0.18431728\n",
      "Iteration 948, loss = 0.18424407\n",
      "Iteration 949, loss = 0.18419975\n",
      "Iteration 950, loss = 0.18405747\n",
      "Iteration 951, loss = 0.18391905\n",
      "Iteration 952, loss = 0.18383374\n",
      "Iteration 953, loss = 0.18369752\n",
      "Iteration 954, loss = 0.18357297\n",
      "Iteration 955, loss = 0.18347151\n",
      "Iteration 956, loss = 0.18352767\n",
      "Iteration 957, loss = 0.18352441\n",
      "Iteration 958, loss = 0.18361266\n",
      "Iteration 959, loss = 0.18367151\n",
      "Iteration 960, loss = 0.18376835\n",
      "Iteration 961, loss = 0.18383724\n",
      "Iteration 962, loss = 0.18388860\n",
      "Iteration 963, loss = 0.18370165\n",
      "Iteration 964, loss = 0.18342995\n",
      "Iteration 965, loss = 0.18311228\n",
      "Iteration 966, loss = 0.18278575\n",
      "Iteration 967, loss = 0.18257139\n",
      "Iteration 968, loss = 0.18232661\n",
      "Iteration 969, loss = 0.18212051\n",
      "Iteration 970, loss = 0.18198694\n",
      "Iteration 971, loss = 0.18196084\n",
      "Iteration 972, loss = 0.18190787\n",
      "Iteration 973, loss = 0.18185676\n",
      "Iteration 974, loss = 0.18178151\n",
      "Iteration 975, loss = 0.18168910\n",
      "Iteration 976, loss = 0.18151331\n",
      "Iteration 977, loss = 0.18123535\n",
      "Iteration 978, loss = 0.18098980\n",
      "Iteration 979, loss = 0.18072216\n",
      "Iteration 980, loss = 0.18055420\n",
      "Iteration 981, loss = 0.18037967\n",
      "Iteration 982, loss = 0.18016020\n",
      "Iteration 983, loss = 0.18004245\n",
      "Iteration 984, loss = 0.17989938\n",
      "Iteration 985, loss = 0.17977994\n",
      "Iteration 986, loss = 0.17965537\n",
      "Iteration 987, loss = 0.17958334\n",
      "Iteration 988, loss = 0.17947751\n",
      "Iteration 989, loss = 0.17939745\n",
      "Iteration 990, loss = 0.17923896\n",
      "Iteration 991, loss = 0.17915328\n",
      "Iteration 992, loss = 0.17906408\n",
      "Iteration 993, loss = 0.17902366\n",
      "Iteration 994, loss = 0.17898946\n",
      "Iteration 995, loss = 0.17890832\n",
      "Iteration 996, loss = 0.17884280\n",
      "Iteration 997, loss = 0.17874891\n",
      "Iteration 998, loss = 0.17870138\n",
      "Iteration 999, loss = 0.17869818\n",
      "Iteration 1000, loss = 0.17873751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier2 = MLPClassifier(max_iter=1000,activation='relu', verbose=True, hidden_layer_sizes=(15,),random_state=1)\n",
    "mlpClassifier2.fit(X_train, y_train)\n",
    "y_pred2 = mlpClassifier2.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c546e48c",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c0455a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56989247311827"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059da8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde34c2bf40>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 2')\n",
    "plt.plot(range(len(mlpClassifier2.loss_curve_)),mlpClassifier2.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2906563f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix2 = metrics.confusion_matrix(y_test,y_pred2)\n",
    "sns.heatmap(pd.DataFrame(confMatrix2),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f82e7b2",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79be464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04413811\n",
      "Iteration 2, loss = 1.03103132\n",
      "Iteration 3, loss = 1.01869998\n",
      "Iteration 4, loss = 1.00663400\n",
      "Iteration 5, loss = 0.99494172\n",
      "Iteration 6, loss = 0.98332969\n",
      "Iteration 7, loss = 0.97231050\n",
      "Iteration 8, loss = 0.96151232\n",
      "Iteration 9, loss = 0.95075399\n",
      "Iteration 10, loss = 0.94039067\n",
      "Iteration 11, loss = 0.92991170\n",
      "Iteration 12, loss = 0.91993123\n",
      "Iteration 13, loss = 0.90992657\n",
      "Iteration 14, loss = 0.90040501\n",
      "Iteration 15, loss = 0.89084888\n",
      "Iteration 16, loss = 0.88173495\n",
      "Iteration 17, loss = 0.87270159\n",
      "Iteration 18, loss = 0.86406929\n",
      "Iteration 19, loss = 0.85548709\n",
      "Iteration 20, loss = 0.84697643\n",
      "Iteration 21, loss = 0.83893465\n",
      "Iteration 22, loss = 0.83056830\n",
      "Iteration 23, loss = 0.82269022\n",
      "Iteration 24, loss = 0.81469552\n",
      "Iteration 25, loss = 0.80695304\n",
      "Iteration 26, loss = 0.79941992\n",
      "Iteration 27, loss = 0.79219324\n",
      "Iteration 28, loss = 0.78504727\n",
      "Iteration 29, loss = 0.77801535\n",
      "Iteration 30, loss = 0.77109799\n",
      "Iteration 31, loss = 0.76446092\n",
      "Iteration 32, loss = 0.75755139\n",
      "Iteration 33, loss = 0.75133075\n",
      "Iteration 34, loss = 0.74522786\n",
      "Iteration 35, loss = 0.73942968\n",
      "Iteration 36, loss = 0.73381133\n",
      "Iteration 37, loss = 0.72854403\n",
      "Iteration 38, loss = 0.72324233\n",
      "Iteration 39, loss = 0.71829853\n",
      "Iteration 40, loss = 0.71319193\n",
      "Iteration 41, loss = 0.70815346\n",
      "Iteration 42, loss = 0.70347739\n",
      "Iteration 43, loss = 0.69863151\n",
      "Iteration 44, loss = 0.69424932\n",
      "Iteration 45, loss = 0.68985073\n",
      "Iteration 46, loss = 0.68573506\n",
      "Iteration 47, loss = 0.68169639\n",
      "Iteration 48, loss = 0.67777333\n",
      "Iteration 49, loss = 0.67393390\n",
      "Iteration 50, loss = 0.67026835\n",
      "Iteration 51, loss = 0.66673165\n",
      "Iteration 52, loss = 0.66319840\n",
      "Iteration 53, loss = 0.65963657\n",
      "Iteration 54, loss = 0.65593753\n",
      "Iteration 55, loss = 0.65235112\n",
      "Iteration 56, loss = 0.64868017\n",
      "Iteration 57, loss = 0.64518624\n",
      "Iteration 58, loss = 0.64166608\n",
      "Iteration 59, loss = 0.63825409\n",
      "Iteration 60, loss = 0.63502326\n",
      "Iteration 61, loss = 0.63172809\n",
      "Iteration 62, loss = 0.62858623\n",
      "Iteration 63, loss = 0.62542330\n",
      "Iteration 64, loss = 0.62218807\n",
      "Iteration 65, loss = 0.61897260\n",
      "Iteration 66, loss = 0.61578821\n",
      "Iteration 67, loss = 0.61266514\n",
      "Iteration 68, loss = 0.60963761\n",
      "Iteration 69, loss = 0.60673266\n",
      "Iteration 70, loss = 0.60398261\n",
      "Iteration 71, loss = 0.60123848\n",
      "Iteration 72, loss = 0.59858833\n",
      "Iteration 73, loss = 0.59606141\n",
      "Iteration 74, loss = 0.59351256\n",
      "Iteration 75, loss = 0.59102942\n",
      "Iteration 76, loss = 0.58875570\n",
      "Iteration 77, loss = 0.58637554\n",
      "Iteration 78, loss = 0.58414020\n",
      "Iteration 79, loss = 0.58189289\n",
      "Iteration 80, loss = 0.57964092\n",
      "Iteration 81, loss = 0.57754067\n",
      "Iteration 82, loss = 0.57540124\n",
      "Iteration 83, loss = 0.57338592\n",
      "Iteration 84, loss = 0.57122477\n",
      "Iteration 85, loss = 0.56918114\n",
      "Iteration 86, loss = 0.56708426\n",
      "Iteration 87, loss = 0.56500789\n",
      "Iteration 88, loss = 0.56298307\n",
      "Iteration 89, loss = 0.56104285\n",
      "Iteration 90, loss = 0.55907935\n",
      "Iteration 91, loss = 0.55726979\n",
      "Iteration 92, loss = 0.55540499\n",
      "Iteration 93, loss = 0.55356998\n",
      "Iteration 94, loss = 0.55173578\n",
      "Iteration 95, loss = 0.54990206\n",
      "Iteration 96, loss = 0.54808739\n",
      "Iteration 97, loss = 0.54623161\n",
      "Iteration 98, loss = 0.54442123\n",
      "Iteration 99, loss = 0.54259502\n",
      "Iteration 100, loss = 0.54077832\n",
      "Iteration 101, loss = 0.53891650\n",
      "Iteration 102, loss = 0.53711159\n",
      "Iteration 103, loss = 0.53525446\n",
      "Iteration 104, loss = 0.53343365\n",
      "Iteration 105, loss = 0.53153764\n",
      "Iteration 106, loss = 0.52972211\n",
      "Iteration 107, loss = 0.52794455\n",
      "Iteration 108, loss = 0.52618193\n",
      "Iteration 109, loss = 0.52445156\n",
      "Iteration 110, loss = 0.52282581\n",
      "Iteration 111, loss = 0.52115292\n",
      "Iteration 112, loss = 0.51960123\n",
      "Iteration 113, loss = 0.51801359\n",
      "Iteration 114, loss = 0.51655833\n",
      "Iteration 115, loss = 0.51501233\n",
      "Iteration 116, loss = 0.51356829\n",
      "Iteration 117, loss = 0.51210320\n",
      "Iteration 118, loss = 0.51065990\n",
      "Iteration 119, loss = 0.50923246\n",
      "Iteration 120, loss = 0.50784716\n",
      "Iteration 121, loss = 0.50652062\n",
      "Iteration 122, loss = 0.50518556\n",
      "Iteration 123, loss = 0.50384939\n",
      "Iteration 124, loss = 0.50259394\n",
      "Iteration 125, loss = 0.50135713\n",
      "Iteration 126, loss = 0.50010765\n",
      "Iteration 127, loss = 0.49886549\n",
      "Iteration 128, loss = 0.49765062\n",
      "Iteration 129, loss = 0.49638909\n",
      "Iteration 130, loss = 0.49517984\n",
      "Iteration 131, loss = 0.49402926\n",
      "Iteration 132, loss = 0.49283931\n",
      "Iteration 133, loss = 0.49167783\n",
      "Iteration 134, loss = 0.49052017\n",
      "Iteration 135, loss = 0.48941016\n",
      "Iteration 136, loss = 0.48837189\n",
      "Iteration 137, loss = 0.48732650\n",
      "Iteration 138, loss = 0.48631465\n",
      "Iteration 139, loss = 0.48525648\n",
      "Iteration 140, loss = 0.48418897\n",
      "Iteration 141, loss = 0.48314798\n",
      "Iteration 142, loss = 0.48203744\n",
      "Iteration 143, loss = 0.48085978\n",
      "Iteration 144, loss = 0.47971007\n",
      "Iteration 145, loss = 0.47860172\n",
      "Iteration 146, loss = 0.47740649\n",
      "Iteration 147, loss = 0.47633246\n",
      "Iteration 148, loss = 0.47520063\n",
      "Iteration 149, loss = 0.47416616\n",
      "Iteration 150, loss = 0.47312207\n",
      "Iteration 151, loss = 0.47204949\n",
      "Iteration 152, loss = 0.47100915\n",
      "Iteration 153, loss = 0.46999875\n",
      "Iteration 154, loss = 0.46896576\n",
      "Iteration 155, loss = 0.46798240\n",
      "Iteration 156, loss = 0.46695593\n",
      "Iteration 157, loss = 0.46592405\n",
      "Iteration 158, loss = 0.46492240\n",
      "Iteration 159, loss = 0.46390591\n",
      "Iteration 160, loss = 0.46289421\n",
      "Iteration 161, loss = 0.46194594\n",
      "Iteration 162, loss = 0.46097513\n",
      "Iteration 163, loss = 0.46007008\n",
      "Iteration 164, loss = 0.45915884\n",
      "Iteration 165, loss = 0.45825314\n",
      "Iteration 166, loss = 0.45732971\n",
      "Iteration 167, loss = 0.45637920\n",
      "Iteration 168, loss = 0.45540081\n",
      "Iteration 169, loss = 0.45443426\n",
      "Iteration 170, loss = 0.45350281\n",
      "Iteration 171, loss = 0.45258482\n",
      "Iteration 172, loss = 0.45172020\n",
      "Iteration 173, loss = 0.45082705\n",
      "Iteration 174, loss = 0.44990659\n",
      "Iteration 175, loss = 0.44899913\n",
      "Iteration 176, loss = 0.44812602\n",
      "Iteration 177, loss = 0.44717259\n",
      "Iteration 178, loss = 0.44630118\n",
      "Iteration 179, loss = 0.44541684\n",
      "Iteration 180, loss = 0.44455581\n",
      "Iteration 181, loss = 0.44368131\n",
      "Iteration 182, loss = 0.44282139\n",
      "Iteration 183, loss = 0.44195525\n",
      "Iteration 184, loss = 0.44109302\n",
      "Iteration 185, loss = 0.44019623\n",
      "Iteration 186, loss = 0.43933055\n",
      "Iteration 187, loss = 0.43848376\n",
      "Iteration 188, loss = 0.43758799\n",
      "Iteration 189, loss = 0.43672857\n",
      "Iteration 190, loss = 0.43584162\n",
      "Iteration 191, loss = 0.43498136\n",
      "Iteration 192, loss = 0.43412893\n",
      "Iteration 193, loss = 0.43325760\n",
      "Iteration 194, loss = 0.43244315\n",
      "Iteration 195, loss = 0.43161339\n",
      "Iteration 196, loss = 0.43077717\n",
      "Iteration 197, loss = 0.42994547\n",
      "Iteration 198, loss = 0.42910711\n",
      "Iteration 199, loss = 0.42831363\n",
      "Iteration 200, loss = 0.42754974\n",
      "Iteration 201, loss = 0.42674037\n",
      "Iteration 202, loss = 0.42596797\n",
      "Iteration 203, loss = 0.42519047\n",
      "Iteration 204, loss = 0.42444253\n",
      "Iteration 205, loss = 0.42368052\n",
      "Iteration 206, loss = 0.42291605\n",
      "Iteration 207, loss = 0.42218743\n",
      "Iteration 208, loss = 0.42140113\n",
      "Iteration 209, loss = 0.42060484\n",
      "Iteration 210, loss = 0.41979703\n",
      "Iteration 211, loss = 0.41898458\n",
      "Iteration 212, loss = 0.41825862\n",
      "Iteration 213, loss = 0.41746580\n",
      "Iteration 214, loss = 0.41666035\n",
      "Iteration 215, loss = 0.41583397\n",
      "Iteration 216, loss = 0.41508289\n",
      "Iteration 217, loss = 0.41428190\n",
      "Iteration 218, loss = 0.41348972\n",
      "Iteration 219, loss = 0.41274403\n",
      "Iteration 220, loss = 0.41197087\n",
      "Iteration 221, loss = 0.41119507\n",
      "Iteration 222, loss = 0.41040640\n",
      "Iteration 223, loss = 0.40958609\n",
      "Iteration 224, loss = 0.40882145\n",
      "Iteration 225, loss = 0.40807058\n",
      "Iteration 226, loss = 0.40727186\n",
      "Iteration 227, loss = 0.40647852\n",
      "Iteration 228, loss = 0.40567712\n",
      "Iteration 229, loss = 0.40486214\n",
      "Iteration 230, loss = 0.40409042\n",
      "Iteration 231, loss = 0.40330185\n",
      "Iteration 232, loss = 0.40250854\n",
      "Iteration 233, loss = 0.40172373\n",
      "Iteration 234, loss = 0.40100952\n",
      "Iteration 235, loss = 0.40027134\n",
      "Iteration 236, loss = 0.39957987\n",
      "Iteration 237, loss = 0.39887804\n",
      "Iteration 238, loss = 0.39824053\n",
      "Iteration 239, loss = 0.39762529\n",
      "Iteration 240, loss = 0.39688689\n",
      "Iteration 241, loss = 0.39615874\n",
      "Iteration 242, loss = 0.39537649\n",
      "Iteration 243, loss = 0.39464139\n",
      "Iteration 244, loss = 0.39386639\n",
      "Iteration 245, loss = 0.39310522\n",
      "Iteration 246, loss = 0.39234644\n",
      "Iteration 247, loss = 0.39156755\n",
      "Iteration 248, loss = 0.39083356\n",
      "Iteration 249, loss = 0.39004741\n",
      "Iteration 250, loss = 0.38932453\n",
      "Iteration 251, loss = 0.38857346\n",
      "Iteration 252, loss = 0.38790191\n",
      "Iteration 253, loss = 0.38717170\n",
      "Iteration 254, loss = 0.38648055\n",
      "Iteration 255, loss = 0.38578145\n",
      "Iteration 256, loss = 0.38504847\n",
      "Iteration 257, loss = 0.38432281\n",
      "Iteration 258, loss = 0.38362155\n",
      "Iteration 259, loss = 0.38290476\n",
      "Iteration 260, loss = 0.38216748\n",
      "Iteration 261, loss = 0.38148659\n",
      "Iteration 262, loss = 0.38081177\n",
      "Iteration 263, loss = 0.38012095\n",
      "Iteration 264, loss = 0.37948531\n",
      "Iteration 265, loss = 0.37882576\n",
      "Iteration 266, loss = 0.37819796\n",
      "Iteration 267, loss = 0.37756563\n",
      "Iteration 268, loss = 0.37694196\n",
      "Iteration 269, loss = 0.37631076\n",
      "Iteration 270, loss = 0.37569019\n",
      "Iteration 271, loss = 0.37504547\n",
      "Iteration 272, loss = 0.37441080\n",
      "Iteration 273, loss = 0.37376330\n",
      "Iteration 274, loss = 0.37312396\n",
      "Iteration 275, loss = 0.37247312\n",
      "Iteration 276, loss = 0.37183539\n",
      "Iteration 277, loss = 0.37120360\n",
      "Iteration 278, loss = 0.37057281\n",
      "Iteration 279, loss = 0.36992417\n",
      "Iteration 280, loss = 0.36919821\n",
      "Iteration 281, loss = 0.36856217\n",
      "Iteration 282, loss = 0.36790510\n",
      "Iteration 283, loss = 0.36730167\n",
      "Iteration 284, loss = 0.36663850\n",
      "Iteration 285, loss = 0.36601718\n",
      "Iteration 286, loss = 0.36537104\n",
      "Iteration 287, loss = 0.36481053\n",
      "Iteration 288, loss = 0.36420321\n",
      "Iteration 289, loss = 0.36361580\n",
      "Iteration 290, loss = 0.36306186\n",
      "Iteration 291, loss = 0.36248836\n",
      "Iteration 292, loss = 0.36192387\n",
      "Iteration 293, loss = 0.36136213\n",
      "Iteration 294, loss = 0.36078365\n",
      "Iteration 295, loss = 0.36022273\n",
      "Iteration 296, loss = 0.35970464\n",
      "Iteration 297, loss = 0.35914167\n",
      "Iteration 298, loss = 0.35856636\n",
      "Iteration 299, loss = 0.35801937\n",
      "Iteration 300, loss = 0.35742651\n",
      "Iteration 301, loss = 0.35688192\n",
      "Iteration 302, loss = 0.35628363\n",
      "Iteration 303, loss = 0.35571017\n",
      "Iteration 304, loss = 0.35513293\n",
      "Iteration 305, loss = 0.35451656\n",
      "Iteration 306, loss = 0.35399798\n",
      "Iteration 307, loss = 0.35346286\n",
      "Iteration 308, loss = 0.35290318\n",
      "Iteration 309, loss = 0.35238240\n",
      "Iteration 310, loss = 0.35183059\n",
      "Iteration 311, loss = 0.35128039\n",
      "Iteration 312, loss = 0.35068234\n",
      "Iteration 313, loss = 0.35008561\n",
      "Iteration 314, loss = 0.34952582\n",
      "Iteration 315, loss = 0.34894324\n",
      "Iteration 316, loss = 0.34837519\n",
      "Iteration 317, loss = 0.34781974\n",
      "Iteration 318, loss = 0.34724646\n",
      "Iteration 319, loss = 0.34674310\n",
      "Iteration 320, loss = 0.34624773\n",
      "Iteration 321, loss = 0.34575828\n",
      "Iteration 322, loss = 0.34538621\n",
      "Iteration 323, loss = 0.34486894\n",
      "Iteration 324, loss = 0.34438776\n",
      "Iteration 325, loss = 0.34393244\n",
      "Iteration 326, loss = 0.34350733\n",
      "Iteration 327, loss = 0.34308542\n",
      "Iteration 328, loss = 0.34263190\n",
      "Iteration 329, loss = 0.34209419\n",
      "Iteration 330, loss = 0.34155530\n",
      "Iteration 331, loss = 0.34098420\n",
      "Iteration 332, loss = 0.34038598\n",
      "Iteration 333, loss = 0.33989244\n",
      "Iteration 334, loss = 0.33929193\n",
      "Iteration 335, loss = 0.33879565\n",
      "Iteration 336, loss = 0.33833017\n",
      "Iteration 337, loss = 0.33782698\n",
      "Iteration 338, loss = 0.33727999\n",
      "Iteration 339, loss = 0.33677711\n",
      "Iteration 340, loss = 0.33624394\n",
      "Iteration 341, loss = 0.33572685\n",
      "Iteration 342, loss = 0.33516522\n",
      "Iteration 343, loss = 0.33462144\n",
      "Iteration 344, loss = 0.33406124\n",
      "Iteration 345, loss = 0.33355002\n",
      "Iteration 346, loss = 0.33301828\n",
      "Iteration 347, loss = 0.33248983\n",
      "Iteration 348, loss = 0.33197322\n",
      "Iteration 349, loss = 0.33150541\n",
      "Iteration 350, loss = 0.33105068\n",
      "Iteration 351, loss = 0.33062483\n",
      "Iteration 352, loss = 0.33016275\n",
      "Iteration 353, loss = 0.32973052\n",
      "Iteration 354, loss = 0.32929261\n",
      "Iteration 355, loss = 0.32884398\n",
      "Iteration 356, loss = 0.32828563\n",
      "Iteration 357, loss = 0.32774644\n",
      "Iteration 358, loss = 0.32720678\n",
      "Iteration 359, loss = 0.32672016\n",
      "Iteration 360, loss = 0.32617896\n",
      "Iteration 361, loss = 0.32562692\n",
      "Iteration 362, loss = 0.32525104\n",
      "Iteration 363, loss = 0.32468914\n",
      "Iteration 364, loss = 0.32424382\n",
      "Iteration 365, loss = 0.32384788\n",
      "Iteration 366, loss = 0.32341985\n",
      "Iteration 367, loss = 0.32305855\n",
      "Iteration 368, loss = 0.32263203\n",
      "Iteration 369, loss = 0.32222082\n",
      "Iteration 370, loss = 0.32176456\n",
      "Iteration 371, loss = 0.32129509\n",
      "Iteration 372, loss = 0.32081750\n",
      "Iteration 373, loss = 0.32033119\n",
      "Iteration 374, loss = 0.31992593\n",
      "Iteration 375, loss = 0.31944882\n",
      "Iteration 376, loss = 0.31901635\n",
      "Iteration 377, loss = 0.31859910\n",
      "Iteration 378, loss = 0.31814277\n",
      "Iteration 379, loss = 0.31773458\n",
      "Iteration 380, loss = 0.31728958\n",
      "Iteration 381, loss = 0.31683155\n",
      "Iteration 382, loss = 0.31639803\n",
      "Iteration 383, loss = 0.31598532\n",
      "Iteration 384, loss = 0.31549077\n",
      "Iteration 385, loss = 0.31504701\n",
      "Iteration 386, loss = 0.31459937\n",
      "Iteration 387, loss = 0.31413220\n",
      "Iteration 388, loss = 0.31367678\n",
      "Iteration 389, loss = 0.31322110\n",
      "Iteration 390, loss = 0.31280564\n",
      "Iteration 391, loss = 0.31247283\n",
      "Iteration 392, loss = 0.31207352\n",
      "Iteration 393, loss = 0.31165995\n",
      "Iteration 394, loss = 0.31131254\n",
      "Iteration 395, loss = 0.31098867\n",
      "Iteration 396, loss = 0.31068239\n",
      "Iteration 397, loss = 0.31032664\n",
      "Iteration 398, loss = 0.30997427\n",
      "Iteration 399, loss = 0.30957908\n",
      "Iteration 400, loss = 0.30921171\n",
      "Iteration 401, loss = 0.30889621\n",
      "Iteration 402, loss = 0.30857988\n",
      "Iteration 403, loss = 0.30826891\n",
      "Iteration 404, loss = 0.30787081\n",
      "Iteration 405, loss = 0.30751619\n",
      "Iteration 406, loss = 0.30711878\n",
      "Iteration 407, loss = 0.30668222\n",
      "Iteration 408, loss = 0.30620949\n",
      "Iteration 409, loss = 0.30584859\n",
      "Iteration 410, loss = 0.30535672\n",
      "Iteration 411, loss = 0.30507537\n",
      "Iteration 412, loss = 0.30459601\n",
      "Iteration 413, loss = 0.30426950\n",
      "Iteration 414, loss = 0.30394035\n",
      "Iteration 415, loss = 0.30353390\n",
      "Iteration 416, loss = 0.30313115\n",
      "Iteration 417, loss = 0.30272172\n",
      "Iteration 418, loss = 0.30232180\n",
      "Iteration 419, loss = 0.30191157\n",
      "Iteration 420, loss = 0.30151589\n",
      "Iteration 421, loss = 0.30111378\n",
      "Iteration 422, loss = 0.30070689\n",
      "Iteration 423, loss = 0.30032857\n",
      "Iteration 424, loss = 0.29994343\n",
      "Iteration 425, loss = 0.29958812\n",
      "Iteration 426, loss = 0.29921029\n",
      "Iteration 427, loss = 0.29887855\n",
      "Iteration 428, loss = 0.29857021\n",
      "Iteration 429, loss = 0.29821831\n",
      "Iteration 430, loss = 0.29788956\n",
      "Iteration 431, loss = 0.29758072\n",
      "Iteration 432, loss = 0.29731037\n",
      "Iteration 433, loss = 0.29698870\n",
      "Iteration 434, loss = 0.29668509\n",
      "Iteration 435, loss = 0.29640981\n",
      "Iteration 436, loss = 0.29605399\n",
      "Iteration 437, loss = 0.29567741\n",
      "Iteration 438, loss = 0.29525953\n",
      "Iteration 439, loss = 0.29483615\n",
      "Iteration 440, loss = 0.29437439\n",
      "Iteration 441, loss = 0.29396206\n",
      "Iteration 442, loss = 0.29350250\n",
      "Iteration 443, loss = 0.29307557\n",
      "Iteration 444, loss = 0.29265689\n",
      "Iteration 445, loss = 0.29227571\n",
      "Iteration 446, loss = 0.29187749\n",
      "Iteration 447, loss = 0.29147954\n",
      "Iteration 448, loss = 0.29113050\n",
      "Iteration 449, loss = 0.29083228\n",
      "Iteration 450, loss = 0.29053941\n",
      "Iteration 451, loss = 0.29017559\n",
      "Iteration 452, loss = 0.28983588\n",
      "Iteration 453, loss = 0.28948555\n",
      "Iteration 454, loss = 0.28918932\n",
      "Iteration 455, loss = 0.28883295\n",
      "Iteration 456, loss = 0.28846387\n",
      "Iteration 457, loss = 0.28803153\n",
      "Iteration 458, loss = 0.28762538\n",
      "Iteration 459, loss = 0.28732707\n",
      "Iteration 460, loss = 0.28696462\n",
      "Iteration 461, loss = 0.28660673\n",
      "Iteration 462, loss = 0.28620809\n",
      "Iteration 463, loss = 0.28594279\n",
      "Iteration 464, loss = 0.28553977\n",
      "Iteration 465, loss = 0.28520031\n",
      "Iteration 466, loss = 0.28484765\n",
      "Iteration 467, loss = 0.28456290\n",
      "Iteration 468, loss = 0.28428239\n",
      "Iteration 469, loss = 0.28392082\n",
      "Iteration 470, loss = 0.28356648\n",
      "Iteration 471, loss = 0.28316265\n",
      "Iteration 472, loss = 0.28275640\n",
      "Iteration 473, loss = 0.28237095\n",
      "Iteration 474, loss = 0.28191179\n",
      "Iteration 475, loss = 0.28146368\n",
      "Iteration 476, loss = 0.28101816\n",
      "Iteration 477, loss = 0.28056842\n",
      "Iteration 478, loss = 0.28015723\n",
      "Iteration 479, loss = 0.27976361\n",
      "Iteration 480, loss = 0.27940105\n",
      "Iteration 481, loss = 0.27903463\n",
      "Iteration 482, loss = 0.27868662\n",
      "Iteration 483, loss = 0.27834008\n",
      "Iteration 484, loss = 0.27802921\n",
      "Iteration 485, loss = 0.27766647\n",
      "Iteration 486, loss = 0.27739555\n",
      "Iteration 487, loss = 0.27708975\n",
      "Iteration 488, loss = 0.27676524\n",
      "Iteration 489, loss = 0.27653611\n",
      "Iteration 490, loss = 0.27622560\n",
      "Iteration 491, loss = 0.27590285\n",
      "Iteration 492, loss = 0.27558974\n",
      "Iteration 493, loss = 0.27533267\n",
      "Iteration 494, loss = 0.27504337\n",
      "Iteration 495, loss = 0.27471998\n",
      "Iteration 496, loss = 0.27440953\n",
      "Iteration 497, loss = 0.27409963\n",
      "Iteration 498, loss = 0.27376535\n",
      "Iteration 499, loss = 0.27346506\n",
      "Iteration 500, loss = 0.27317274\n",
      "Iteration 501, loss = 0.27288192\n",
      "Iteration 502, loss = 0.27262325\n",
      "Iteration 503, loss = 0.27231085\n",
      "Iteration 504, loss = 0.27194849\n",
      "Iteration 505, loss = 0.27159324\n",
      "Iteration 506, loss = 0.27124500\n",
      "Iteration 507, loss = 0.27096194\n",
      "Iteration 508, loss = 0.27059459\n",
      "Iteration 509, loss = 0.27025865\n",
      "Iteration 510, loss = 0.26994563\n",
      "Iteration 511, loss = 0.26969815\n",
      "Iteration 512, loss = 0.26934075\n",
      "Iteration 513, loss = 0.26907655\n",
      "Iteration 514, loss = 0.26881285\n",
      "Iteration 515, loss = 0.26849107\n",
      "Iteration 516, loss = 0.26823060\n",
      "Iteration 517, loss = 0.26790902\n",
      "Iteration 518, loss = 0.26765802\n",
      "Iteration 519, loss = 0.26743514\n",
      "Iteration 520, loss = 0.26712595\n",
      "Iteration 521, loss = 0.26689175\n",
      "Iteration 522, loss = 0.26668814\n",
      "Iteration 523, loss = 0.26644837\n",
      "Iteration 524, loss = 0.26621110\n",
      "Iteration 525, loss = 0.26592254\n",
      "Iteration 526, loss = 0.26564157\n",
      "Iteration 527, loss = 0.26539207\n",
      "Iteration 528, loss = 0.26522254\n",
      "Iteration 529, loss = 0.26504844\n",
      "Iteration 530, loss = 0.26491008\n",
      "Iteration 531, loss = 0.26471536\n",
      "Iteration 532, loss = 0.26449606\n",
      "Iteration 533, loss = 0.26421155\n",
      "Iteration 534, loss = 0.26386931\n",
      "Iteration 535, loss = 0.26343802\n",
      "Iteration 536, loss = 0.26291105\n",
      "Iteration 537, loss = 0.26232305\n",
      "Iteration 538, loss = 0.26196154\n",
      "Iteration 539, loss = 0.26162516\n",
      "Iteration 540, loss = 0.26145989\n",
      "Iteration 541, loss = 0.26107222\n",
      "Iteration 542, loss = 0.26082507\n",
      "Iteration 543, loss = 0.26061862\n",
      "Iteration 544, loss = 0.26048961\n",
      "Iteration 545, loss = 0.26031511\n",
      "Iteration 546, loss = 0.26010142\n",
      "Iteration 547, loss = 0.25991135\n",
      "Iteration 548, loss = 0.25964301\n",
      "Iteration 549, loss = 0.25934314\n",
      "Iteration 550, loss = 0.25896346\n",
      "Iteration 551, loss = 0.25860910\n",
      "Iteration 552, loss = 0.25833392\n",
      "Iteration 553, loss = 0.25795296\n",
      "Iteration 554, loss = 0.25768340\n",
      "Iteration 555, loss = 0.25745608\n",
      "Iteration 556, loss = 0.25709556\n",
      "Iteration 557, loss = 0.25702571\n",
      "Iteration 558, loss = 0.25672951\n",
      "Iteration 559, loss = 0.25652420\n",
      "Iteration 560, loss = 0.25627002\n",
      "Iteration 561, loss = 0.25600643\n",
      "Iteration 562, loss = 0.25569979\n",
      "Iteration 563, loss = 0.25549408\n",
      "Iteration 564, loss = 0.25517846\n",
      "Iteration 565, loss = 0.25489125\n",
      "Iteration 566, loss = 0.25457108\n",
      "Iteration 567, loss = 0.25425216\n",
      "Iteration 568, loss = 0.25391626\n",
      "Iteration 569, loss = 0.25365962\n",
      "Iteration 570, loss = 0.25342122\n",
      "Iteration 571, loss = 0.25305751\n",
      "Iteration 572, loss = 0.25280443\n",
      "Iteration 573, loss = 0.25250806\n",
      "Iteration 574, loss = 0.25223517\n",
      "Iteration 575, loss = 0.25195210\n",
      "Iteration 576, loss = 0.25171549\n",
      "Iteration 577, loss = 0.25138172\n",
      "Iteration 578, loss = 0.25111357\n",
      "Iteration 579, loss = 0.25086466\n",
      "Iteration 580, loss = 0.25054041\n",
      "Iteration 581, loss = 0.25020647\n",
      "Iteration 582, loss = 0.24999810\n",
      "Iteration 583, loss = 0.24967840\n",
      "Iteration 584, loss = 0.24948248\n",
      "Iteration 585, loss = 0.24935272\n",
      "Iteration 586, loss = 0.24906412\n",
      "Iteration 587, loss = 0.24875215\n",
      "Iteration 588, loss = 0.24850319\n",
      "Iteration 589, loss = 0.24823646\n",
      "Iteration 590, loss = 0.24804325\n",
      "Iteration 591, loss = 0.24776762\n",
      "Iteration 592, loss = 0.24748523\n",
      "Iteration 593, loss = 0.24727747\n",
      "Iteration 594, loss = 0.24720437\n",
      "Iteration 595, loss = 0.24703575\n",
      "Iteration 596, loss = 0.24680189\n",
      "Iteration 597, loss = 0.24651449\n",
      "Iteration 598, loss = 0.24617829\n",
      "Iteration 599, loss = 0.24584392\n",
      "Iteration 600, loss = 0.24552365\n",
      "Iteration 601, loss = 0.24517125\n",
      "Iteration 602, loss = 0.24484017\n",
      "Iteration 603, loss = 0.24450795\n",
      "Iteration 604, loss = 0.24426737\n",
      "Iteration 605, loss = 0.24397132\n",
      "Iteration 606, loss = 0.24370974\n",
      "Iteration 607, loss = 0.24343801\n",
      "Iteration 608, loss = 0.24316452\n",
      "Iteration 609, loss = 0.24290092\n",
      "Iteration 610, loss = 0.24267372\n",
      "Iteration 611, loss = 0.24241192\n",
      "Iteration 612, loss = 0.24224619\n",
      "Iteration 613, loss = 0.24201874\n",
      "Iteration 614, loss = 0.24179166\n",
      "Iteration 615, loss = 0.24154820\n",
      "Iteration 616, loss = 0.24130314\n",
      "Iteration 617, loss = 0.24098014\n",
      "Iteration 618, loss = 0.24067824\n",
      "Iteration 619, loss = 0.24041860\n",
      "Iteration 620, loss = 0.24010687\n",
      "Iteration 621, loss = 0.23991610\n",
      "Iteration 622, loss = 0.23976959\n",
      "Iteration 623, loss = 0.23952759\n",
      "Iteration 624, loss = 0.23929516\n",
      "Iteration 625, loss = 0.23904280\n",
      "Iteration 626, loss = 0.23880590\n",
      "Iteration 627, loss = 0.23852342\n",
      "Iteration 628, loss = 0.23829515\n",
      "Iteration 629, loss = 0.23802168\n",
      "Iteration 630, loss = 0.23777392\n",
      "Iteration 631, loss = 0.23755602\n",
      "Iteration 632, loss = 0.23732815\n",
      "Iteration 633, loss = 0.23701693\n",
      "Iteration 634, loss = 0.23678396\n",
      "Iteration 635, loss = 0.23650040\n",
      "Iteration 636, loss = 0.23626036\n",
      "Iteration 637, loss = 0.23601021\n",
      "Iteration 638, loss = 0.23580007\n",
      "Iteration 639, loss = 0.23560878\n",
      "Iteration 640, loss = 0.23531191\n",
      "Iteration 641, loss = 0.23512616\n",
      "Iteration 642, loss = 0.23491067\n",
      "Iteration 643, loss = 0.23474462\n",
      "Iteration 644, loss = 0.23472909\n",
      "Iteration 645, loss = 0.23456632\n",
      "Iteration 646, loss = 0.23438426\n",
      "Iteration 647, loss = 0.23421626\n",
      "Iteration 648, loss = 0.23404808\n",
      "Iteration 649, loss = 0.23393424\n",
      "Iteration 650, loss = 0.23377959\n",
      "Iteration 651, loss = 0.23361531\n",
      "Iteration 652, loss = 0.23341253\n",
      "Iteration 653, loss = 0.23312164\n",
      "Iteration 654, loss = 0.23284886\n",
      "Iteration 655, loss = 0.23265399\n",
      "Iteration 656, loss = 0.23247451\n",
      "Iteration 657, loss = 0.23227067\n",
      "Iteration 658, loss = 0.23208725\n",
      "Iteration 659, loss = 0.23186506\n",
      "Iteration 660, loss = 0.23161775\n",
      "Iteration 661, loss = 0.23139755\n",
      "Iteration 662, loss = 0.23119943\n",
      "Iteration 663, loss = 0.23095913\n",
      "Iteration 664, loss = 0.23074918\n",
      "Iteration 665, loss = 0.23057380\n",
      "Iteration 666, loss = 0.23027369\n",
      "Iteration 667, loss = 0.23005406\n",
      "Iteration 668, loss = 0.22974876\n",
      "Iteration 669, loss = 0.22952755\n",
      "Iteration 670, loss = 0.22930215\n",
      "Iteration 671, loss = 0.22905405\n",
      "Iteration 672, loss = 0.22880963\n",
      "Iteration 673, loss = 0.22854174\n",
      "Iteration 674, loss = 0.22824562\n",
      "Iteration 675, loss = 0.22802159\n",
      "Iteration 676, loss = 0.22772378\n",
      "Iteration 677, loss = 0.22748850\n",
      "Iteration 678, loss = 0.22717938\n",
      "Iteration 679, loss = 0.22697278\n",
      "Iteration 680, loss = 0.22673378\n",
      "Iteration 681, loss = 0.22657898\n",
      "Iteration 682, loss = 0.22631268\n",
      "Iteration 683, loss = 0.22609852\n",
      "Iteration 684, loss = 0.22586606\n",
      "Iteration 685, loss = 0.22572065\n",
      "Iteration 686, loss = 0.22553927\n",
      "Iteration 687, loss = 0.22554289\n",
      "Iteration 688, loss = 0.22537117\n",
      "Iteration 689, loss = 0.22528882\n",
      "Iteration 690, loss = 0.22517551\n",
      "Iteration 691, loss = 0.22497166\n",
      "Iteration 692, loss = 0.22476723\n",
      "Iteration 693, loss = 0.22446179\n",
      "Iteration 694, loss = 0.22423136\n",
      "Iteration 695, loss = 0.22391872\n",
      "Iteration 696, loss = 0.22364384\n",
      "Iteration 697, loss = 0.22346479\n",
      "Iteration 698, loss = 0.22322112\n",
      "Iteration 699, loss = 0.22306642\n",
      "Iteration 700, loss = 0.22285619\n",
      "Iteration 701, loss = 0.22273824\n",
      "Iteration 702, loss = 0.22256370\n",
      "Iteration 703, loss = 0.22248239\n",
      "Iteration 704, loss = 0.22237206\n",
      "Iteration 705, loss = 0.22231317\n",
      "Iteration 706, loss = 0.22213279\n",
      "Iteration 707, loss = 0.22194930\n",
      "Iteration 708, loss = 0.22176124\n",
      "Iteration 709, loss = 0.22158593\n",
      "Iteration 710, loss = 0.22138737\n",
      "Iteration 711, loss = 0.22117930\n",
      "Iteration 712, loss = 0.22097183\n",
      "Iteration 713, loss = 0.22084357\n",
      "Iteration 714, loss = 0.22067413\n",
      "Iteration 715, loss = 0.22041734\n",
      "Iteration 716, loss = 0.22019069\n",
      "Iteration 717, loss = 0.21996027\n",
      "Iteration 718, loss = 0.21975103\n",
      "Iteration 719, loss = 0.21966270\n",
      "Iteration 720, loss = 0.21944335\n",
      "Iteration 721, loss = 0.21929512\n",
      "Iteration 722, loss = 0.21910915\n",
      "Iteration 723, loss = 0.21895431\n",
      "Iteration 724, loss = 0.21876727\n",
      "Iteration 725, loss = 0.21856736\n",
      "Iteration 726, loss = 0.21840667\n",
      "Iteration 727, loss = 0.21822273\n",
      "Iteration 728, loss = 0.21803137\n",
      "Iteration 729, loss = 0.21786582\n",
      "Iteration 730, loss = 0.21765948\n",
      "Iteration 731, loss = 0.21750523\n",
      "Iteration 732, loss = 0.21730074\n",
      "Iteration 733, loss = 0.21713080\n",
      "Iteration 734, loss = 0.21695500\n",
      "Iteration 735, loss = 0.21679858\n",
      "Iteration 736, loss = 0.21652341\n",
      "Iteration 737, loss = 0.21632581\n",
      "Iteration 738, loss = 0.21604276\n",
      "Iteration 739, loss = 0.21584917\n",
      "Iteration 740, loss = 0.21572386\n",
      "Iteration 741, loss = 0.21554258\n",
      "Iteration 742, loss = 0.21536780\n",
      "Iteration 743, loss = 0.21516532\n",
      "Iteration 744, loss = 0.21503558\n",
      "Iteration 745, loss = 0.21487792\n",
      "Iteration 746, loss = 0.21463480\n",
      "Iteration 747, loss = 0.21435514\n",
      "Iteration 748, loss = 0.21418190\n",
      "Iteration 749, loss = 0.21392046\n",
      "Iteration 750, loss = 0.21373031\n",
      "Iteration 751, loss = 0.21354451\n",
      "Iteration 752, loss = 0.21334668\n",
      "Iteration 753, loss = 0.21327830\n",
      "Iteration 754, loss = 0.21305725\n",
      "Iteration 755, loss = 0.21290752\n",
      "Iteration 756, loss = 0.21274030\n",
      "Iteration 757, loss = 0.21260434\n",
      "Iteration 758, loss = 0.21259100\n",
      "Iteration 759, loss = 0.21251921\n",
      "Iteration 760, loss = 0.21257916\n",
      "Iteration 761, loss = 0.21248221\n",
      "Iteration 762, loss = 0.21230807\n",
      "Iteration 763, loss = 0.21211190\n",
      "Iteration 764, loss = 0.21189177\n",
      "Iteration 765, loss = 0.21161810\n",
      "Iteration 766, loss = 0.21138397\n",
      "Iteration 767, loss = 0.21111474\n",
      "Iteration 768, loss = 0.21088731\n",
      "Iteration 769, loss = 0.21067969\n",
      "Iteration 770, loss = 0.21042816\n",
      "Iteration 771, loss = 0.21020509\n",
      "Iteration 772, loss = 0.20995349\n",
      "Iteration 773, loss = 0.20972947\n",
      "Iteration 774, loss = 0.20948390\n",
      "Iteration 775, loss = 0.20926683\n",
      "Iteration 776, loss = 0.20902902\n",
      "Iteration 777, loss = 0.20884974\n",
      "Iteration 778, loss = 0.20864642\n",
      "Iteration 779, loss = 0.20844477\n",
      "Iteration 780, loss = 0.20831353\n",
      "Iteration 781, loss = 0.20815971\n",
      "Iteration 782, loss = 0.20804520\n",
      "Iteration 783, loss = 0.20803796\n",
      "Iteration 784, loss = 0.20790928\n",
      "Iteration 785, loss = 0.20773149\n",
      "Iteration 786, loss = 0.20748217\n",
      "Iteration 787, loss = 0.20733135\n",
      "Iteration 788, loss = 0.20711161\n",
      "Iteration 789, loss = 0.20685971\n",
      "Iteration 790, loss = 0.20668865\n",
      "Iteration 791, loss = 0.20650710\n",
      "Iteration 792, loss = 0.20634051\n",
      "Iteration 793, loss = 0.20620741\n",
      "Iteration 794, loss = 0.20615001\n",
      "Iteration 795, loss = 0.20604587\n",
      "Iteration 796, loss = 0.20600077\n",
      "Iteration 797, loss = 0.20579369\n",
      "Iteration 798, loss = 0.20558968\n",
      "Iteration 799, loss = 0.20543943\n",
      "Iteration 800, loss = 0.20525710\n",
      "Iteration 801, loss = 0.20507007\n",
      "Iteration 802, loss = 0.20488507\n",
      "Iteration 803, loss = 0.20467798\n",
      "Iteration 804, loss = 0.20444659\n",
      "Iteration 805, loss = 0.20425715\n",
      "Iteration 806, loss = 0.20404786\n",
      "Iteration 807, loss = 0.20383914\n",
      "Iteration 808, loss = 0.20364516\n",
      "Iteration 809, loss = 0.20343200\n",
      "Iteration 810, loss = 0.20337170\n",
      "Iteration 811, loss = 0.20312661\n",
      "Iteration 812, loss = 0.20296274\n",
      "Iteration 813, loss = 0.20283275\n",
      "Iteration 814, loss = 0.20268966\n",
      "Iteration 815, loss = 0.20245809\n",
      "Iteration 816, loss = 0.20231240\n",
      "Iteration 817, loss = 0.20214664\n",
      "Iteration 818, loss = 0.20201363\n",
      "Iteration 819, loss = 0.20187186\n",
      "Iteration 820, loss = 0.20168905\n",
      "Iteration 821, loss = 0.20157932\n",
      "Iteration 822, loss = 0.20147423\n",
      "Iteration 823, loss = 0.20134106\n",
      "Iteration 824, loss = 0.20130752\n",
      "Iteration 825, loss = 0.20118067\n",
      "Iteration 826, loss = 0.20104720\n",
      "Iteration 827, loss = 0.20089458\n",
      "Iteration 828, loss = 0.20077135\n",
      "Iteration 829, loss = 0.20070351\n",
      "Iteration 830, loss = 0.20057761\n",
      "Iteration 831, loss = 0.20044123\n",
      "Iteration 832, loss = 0.20031929\n",
      "Iteration 833, loss = 0.20015609\n",
      "Iteration 834, loss = 0.19994732\n",
      "Iteration 835, loss = 0.19979972\n",
      "Iteration 836, loss = 0.19964237\n",
      "Iteration 837, loss = 0.19950513\n",
      "Iteration 838, loss = 0.19931000\n",
      "Iteration 839, loss = 0.19911706\n",
      "Iteration 840, loss = 0.19900474\n",
      "Iteration 841, loss = 0.19905104\n",
      "Iteration 842, loss = 0.19892949\n",
      "Iteration 843, loss = 0.19888824\n",
      "Iteration 844, loss = 0.19894215\n",
      "Iteration 845, loss = 0.19897072\n",
      "Iteration 846, loss = 0.19891580\n",
      "Iteration 847, loss = 0.19888577\n",
      "Iteration 848, loss = 0.19882040\n",
      "Iteration 849, loss = 0.19884294\n",
      "Iteration 850, loss = 0.19870337\n",
      "Iteration 851, loss = 0.19856050\n",
      "Iteration 852, loss = 0.19838033\n",
      "Iteration 853, loss = 0.19821833\n",
      "Iteration 854, loss = 0.19817515\n",
      "Iteration 855, loss = 0.19805161\n",
      "Iteration 856, loss = 0.19776115\n",
      "Iteration 857, loss = 0.19739482\n",
      "Iteration 858, loss = 0.19702211\n",
      "Iteration 859, loss = 0.19664180\n",
      "Iteration 860, loss = 0.19635207\n",
      "Iteration 861, loss = 0.19619925\n",
      "Iteration 862, loss = 0.19594452\n",
      "Iteration 863, loss = 0.19579208\n",
      "Iteration 864, loss = 0.19568380\n",
      "Iteration 865, loss = 0.19572085\n",
      "Iteration 866, loss = 0.19571435\n",
      "Iteration 867, loss = 0.19577041\n",
      "Iteration 868, loss = 0.19565867\n",
      "Iteration 869, loss = 0.19554943\n",
      "Iteration 870, loss = 0.19546264\n",
      "Iteration 871, loss = 0.19529067\n",
      "Iteration 872, loss = 0.19514629\n",
      "Iteration 873, loss = 0.19501341\n",
      "Iteration 874, loss = 0.19479355\n",
      "Iteration 875, loss = 0.19459219\n",
      "Iteration 876, loss = 0.19444068\n",
      "Iteration 877, loss = 0.19429758\n",
      "Iteration 878, loss = 0.19422254\n",
      "Iteration 879, loss = 0.19400727\n",
      "Iteration 880, loss = 0.19381077\n",
      "Iteration 881, loss = 0.19363831\n",
      "Iteration 882, loss = 0.19342078\n",
      "Iteration 883, loss = 0.19321534\n",
      "Iteration 884, loss = 0.19300969\n",
      "Iteration 885, loss = 0.19276212\n",
      "Iteration 886, loss = 0.19270800\n",
      "Iteration 887, loss = 0.19258418\n",
      "Iteration 888, loss = 0.19243243\n",
      "Iteration 889, loss = 0.19226385\n",
      "Iteration 890, loss = 0.19221339\n",
      "Iteration 891, loss = 0.19203782\n",
      "Iteration 892, loss = 0.19188046\n",
      "Iteration 893, loss = 0.19176068\n",
      "Iteration 894, loss = 0.19160237\n",
      "Iteration 895, loss = 0.19175003\n",
      "Iteration 896, loss = 0.19185657\n",
      "Iteration 897, loss = 0.19187060\n",
      "Iteration 898, loss = 0.19183369\n",
      "Iteration 899, loss = 0.19169618\n",
      "Iteration 900, loss = 0.19149894\n",
      "Iteration 901, loss = 0.19130472\n",
      "Iteration 902, loss = 0.19111334\n",
      "Iteration 903, loss = 0.19093191\n",
      "Iteration 904, loss = 0.19077013\n",
      "Iteration 905, loss = 0.19048016\n",
      "Iteration 906, loss = 0.19024856\n",
      "Iteration 907, loss = 0.19002097\n",
      "Iteration 908, loss = 0.18989339\n",
      "Iteration 909, loss = 0.18966273\n",
      "Iteration 910, loss = 0.18950186\n",
      "Iteration 911, loss = 0.18941066\n",
      "Iteration 912, loss = 0.18921648\n",
      "Iteration 913, loss = 0.18908410\n",
      "Iteration 914, loss = 0.18891988\n",
      "Iteration 915, loss = 0.18875786\n",
      "Iteration 916, loss = 0.18856678\n",
      "Iteration 917, loss = 0.18840193\n",
      "Iteration 918, loss = 0.18820656\n",
      "Iteration 919, loss = 0.18800889\n",
      "Iteration 920, loss = 0.18785935\n",
      "Iteration 921, loss = 0.18770039\n",
      "Iteration 922, loss = 0.18759460\n",
      "Iteration 923, loss = 0.18744923\n",
      "Iteration 924, loss = 0.18729103\n",
      "Iteration 925, loss = 0.18710068\n",
      "Iteration 926, loss = 0.18694186\n",
      "Iteration 927, loss = 0.18675879\n",
      "Iteration 928, loss = 0.18657808\n",
      "Iteration 929, loss = 0.18646170\n",
      "Iteration 930, loss = 0.18634236\n",
      "Iteration 931, loss = 0.18623276\n",
      "Iteration 932, loss = 0.18603368\n",
      "Iteration 933, loss = 0.18589587\n",
      "Iteration 934, loss = 0.18574415\n",
      "Iteration 935, loss = 0.18562828\n",
      "Iteration 936, loss = 0.18556752\n",
      "Iteration 937, loss = 0.18540814\n",
      "Iteration 938, loss = 0.18522531\n",
      "Iteration 939, loss = 0.18510407\n",
      "Iteration 940, loss = 0.18496294\n",
      "Iteration 941, loss = 0.18487537\n",
      "Iteration 942, loss = 0.18478873\n",
      "Iteration 943, loss = 0.18466672\n",
      "Iteration 944, loss = 0.18457691\n",
      "Iteration 945, loss = 0.18445112\n",
      "Iteration 946, loss = 0.18436754\n",
      "Iteration 947, loss = 0.18431728\n",
      "Iteration 948, loss = 0.18424407\n",
      "Iteration 949, loss = 0.18419975\n",
      "Iteration 950, loss = 0.18405747\n",
      "Iteration 951, loss = 0.18391905\n",
      "Iteration 952, loss = 0.18383374\n",
      "Iteration 953, loss = 0.18369752\n",
      "Iteration 954, loss = 0.18357297\n",
      "Iteration 955, loss = 0.18347151\n",
      "Iteration 956, loss = 0.18352767\n",
      "Iteration 957, loss = 0.18352441\n",
      "Iteration 958, loss = 0.18361266\n",
      "Iteration 959, loss = 0.18367151\n",
      "Iteration 960, loss = 0.18376835\n",
      "Iteration 961, loss = 0.18383724\n",
      "Iteration 962, loss = 0.18388860\n",
      "Iteration 963, loss = 0.18370165\n",
      "Iteration 964, loss = 0.18342995\n",
      "Iteration 965, loss = 0.18311228\n",
      "Iteration 966, loss = 0.18278575\n",
      "Iteration 967, loss = 0.18257139\n",
      "Iteration 968, loss = 0.18232661\n",
      "Iteration 969, loss = 0.18212051\n",
      "Iteration 970, loss = 0.18198694\n",
      "Iteration 971, loss = 0.18196084\n",
      "Iteration 972, loss = 0.18190787\n",
      "Iteration 973, loss = 0.18185676\n",
      "Iteration 974, loss = 0.18178151\n",
      "Iteration 975, loss = 0.18168910\n",
      "Iteration 976, loss = 0.18151331\n",
      "Iteration 977, loss = 0.18123535\n",
      "Iteration 978, loss = 0.18098980\n",
      "Iteration 979, loss = 0.18072216\n",
      "Iteration 980, loss = 0.18055420\n",
      "Iteration 981, loss = 0.18037967\n",
      "Iteration 982, loss = 0.18016020\n",
      "Iteration 983, loss = 0.18004245\n",
      "Iteration 984, loss = 0.17989938\n",
      "Iteration 985, loss = 0.17977994\n",
      "Iteration 986, loss = 0.17965537\n",
      "Iteration 987, loss = 0.17958334\n",
      "Iteration 988, loss = 0.17947751\n",
      "Iteration 989, loss = 0.17939745\n",
      "Iteration 990, loss = 0.17923896\n",
      "Iteration 991, loss = 0.17915328\n",
      "Iteration 992, loss = 0.17906408\n",
      "Iteration 993, loss = 0.17902366\n",
      "Iteration 994, loss = 0.17898946\n",
      "Iteration 995, loss = 0.17890832\n",
      "Iteration 996, loss = 0.17884280\n",
      "Iteration 997, loss = 0.17874891\n",
      "Iteration 998, loss = 0.17870138\n",
      "Iteration 999, loss = 0.17869818\n",
      "Iteration 1000, loss = 0.17873751\n",
      "Iteration 1001, loss = 0.17856919\n",
      "Iteration 1002, loss = 0.17834786\n",
      "Iteration 1003, loss = 0.17816970\n",
      "Iteration 1004, loss = 0.17805273\n",
      "Iteration 1005, loss = 0.17790629\n",
      "Iteration 1006, loss = 0.17769242\n",
      "Iteration 1007, loss = 0.17759543\n",
      "Iteration 1008, loss = 0.17754134\n",
      "Iteration 1009, loss = 0.17755998\n",
      "Iteration 1010, loss = 0.17747530\n",
      "Iteration 1011, loss = 0.17750339\n",
      "Iteration 1012, loss = 0.17736107\n",
      "Iteration 1013, loss = 0.17725674\n",
      "Iteration 1014, loss = 0.17712178\n",
      "Iteration 1015, loss = 0.17700691\n",
      "Iteration 1016, loss = 0.17675056\n",
      "Iteration 1017, loss = 0.17661161\n",
      "Iteration 1018, loss = 0.17642916\n",
      "Iteration 1019, loss = 0.17623279\n",
      "Iteration 1020, loss = 0.17605703\n",
      "Iteration 1021, loss = 0.17598502\n",
      "Iteration 1022, loss = 0.17576729\n",
      "Iteration 1023, loss = 0.17560991\n",
      "Iteration 1024, loss = 0.17573695\n",
      "Iteration 1025, loss = 0.17565385\n",
      "Iteration 1026, loss = 0.17564607\n",
      "Iteration 1027, loss = 0.17547260\n",
      "Iteration 1028, loss = 0.17521685\n",
      "Iteration 1029, loss = 0.17516364\n",
      "Iteration 1030, loss = 0.17485148\n",
      "Iteration 1031, loss = 0.17476016\n",
      "Iteration 1032, loss = 0.17448825\n",
      "Iteration 1033, loss = 0.17435738\n",
      "Iteration 1034, loss = 0.17423685\n",
      "Iteration 1035, loss = 0.17413096\n",
      "Iteration 1036, loss = 0.17398818\n",
      "Iteration 1037, loss = 0.17391823\n",
      "Iteration 1038, loss = 0.17380148\n",
      "Iteration 1039, loss = 0.17365372\n",
      "Iteration 1040, loss = 0.17351371\n",
      "Iteration 1041, loss = 0.17335582\n",
      "Iteration 1042, loss = 0.17320545\n",
      "Iteration 1043, loss = 0.17308456\n",
      "Iteration 1044, loss = 0.17293449\n",
      "Iteration 1045, loss = 0.17281744\n",
      "Iteration 1046, loss = 0.17270686\n",
      "Iteration 1047, loss = 0.17262802\n",
      "Iteration 1048, loss = 0.17252377\n",
      "Iteration 1049, loss = 0.17241243\n",
      "Iteration 1050, loss = 0.17230796\n",
      "Iteration 1051, loss = 0.17224666\n",
      "Iteration 1052, loss = 0.17215713\n",
      "Iteration 1053, loss = 0.17214268\n",
      "Iteration 1054, loss = 0.17207365\n",
      "Iteration 1055, loss = 0.17195564\n",
      "Iteration 1056, loss = 0.17180278\n",
      "Iteration 1057, loss = 0.17164815\n",
      "Iteration 1058, loss = 0.17156434\n",
      "Iteration 1059, loss = 0.17142338\n",
      "Iteration 1060, loss = 0.17131467\n",
      "Iteration 1061, loss = 0.17118748\n",
      "Iteration 1062, loss = 0.17103607\n",
      "Iteration 1063, loss = 0.17087714\n",
      "Iteration 1064, loss = 0.17072494\n",
      "Iteration 1065, loss = 0.17068952\n",
      "Iteration 1066, loss = 0.17064316\n",
      "Iteration 1067, loss = 0.17064171\n",
      "Iteration 1068, loss = 0.17072977\n",
      "Iteration 1069, loss = 0.17075820\n",
      "Iteration 1070, loss = 0.17084226\n",
      "Iteration 1071, loss = 0.17088231\n",
      "Iteration 1072, loss = 0.17086522\n",
      "Iteration 1073, loss = 0.17114206\n",
      "Iteration 1074, loss = 0.17128774\n",
      "Iteration 1075, loss = 0.17121559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier3 = MLPClassifier(max_iter=2000,activation='relu', verbose=True, hidden_layer_sizes=(15,),random_state=1)\n",
    "mlpClassifier3.fit(X_train, y_train)\n",
    "y_pred3 = mlpClassifier3.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b965c8f",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70fe108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56989247311827"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d648571c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde34e31220>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjhUlEQVR4nO3deXwd5X3v8c9P+75v1mLLGzYCs9pmDwYcAg6F20BTnKQhhIRcElLScNtCSUvDve0t5V4SklISmhKaBXwhIYEQggmYfTMyi8G7vGmxZUnWvm/P/eOMjGzLtiwfeXTmfN+v13n5zJI5v8mI73nOM8/MmHMOERGJfDF+FyAiIuGhQBcRCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIQCXXxlZjvMbKlPn73YzJ4xs1Yzazaz1WZ2vR+1HFBXhZlVmlmL93rezCr8rkumPgW6RCUzOwdYBbwMzAFygZuAyye4vdjwVccu4BogB8gDngJWhHH7ElAKdJmSzCzRzL5vZru81/fNLNFblmdmT49qWb9qZjHesr81szoz6zCzTWZ2ySE+4h7gv5xzdzvnmlzIGufcZ73tfMnMXjugJmdmc7z3D5vZA14Lvwv4H2ZWPzrYzexPzWyt9z7GzG4zs61mttfMHjOznLEKc861Oud2uNBl3AYMEfrSETksBbpMVXcAZwOnAacCi4HveMtuBWqBfKAQ+DvAmdk84GZgkXMuHfgUsOPADZtZCnAO8KtjrPFzwD8B6cB9QBdw8QHLH/HefxP4b8CFQDHQAtx/uI2bWSvQC/wQ+OdjrFWigAJdpqrPA3c55xqcc43Ad4G/8JYNANOAGc65Aefcq15rdghIBCrMLN5r5W4dY9vZhP72dx9jjU865153zg0753qBR4HlAGaWDizz5gH8d+AO51ytc64P+EfgGjOLO9TGnXNZQCahL6n3jrFWiQIKdJmqioGdo6Z3evMg1F1SBTxnZtvM7DYA51wV8C1CYdlgZivMrJiDtQDDhL4UjkXNAdOPAJ/xuoY+A7zrnBvZhxnAb7xuolZgA6EvoMLDfYBzrgv4EfAzMys4xnol4BToMlXtIhSCI6Z783DOdTjnbnXOzQKuBL490lfunHvEOXe+9791wN0Hbtg51w28CVx9mM/vAlJGJsysaIx19rtVqXNuPaEvnsvZv7sFQuF/uXMua9QryTlXd5gaRsR4tZSMY12JYgp0mQrizSxp1CuOUFfFd8ws38zygH8AfgFgZleY2RwzM6CNUEt32MzmmdnFXgu5F+gh1BIfy98AXzKzvzazXG+7p5rZyGiSD4CTzOw0M0si1Oofj0eAW4BPAI+Pmv8j4J/MbIb3WflmdtVYGzCzT5rZ6WYWa2YZwL2EflVsGGcNEqUU6DIVPEMofEde/wj8L6ASWAt8CLzrzQOYCzwPdBJqaf+7c+5FQv3n/wI0AfVAAXD7WB/onHuD0AnMi4FtZtYMPOjVgnNuM3CX9zlbgNfG2s4YHiV04nOVc65p1Pz7CA0/fM7MOoC3gLMOsY0sbzttwFZgNnCZ108vckimB1yIiASDWugiIgGhQBcRCQgFuohIQCjQRUQC4pBXqU22vLw8V15e7tfHi4hEpDVr1jQ55/LHWuZboJeXl1NZWenXx4uIRCQz23moZepyEREJCAW6iEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgIi7Q39nRzN3PbkR3iRQR2V/EBfra2jYeeGkrbT0DfpciIjKlRFygF2YkAlDfrnv9i4iMFnGBXpSRBMCe9j6fKxERmVoiLtAL9wW6WugiIqNFXKDnp4e6XPa0KdBFREaLuEBPio8lOyWePR0KdBGR0SIu0CHU7VLfpj50EZHRjhjoZvaQmTWY2UeHWG5m9gMzqzKztWZ2RvjL3F9BRhINaqGLiOxnPC30h4HLDrP8cmCu97oReODYyzq8ooxEnRQVETnAEQPdOfcK0HyYVa4CfuZC3gKyzGxauAocS2FGEo0dfQwODU/mx4iIRJRw9KGXADWjpmu9eQcxsxvNrNLMKhsbGyf8gYUZSQw72NvVP+FtiIgEzXE9Keqce9A5t9A5tzA/f8xnnI7LyFj0eg1dFBHZJxyBXgeUjZou9eZNmpHL/9WPLiLysXAE+lPAF73RLmcDbc653WHY7iEV6WpREZGDxB1pBTN7FFgC5JlZLXAnEA/gnPsR8AywDKgCuoHrJ6vYEblpicTFGLvV5SIiss8RA905t/wIyx3wjbBVNA6xMUZRZhJ1rT3H82NFRKa0iLxSFKAkK5m6FgW6iMiIyA307GR2qYUuIrJP5AZ6VjL17b0M6OIiEREgwgN92Gmki4jIiIgN9OKsZAD1o4uIeCI20EuyQ4G+q02BLiICERzoxZlqoYuIjBaxgZ6cEEtuagJ1repDFxGBCA50CPWj6+IiEZGQiA70kiyNRRcRGRHZgZ4dulo0dPcBEZHoFtGBXpyVTM/AEC3dA36XIiLiu4gO9BJvLLq6XUREAhLotRq6KCIS4YHuXVykkS4iIhEe6Nkp8aQnxlG9t8vvUkREfBfRgW5mTM9NYWdzt9+liIj4LqIDHaA8N5WdexXoIiIRH+gzclOoae5mUPdFF5EoF4hAHxx2emC0iES9AAR6KgA7dGJURKJcxAd6uRfo6kcXkWgX8YFekJ5IYlwMO9VCF5EoF/GBHhNjzMhNYYda6CIS5SI+0CHUj16tQBeRKBeMQM9JYWdzF8PDuo2uiESvYAR6Xiq9A8M0dPT5XYqIiG8CEejluSkAOjEqIlEtEIE+I0dj0UVEAhHoJdnJJMTFsK1RgS4i0SsQgR4bY8zKS6WqodPvUkREfBOIQAeYU5DGFgW6iESxQAV6TUs3vQNDfpciIuKLwAT63IJ0nIOtjWqli0h0CkygzylIA1A/uohErcAEenleCjGmQBeR6DWuQDezy8xsk5lVmdltYyyfbmYvmtl7ZrbWzJaFv9TDS4yLpTxXI11EJHodMdDNLBa4H7gcqACWm1nFAat9B3jMOXc6cC3w7+EudDxmF6Qp0EUkao2nhb4YqHLObXPO9QMrgKsOWMcBGd77TGBX+Eocv7kFaWxv6mJAzxcVkSg0nkAvAWpGTdd680b7R+ALZlYLPAN8c6wNmdmNZlZpZpWNjY0TKPfw5hSkMTjs9PQiEYlK4Topuhx42DlXCiwDfm5mB23bOfegc26hc25hfn5+mD76Y3ML0gGoaugI+7ZFRKa68QR6HVA2arrUmzfaDcBjAM65N4EkIC8cBR6N2QWhm3SpH11EotF4Av0dYK6ZzTSzBEInPZ86YJ1q4BIAMzuRUKCHv0/lCFIS4ijJStYtAEQkKh0x0J1zg8DNwEpgA6HRLOvM7C4zu9Jb7Vbgq2b2AfAo8CXnnC+PD5pXlM7G3epyEZHoEzeelZxzzxA62Tl63j+Mer8eOC+8pU3MScUZvLy5kd6BIZLiY/0uR0TkuAnMlaIjKqZlMDTs2LxHrXQRiS7BC/Ti0HD49bvafa5EROT4Clygl2WnkJYYx/rdCnQRiS6BC/SYGOPEaelqoYtI1AlcoAOcVJzJht3tDA/7MtBGRMQXgQz0imkZdPUPsbNZtwAQkegRzED3Toyu29XmcyUiIsdPIAP9hMJ0EuNieK+61e9SRESOm0AGekJcDKeWZVG5o9nvUkREjptABjrAwhnZrNvVTnf/oN+liIgcF4EN9EXlOQwOO96vafW7FBGR4yKwgX7G9GwA1uxo8bkSEZHjI7CBnpkSz7zCdFarH11EokRgAx1g8cwc1uxs0TNGRSQqBDrQz5uTS3f/EB+oH11EokCgA/3sWbmYwetVe/0uRURk0gU60LNSEjipOIM3tjb5XYqIyKQLdKADnDc7j/eqW+npH/K7FBGRSRX8QJ+TR//QMG9tU7eLiARb4AP9rFk5pCbE8scNe/wuRURkUgU+0BPjYrlwXj7Pr9+j+6OLSKAFPtABlp5YSENHH2vrdDtdEQmuqAj0i+cXEBtjPLeu3u9SREQmTVQEelZKAufMyuXptbtxTt0uIhJMURHoAFeeVkx1c7fuvigigRU1gX7ZyUUkxMXw5Pu7/C5FRGRSRE2gZyTFc8n8Ap5eu4tB3axLRAIoagId4KrTSmjq7Of1rbrISESCJ6oC/aL5+aQnxfHk+3V+lyIiEnZRFeiJcbEsO3kaKz+q171dRCRwoirQAa46vZiu/iGe160ARCRgoi7Qz5qZS1FGEr99T90uIhIsURfosTHGn55RwkubG6lv6/W7HBGRsIm6QAe4dlEZQ8OOxytr/C5FRCRsojLQZ+Smcv6cPFa8U8OQ7sAoIgERlYEOsHzxdOpae3h1S6PfpYiIhEXUBvonKwrJTU3g0dXVfpciIhIW4wp0M7vMzDaZWZWZ3XaIdT5rZuvNbJ2ZPRLeMsMvIS6Ga84s5fkNDTS06+SoiES+Iwa6mcUC9wOXAxXAcjOrOGCducDtwHnOuZOAb4W/1PBbvng6w87x8Bs7/C5FROSYjaeFvhiocs5tc871AyuAqw5Y56vA/c65FgDnXEN4y5wc5XmpLDt5Gj9/cydtPQN+lyMickzGE+glwOjxfbXevNFOAE4ws9fN7C0zu2ysDZnZjWZWaWaVjY1T42Tk1y+aTUffID9/c4ffpYiIHJNwnRSNA+YCS4DlwH+YWdaBKznnHnTOLXTOLczPzw/TRx+bk4ozuWhePv/52nbae9VKF5HINZ5ArwPKRk2XevNGqwWecs4NOOe2A5sJBXxE+PYn59HaM8D9q6r8LkVEZMLGE+jvAHPNbKaZJQDXAk8dsM5vCbXOMbM8Ql0w28JX5uRaUJrJNWeU8tDr29nR1OV3OSIiE3LEQHfODQI3AyuBDcBjzrl1ZnaXmV3prbYS2Gtm64EXgb92zkXUUyT++lPzSIiN4Z+f2eB3KSIiExI3npWcc88Azxww7x9GvXfAt71XRCrISOLrF83hnpWbeKOqiXPn5PldkojIUYnaK0XHcsP5MynLSea7v1vPgJ47KiIRRoE+SlJ8LHdecRKb9nTw4CsRcwpARARQoB9kaUUhn14wjfte2MK2xk6/yxERGTcF+hjuvLKCpLgYbnviQ4Z1e10RiRAK9DEUpCfx91dUsHp7M//52na/yxERGRcF+iFcc2Ypl1YUcs/KTWysb/e7HBGRI1KgH4KZ8b8/s4CM5Di+teJ9+gaH/C5JROSwFOiHkZuWyN1Xn8LG+g7u/eNmv8sRETksBfoRXHJiIcsXT+fBV7bx9raIuvhVRKKMAn0cvvPpE5mek8Ktj39Ah+7IKCJTlAJ9HFIT47j3s6exq7WHu3633u9yRETGpEAfpzNnZPP1JXN4fE0tv1pT63c5IiIHUaAfhVuWzuW8Obnc/sRa9aeLyJSjQD8K8bEx/PvnzqQsJ4Wv/WKN7p0uIlOKAv0oZabE89MvLcKA6366mt1tPX6XJCICKNAnZEZuKj+9fjHNnf187j/eZk97r98liYgo0CfqtLIsHv7yYhrae1n+4Fs0KNRFxGcK9GNw5oxs/uvLi6lv7+XqH72h2+2KiK8U6MdoYXkOj3z1bLr7hrj6gTdYs7PZ75JEJEop0MPgtLIsnvj6uWQmx/O5/3ibZz+q97skEYlCCvQwmZGbyq9vOpeK4gxu+uUaHnptO6FnZ4uIHB8K9DDKTUvkka+czaUVhdz19Hr+/smPGNTDpkXkOFGgh1lyQiwPfP5MvnbhLH7xVjXXP/wObT26oZeITD4F+iSIiTFuv/xE7r56AW9u3cvVD2gEjIhMPgX6JPrzRdP52Q2Laers48p/e51nPtztd0kiEmAK9El27uw8fv+XFzCnII2v//Jdvvu7dfQPql9dRMJPgX4clGQl89jXzuH688r56es7+OyP32TnXt3YS0TCS4F+nCTExXDnn5zEA58/g22NnSy771Ueq6zR0EYRCRsF+nF2+YJpPPutT3BKaRZ/86u13PSLd2np6ve7LBEJAAW6D4qzkvnlV87i9svn88LGPXzq+6/wyuZGv8sSkQinQPdJTIzxtQtn89tvnEdmcjxffGg1d/zmQ7r6Bv0uTUQilALdZycVZ/K7b57PVy+YySOrq7nsvld4S4+3E5EJUKBPAUnxsdzx6Qoe+9o5xJhx7YNv8Xe/+ZC2bl1hKiLjp0CfQhaV5/CHWy7gqxfMZMXqai659yWefL9OI2FEZFwU6FNMSkIcd3y6gqduPp+SrGRuWfE+X3xotcati8gRKdCnqJNLMnni6+fx3StP4r3qVi793ivc/2KVrjIVkUMaV6Cb2WVmtsnMqszstsOsd7WZOTNbGL4So1dsjHHdueU8/+0LuXh+Afes3MSnf/Aq7+zQU5FE5GBHDHQziwXuBy4HKoDlZlYxxnrpwC3A2+EuMtoVZSbxwBfO5CdfXEh3/xB/9qM3ue3Xa2nWBUkiMsp4WuiLgSrn3DbnXD+wArhqjPX+J3A30BvG+mSUpRWFPPdXn+DGT8zi8TW1LLnnRR56bTsDeoiGiDC+QC8BakZN13rz9jGzM4Ay59zvD7chM7vRzCrNrLKxUVdGTkRqYhx/t+xE/nDLBZxalsVdT6/n8vte5WVdaSoS9Y75pKiZxQD3ArceaV3n3IPOuYXOuYX5+fnH+tFR7YTCdH725cX85IsLGRwa5rqHVvPVn1VSvbfb79JExCfjCfQ6oGzUdKk3b0Q6cDLwkpntAM4GntKJ0clnZiytKGTlX32Cv71sPq9XNbH0ey/zf5/bRHe/biEgEm3GE+jvAHPNbKaZJQDXAk+NLHTOtTnn8pxz5c65cuAt4ErnXOWkVCwHSYyL5aYls1l16xKWnVzED1dVceE9L/HzN3domKNIFDlioDvnBoGbgZXABuAx59w6M7vLzK6c7AJl/Ioyk/j+tafz65vOYWZuKn//5DqW3vsyv32vjuFhXW0qEnTm12XlCxcudJWVasRPFuccL21u5F+f3cSG3e3MLUjj5ovncMUpxcTGmN/licgEmdka59yYXdq6UjSgzIyL5hXw+2+ezw+Xn44Z3LLifT5578v8ak0tgxrqKBI4aqFHieFhx8p19fxgVRUbdrdTlpPMN5bM4TNnlJIQp+91kUhxuBa6Aj3KOOd4fkMDP1y1hbW1bZRkJfOVC2byZwvLSEuM87s8ETkCBbocZKSP/d9WVbFmZwvpiXH8+aIyrju3nLKcFL/LE5FDUKDLYb1b3cJPX9/BHz7czbBzXFpRxJfPn8mi8mzMdAJVZCo5XKDrN7ZwxvRszpieze5l8/n5mzt5ZHU1z66r5+SSDK4/dyZXnDqNxLhYv8sUkSNQC10O0tM/xG/eq+Oh17dT1dBJTmoC15xZyrWLypiVn+Z3eSJRTV0uMiHOOV6rauKRt6v54/o9DA47zpuTyxfOmsHSikLiYzU6RuR4U6DLMWvo6OXxylp++dZOdrX1kpeWyGcXlnLtoulMz9VJVJHjRYEuYTM07Hh5cwOPvF3Dqo17GHZwwdw8vnD2DC6ZX0CcWu0ik0qBLpNid1sPj1fWsmJ1NbvaeilIT+RPTy/hilOKObkkQyNkRCaBAl0m1eDQMC9sbODxyhpe2tTI4LBjek4Kly8o4tMLprGgJFPhLhImCnQ5blq6+vnj+j38/sPdvF7VxOCwoywnmWUnT2PZgmmcUqpwFzkWCnTxRWt3P8+t38MzH+7mtS2hcC/JSmbZgiKWLZjGqaVZxOjOjyJHRYEuvmvrHuC59fWhcK9qYmDIUZiRyNITC1l6YiHnzM4lKV4XL4kciQJdppS27gGe37CH5zfs4eXNjXT3D5EcH8t5c/JYemIBF88voCAjye8yRaYkBbpMWX2DQ7y1rZkXNuzhhQ0N1LX2AHBKaSaXzC/kkhMLOKlYI2ZERijQJSI459hY38GqjQ08v2EP79e04hwUZSRx8YkFXDyvgHPn5JKSoFsQSfRSoEtEauzo48VNDaza0MArW0JdMwlxMZw1M4eL5hWwZF4+M/NS1XqXqKJAl4jXNzhE5Y4WVm1s4KVNDWxt7AJgRm4KS07IZ8n8As6ZpROrEnwKdAmcmuZuXtrUwEubGnl9axO9A8Mkxcfwibn5XHpSEUvm5ZOXluh3mSJhp0CXQOsdGGL19tCJ1efW72F3Wy8As/JSWVSew+KZoVdpdrK6ZyTiKdAlajjn+LCujTe27uWd7c28s6OZ9t5BIHRyddHMHBaXZ7NoZg4nFKTrwiaJOAp0iVrDw47NDR28s72Zt72A39PeB0BmcjwLZ2RzZnk2p5VlcUpplh6ULVOeHkEnUSsmxphflMH8ogz+4pxynHPUNPewekfzvhb8CxsbQusazC1I5/y5eZw1M4f5RRmUZierFS8RQy10iXotXf28X9vK+9WtvFfTyltb99I/NAxAWmIcp0/P4ozp2Sz0WvLpSfE+VyzRTC10kcPITk3gonkFXDSvAICuvkE27elgU30HH9W18W51Kz9YtQXnwAzmFaZz5ozsfa/pOSk62SpTglroIuPQ0TvABzVtVO5sZs3OFt6vbqWjL3SyNSc1gVNKMzmlNIvTykL/asikTBa10EWOUXpSPOfPzeP8uXlA6FF8Wxo6WLOzhQ9qWvmgpo1XNm9h2GsflWQlc6oX7qeUZrKgJFNdNTLpFOgiExA76mTr58+aAYS6atbtag8FfG0ra2vbeObDeiDUVTM7P41TSjOpmJbBidMyOKEwnfx0teQlfBToImGSmhi37yKmEc1d/aytDbXg19a28uqWJp54t27f8tzUBE4oTGdeUeh1QmE6c/LTyExRa16OngJdZBLlpCawZF4BS7wTrgBNnX1squ/4+LWng8cqa+juH9q3Tl7ax0FfMS2DiuIM5hakkxAX48duSIRQoIscZ3lpieTNSeS8OXn75g0PO+pae9hU38HWxk62NnayaU8nK1bX0DMQCvqE2BhOKEpjQUnmvjHyZTkplOemKugFUKCLTAkxMUZZTgplOSkspXDf/KFhx469Xazb1c66XW2sq2vnDx/V8+jqmn3rxMUYs/JTmVeUwfScZEqyUijNTmZ6TujfuFiFfbRQoItMYbExxuz8NGbnp3HlqcVA6H41jR191LX2UN3czab6DjbWd/BedQvPfLiboeGPhyLHxxrTc1KYmZfG7PxUZualMis/jZl5qeSlJWj8fMAo0EUijJlRkJFEQUYSp0/P3m/Z4NAwezr6qGvpYefeLrY1dbG9sYttTZ28srlx3xWwAOlJcZTnpjI9N4Xy3BRm5KZSmpVMUWYSpdkp6saJQOMKdDO7DLgPiAV+4pz7lwOWfxv4CjAINAJfds7tDHOtInIEcbExlGQlU5KVvN9oGwh13+xq7WFrYyfbGrvY3tTFzuZu1tW18exH9fu17ONijPK8VE4oTGNuQTpzC9OYlplMeW4KOalq2U9VRwx0M4sF7gc+CdQC75jZU8659aNWew9Y6JzrNrObgH8F/nwyChaRiYkd1U+/ZN7+ywaGhtnV2kNdaw+7W3vZ1tTJ5j2drN8V6rMffUF5ZnI8JVnJFGYkhraXneJtN3SSNkMXUPlmPC30xUCVc24bgJmtAK4C9gW6c+7FUeu/BXwhnEWKyOSKj41hRm4qM3JTD1rWOzDEtsYu6tt72N7UzbbGTurbeqlv76VyZwsd3v3mR2SlxFOWnUJhRhL56QnkpiaSl5ZAXnoiuamJZKfGk52SQHZKgrp1wmw8gV4C1IyargXOOsz6NwB/GGuBmd0I3Agwffr0cZYoIn5Kio+lojg0Fn4sbd0D1LR0U93cTU2z929LD7Ut3bxf00pzVx/DY9wyKi7GmFeUzimlWRRnJpEUH0tRZpLX6k9W184EhPWkqJl9AVgIXDjWcufcg8CDELo5Vzg/W0T8kZkST2ZKJieXZI65fGjY0drdT1NnP3s7+2jpHqClu59drT18WNfG02t3HdTKB0hJiPW6c5LJT09iaHiYsuwU5hSkUZ6XSllOih5IcoDx/L9RB5SNmi715u3HzJYCdwAXOuf6wlOeiES62BgjNy2R3LREIP2g5c45BocdPQND7Grtoaa5h5rmbmpauqlp/rilD0ZT5/7Rkp0ST3FWMsXeieDirKT9pvPTEqPqASXjCfR3gLlmNpNQkF8LfG70CmZ2OvBj4DLnXEPYqxSRwDIz4mON+NgYMorimV80dtcOhG6Atq2xi+rmbnY2d4VO5Lb0UL23mze37qWzb/+WvhmkxMeSmhhHWmIcKYmxxFroC2ZeUTqz8lIpyEjadyFWfIRfhHXEQHfODZrZzcBKQsMWH3LOrTOzu4BK59xTwD1AGvC41+dV7Zy7chLrFpEolJoYx4LSTBaUjt290947wK7WHm/ETi+N7b109Q/R1TdIZ98gXX2DDDvY1drDq1saGRjaf6hmXloimcnxZKbEk5UcT1ZKPNmpCeSnJZKfnkhRRhJFmUnkpiWSmhA75fr49YALEYlK/YPD7G7roaGjj517Q6N3mjr7aO0eoK0n9Grp7qela2C/C7JGJMTFUJCeSHluKsVZoZDPTU0g3xvNk5uWQF5aItkp8QfdfsE5N+EvAz3gQkTkAAlxHw/VXFSec8j1nHO09w7S0B4aqlnf1ktzVz97u/rZ097LjqYuXtncyd6uvv1a/CPMIDslgYykOLr6h+jsHeTOP6ng2sXhH+mnQBcROQwzC3XDJMczt/Dgk7ojnHO09wzS2NnH3s4+9nb109TZt290T3vvIGmJsaQlxnFC0aG3cywU6CIiYWBm3hDOeOYUpPlSQ2Sf0hURkX0U6CIiAaFAFxEJCAW6iEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhG/3cjGzRmCizx3NA5rCWM5UpH2MfEHfP9A++mGGcy5/rAW+BfqxMLPKQ92cJii0j5Ev6PsH2sepRl0uIiIBoUAXEQmISA30B/0u4DjQPka+oO8faB+nlIjsQxcRkYNFagtdREQOoEAXEQmIiAt0M7vMzDaZWZWZ3eZ3PRNhZmVm9qKZrTezdWZ2izc/x8z+aGZbvH+zvflmZj/w9nmtmZ3h7x6Mn5nFmtl7Zva0Nz3TzN729uX/mVmCNz/Rm67ylpf7Wvg4mVmWmf3KzDaa2QYzOydIx9HM/sr7G/3IzB41s6RIP4Zm9pCZNZjZR6PmHfUxM7PrvPW3mNl1fuzLgSIq0M0sFrgfuByoAJabWYW/VU3IIHCrc64COBv4hrcftwEvOOfmAi940xDa37ne60bggeNf8oTdAmwYNX038D3n3BygBbjBm38D0OLN/563XiS4D3jWOTcfOJXQvgbiOJpZCfCXwELn3MlALHAtkX8MHwYuO2DeUR0zM8sB7gTOAhYDd458CfjKORcxL+AcYOWo6duB2/2uKwz79STwSWATMM2bNw3Y5L3/MbB81Pr71pvKL6CU0H8cFwNPA0boiru4A48nsBI4x3sf561nfu/DEfYvE9h+YJ1BOY5ACVAD5HjH5GngU0E4hkA58NFEjxmwHPjxqPn7refXK6Ja6Hz8Bzai1psXsbyfpacDbwOFzrnd3qJ6oNB7H6n7/X3gb4BhbzoXaHXODXrTo/dj3z56y9u89aeymUAj8FOvW+knZpZKQI6jc64O+D9ANbCb0DFZQ7CO4YijPWZT8lhGWqAHipmlAb8GvuWcax+9zIW+9iN2TKmZXQE0OOfW+F3LJIoDzgAecM6dDnTx8U91ILKPo9eFcBWhL65iIJWDuyoCJ5KPWaQFeh1QNmq61JsXccwsnlCY/9I594Q3e4+ZTfOWTwMavPmRuN/nAVea2Q5gBaFul/uALDOL89YZvR/79tFbngnsPZ4FT0AtUOuce9ub/hWhgA/KcVwKbHfONTrnBoAnCB3XIB3DEUd7zKbksYy0QH8HmOudZU8gdILmKZ9rOmpmZsB/Ahucc/eOWvQUMHK2/DpCfesj87/onXE/G2gb9fNwSnLO3e6cK3XOlRM6Tqucc58HXgSu8VY7cB9H9v0ab/0p3UpyztUDNWY2z5t1CbCe4BzHauBsM0vx/mZH9i8wx3CUoz1mK4FLzSzb+yVzqTfPX3534k/gZMYyYDOwFbjD73omuA/nE/pJtxZ433stI9Tf+AKwBXgeyPHWN0Kje7YCHxIadeD7fhzF/i4BnvbezwJWA1XA40CiNz/Jm67yls/yu+5x7ttpQKV3LH8LZAfpOALfBTYCHwE/BxIj/RgCjxI6JzBA6FfWDRM5ZsCXvX2tAq73e7+cc7r0X0QkKCKty0VERA5BgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgGhQBcRCYj/DyEiqxibA8bTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 3')\n",
    "plt.plot(range(len(mlpClassifier3.loss_curve_)),mlpClassifier3.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9e0ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix3 = metrics.confusion_matrix(y_test,y_pred3)\n",
    "sns.heatmap(pd.DataFrame(confMatrix3),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dee31ccf",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76355edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.04078262\n",
      "Iteration 2, loss = 0.91915116\n",
      "Iteration 3, loss = 0.82163672\n",
      "Iteration 4, loss = 0.74247299\n",
      "Iteration 5, loss = 0.68051323\n",
      "Iteration 6, loss = 0.63110580\n",
      "Iteration 7, loss = 0.59457333\n",
      "Iteration 8, loss = 0.56753875\n",
      "Iteration 9, loss = 0.54587708\n",
      "Iteration 10, loss = 0.52982160\n",
      "Iteration 11, loss = 0.51609411\n",
      "Iteration 12, loss = 0.50439755\n",
      "Iteration 13, loss = 0.49363398\n",
      "Iteration 14, loss = 0.48378363\n",
      "Iteration 15, loss = 0.47452632\n",
      "Iteration 16, loss = 0.46555753\n",
      "Iteration 17, loss = 0.45737138\n",
      "Iteration 18, loss = 0.44979583\n",
      "Iteration 19, loss = 0.44267283\n",
      "Iteration 20, loss = 0.43583789\n",
      "Iteration 21, loss = 0.42874688\n",
      "Iteration 22, loss = 0.42239075\n",
      "Iteration 23, loss = 0.41574209\n",
      "Iteration 24, loss = 0.40925842\n",
      "Iteration 25, loss = 0.40301266\n",
      "Iteration 26, loss = 0.39664555\n",
      "Iteration 27, loss = 0.39087218\n",
      "Iteration 28, loss = 0.38477284\n",
      "Iteration 29, loss = 0.37916995\n",
      "Iteration 30, loss = 0.37422572\n",
      "Iteration 31, loss = 0.37082154\n",
      "Iteration 32, loss = 0.36583593\n",
      "Iteration 33, loss = 0.36046863\n",
      "Iteration 34, loss = 0.35469453\n",
      "Iteration 35, loss = 0.34910551\n",
      "Iteration 36, loss = 0.34418905\n",
      "Iteration 37, loss = 0.34103627\n",
      "Iteration 38, loss = 0.33945458\n",
      "Iteration 39, loss = 0.33582967\n",
      "Iteration 40, loss = 0.33128659\n",
      "Iteration 41, loss = 0.32642259\n",
      "Iteration 42, loss = 0.32243214\n",
      "Iteration 43, loss = 0.31798448\n",
      "Iteration 44, loss = 0.31611460\n",
      "Iteration 45, loss = 0.31525116\n",
      "Iteration 46, loss = 0.31402767\n",
      "Iteration 47, loss = 0.31246445\n",
      "Iteration 48, loss = 0.31030406\n",
      "Iteration 49, loss = 0.30798627\n",
      "Iteration 50, loss = 0.30577838\n",
      "Iteration 51, loss = 0.30405664\n",
      "Iteration 52, loss = 0.30342083\n",
      "Iteration 53, loss = 0.30039195\n",
      "Iteration 54, loss = 0.29279697\n",
      "Iteration 55, loss = 0.28700200\n",
      "Iteration 56, loss = 0.28329686\n",
      "Iteration 57, loss = 0.28295054\n",
      "Iteration 58, loss = 0.28189607\n",
      "Iteration 59, loss = 0.27931995\n",
      "Iteration 60, loss = 0.27588911\n",
      "Iteration 61, loss = 0.27209923\n",
      "Iteration 62, loss = 0.26867625\n",
      "Iteration 63, loss = 0.26538084\n",
      "Iteration 64, loss = 0.26279134\n",
      "Iteration 65, loss = 0.26045942\n",
      "Iteration 66, loss = 0.25838636\n",
      "Iteration 67, loss = 0.25635399\n",
      "Iteration 68, loss = 0.25445406\n",
      "Iteration 69, loss = 0.25299088\n",
      "Iteration 70, loss = 0.25298656\n",
      "Iteration 71, loss = 0.25352367\n",
      "Iteration 72, loss = 0.25222903\n",
      "Iteration 73, loss = 0.25033941\n",
      "Iteration 74, loss = 0.24765871\n",
      "Iteration 75, loss = 0.24424958\n",
      "Iteration 76, loss = 0.24081697\n",
      "Iteration 77, loss = 0.23740381\n",
      "Iteration 78, loss = 0.23375395\n",
      "Iteration 79, loss = 0.23091102\n",
      "Iteration 80, loss = 0.22876909\n",
      "Iteration 81, loss = 0.22614101\n",
      "Iteration 82, loss = 0.22466840\n",
      "Iteration 83, loss = 0.22289401\n",
      "Iteration 84, loss = 0.22124655\n",
      "Iteration 85, loss = 0.21938724\n",
      "Iteration 86, loss = 0.21727040\n",
      "Iteration 87, loss = 0.21491728\n",
      "Iteration 88, loss = 0.21321244\n",
      "Iteration 89, loss = 0.21119729\n",
      "Iteration 90, loss = 0.20936090\n",
      "Iteration 91, loss = 0.20907777\n",
      "Iteration 92, loss = 0.20899066\n",
      "Iteration 93, loss = 0.21049718\n",
      "Iteration 94, loss = 0.21136662\n",
      "Iteration 95, loss = 0.21162328\n",
      "Iteration 96, loss = 0.21005403\n",
      "Iteration 97, loss = 0.20647121\n",
      "Iteration 98, loss = 0.20298104\n",
      "Iteration 99, loss = 0.19985427\n",
      "Iteration 100, loss = 0.19664343\n",
      "Iteration 101, loss = 0.19369430\n",
      "Iteration 102, loss = 0.19149221\n",
      "Iteration 103, loss = 0.18952043\n",
      "Iteration 104, loss = 0.18819300\n",
      "Iteration 105, loss = 0.18712832\n",
      "Iteration 106, loss = 0.18629155\n",
      "Iteration 107, loss = 0.18640306\n",
      "Iteration 108, loss = 0.18564217\n",
      "Iteration 109, loss = 0.18438588\n",
      "Iteration 110, loss = 0.18305091\n",
      "Iteration 111, loss = 0.18180433\n",
      "Iteration 112, loss = 0.18097224\n",
      "Iteration 113, loss = 0.18026908\n",
      "Iteration 114, loss = 0.17887357\n",
      "Iteration 115, loss = 0.17691117\n",
      "Iteration 116, loss = 0.17679055\n",
      "Iteration 117, loss = 0.17769357\n",
      "Iteration 118, loss = 0.17702228\n",
      "Iteration 119, loss = 0.17516575\n",
      "Iteration 120, loss = 0.17461029\n",
      "Iteration 121, loss = 0.17414002\n",
      "Iteration 122, loss = 0.17249872\n",
      "Iteration 123, loss = 0.17089271\n",
      "Iteration 124, loss = 0.17037667\n",
      "Iteration 125, loss = 0.16940549\n",
      "Iteration 126, loss = 0.16768521\n",
      "Iteration 127, loss = 0.16616040\n",
      "Iteration 128, loss = 0.16540850\n",
      "Iteration 129, loss = 0.16384063\n",
      "Iteration 130, loss = 0.16269507\n",
      "Iteration 131, loss = 0.16330625\n",
      "Iteration 132, loss = 0.16168238\n",
      "Iteration 133, loss = 0.15947231\n",
      "Iteration 134, loss = 0.15772484\n",
      "Iteration 135, loss = 0.15871176\n",
      "Iteration 136, loss = 0.16095948\n",
      "Iteration 137, loss = 0.16235652\n",
      "Iteration 138, loss = 0.15963785\n",
      "Iteration 139, loss = 0.15529643\n",
      "Iteration 140, loss = 0.15323572\n",
      "Iteration 141, loss = 0.15351133\n",
      "Iteration 142, loss = 0.15661580\n",
      "Iteration 143, loss = 0.15751828\n",
      "Iteration 144, loss = 0.15705189\n",
      "Iteration 145, loss = 0.15582079\n",
      "Iteration 146, loss = 0.15395693\n",
      "Iteration 147, loss = 0.15308778\n",
      "Iteration 148, loss = 0.15210454\n",
      "Iteration 149, loss = 0.15144306\n",
      "Iteration 150, loss = 0.14964374\n",
      "Iteration 151, loss = 0.14668820\n",
      "Iteration 152, loss = 0.14583756\n",
      "Iteration 153, loss = 0.14658922\n",
      "Iteration 154, loss = 0.14775196\n",
      "Iteration 155, loss = 0.14845571\n",
      "Iteration 156, loss = 0.14767993\n",
      "Iteration 157, loss = 0.14622543\n",
      "Iteration 158, loss = 0.14447554\n",
      "Iteration 159, loss = 0.14181770\n",
      "Iteration 160, loss = 0.13960249\n",
      "Iteration 161, loss = 0.13804153\n",
      "Iteration 162, loss = 0.13749246\n",
      "Iteration 163, loss = 0.13732543\n",
      "Iteration 164, loss = 0.13785171\n",
      "Iteration 165, loss = 0.13878907\n",
      "Iteration 166, loss = 0.13908425\n",
      "Iteration 167, loss = 0.13777417\n",
      "Iteration 168, loss = 0.13523915\n",
      "Iteration 169, loss = 0.13228711\n",
      "Iteration 170, loss = 0.13050300\n",
      "Iteration 171, loss = 0.12948721\n",
      "Iteration 172, loss = 0.12890271\n",
      "Iteration 173, loss = 0.12920438\n",
      "Iteration 174, loss = 0.12908049\n",
      "Iteration 175, loss = 0.12922188\n",
      "Iteration 176, loss = 0.12925137\n",
      "Iteration 177, loss = 0.12782074\n",
      "Iteration 178, loss = 0.12552940\n",
      "Iteration 179, loss = 0.12460633\n",
      "Iteration 180, loss = 0.12532699\n",
      "Iteration 181, loss = 0.12707687\n",
      "Iteration 182, loss = 0.12756455\n",
      "Iteration 183, loss = 0.12600227\n",
      "Iteration 184, loss = 0.12349278\n",
      "Iteration 185, loss = 0.12089192\n",
      "Iteration 186, loss = 0.11954537\n",
      "Iteration 187, loss = 0.11871997\n",
      "Iteration 188, loss = 0.11939982\n",
      "Iteration 189, loss = 0.12035531\n",
      "Iteration 190, loss = 0.12069195\n",
      "Iteration 191, loss = 0.12013460\n",
      "Iteration 192, loss = 0.11944925\n",
      "Iteration 193, loss = 0.11889837\n",
      "Iteration 194, loss = 0.12003832\n",
      "Iteration 195, loss = 0.12098972\n",
      "Iteration 196, loss = 0.12100105\n",
      "Iteration 197, loss = 0.11977943\n",
      "Iteration 198, loss = 0.11783856\n",
      "Iteration 199, loss = 0.11575979\n",
      "Iteration 200, loss = 0.11420784\n",
      "Iteration 201, loss = 0.11210433\n",
      "Iteration 202, loss = 0.10982521\n",
      "Iteration 203, loss = 0.11005472\n",
      "Iteration 204, loss = 0.11243965\n",
      "Iteration 205, loss = 0.11220475\n",
      "Iteration 206, loss = 0.11142288\n",
      "Iteration 207, loss = 0.11095742\n",
      "Iteration 208, loss = 0.10944878\n",
      "Iteration 209, loss = 0.10813246\n",
      "Iteration 210, loss = 0.10747363\n",
      "Iteration 211, loss = 0.10665507\n",
      "Iteration 212, loss = 0.10556677\n",
      "Iteration 213, loss = 0.10468785\n",
      "Iteration 214, loss = 0.10414256\n",
      "Iteration 215, loss = 0.10336764\n",
      "Iteration 216, loss = 0.10302073\n",
      "Iteration 217, loss = 0.10285740\n",
      "Iteration 218, loss = 0.10250018\n",
      "Iteration 219, loss = 0.10244650\n",
      "Iteration 220, loss = 0.10186734\n",
      "Iteration 221, loss = 0.10096531\n",
      "Iteration 222, loss = 0.10084674\n",
      "Iteration 223, loss = 0.10086856\n",
      "Iteration 224, loss = 0.10147867\n",
      "Iteration 225, loss = 0.10198761\n",
      "Iteration 226, loss = 0.10076506\n",
      "Iteration 227, loss = 0.09987204\n",
      "Iteration 228, loss = 0.09919004\n",
      "Iteration 229, loss = 0.09873417\n",
      "Iteration 230, loss = 0.09861648\n",
      "Iteration 231, loss = 0.09808530\n",
      "Iteration 232, loss = 0.09742786\n",
      "Iteration 233, loss = 0.09713519\n",
      "Iteration 234, loss = 0.09722695\n",
      "Iteration 235, loss = 0.09688314\n",
      "Iteration 236, loss = 0.09606126\n",
      "Iteration 237, loss = 0.09551629\n",
      "Iteration 238, loss = 0.09538537\n",
      "Iteration 239, loss = 0.09375896\n",
      "Iteration 240, loss = 0.09216155\n",
      "Iteration 241, loss = 0.09418342\n",
      "Iteration 242, loss = 0.09806119\n",
      "Iteration 243, loss = 0.09974870\n",
      "Iteration 244, loss = 0.09521517\n",
      "Iteration 245, loss = 0.08995477\n",
      "Iteration 246, loss = 0.08956689\n",
      "Iteration 247, loss = 0.09394685\n",
      "Iteration 248, loss = 0.09574109\n",
      "Iteration 249, loss = 0.09413923\n",
      "Iteration 250, loss = 0.09154048\n",
      "Iteration 251, loss = 0.08879367\n",
      "Iteration 252, loss = 0.08713576\n",
      "Iteration 253, loss = 0.08682054\n",
      "Iteration 254, loss = 0.08740263\n",
      "Iteration 255, loss = 0.08891963\n",
      "Iteration 256, loss = 0.08903806\n",
      "Iteration 257, loss = 0.08828653\n",
      "Iteration 258, loss = 0.08704103\n",
      "Iteration 259, loss = 0.08475698\n",
      "Iteration 260, loss = 0.08270623\n",
      "Iteration 261, loss = 0.08320060\n",
      "Iteration 262, loss = 0.08435909\n",
      "Iteration 263, loss = 0.08335315\n",
      "Iteration 264, loss = 0.08283399\n",
      "Iteration 265, loss = 0.08376529\n",
      "Iteration 266, loss = 0.08597831\n",
      "Iteration 267, loss = 0.08741771\n",
      "Iteration 268, loss = 0.08696924\n",
      "Iteration 269, loss = 0.08475284\n",
      "Iteration 270, loss = 0.08280334\n",
      "Iteration 271, loss = 0.08250106\n",
      "Iteration 272, loss = 0.08266456\n",
      "Iteration 273, loss = 0.08233939\n",
      "Iteration 274, loss = 0.08008081\n",
      "Iteration 275, loss = 0.07824316\n",
      "Iteration 276, loss = 0.07789252\n",
      "Iteration 277, loss = 0.07804369\n",
      "Iteration 278, loss = 0.07798005\n",
      "Iteration 279, loss = 0.07806398\n",
      "Iteration 280, loss = 0.07692229\n",
      "Iteration 281, loss = 0.07686893\n",
      "Iteration 282, loss = 0.07726388\n",
      "Iteration 283, loss = 0.07702667\n",
      "Iteration 284, loss = 0.07564687\n",
      "Iteration 285, loss = 0.07473954\n",
      "Iteration 286, loss = 0.07511930\n",
      "Iteration 287, loss = 0.07581246\n",
      "Iteration 288, loss = 0.07542435\n",
      "Iteration 289, loss = 0.07423142\n",
      "Iteration 290, loss = 0.07381581\n",
      "Iteration 291, loss = 0.07266621\n",
      "Iteration 292, loss = 0.07460735\n",
      "Iteration 293, loss = 0.07590761\n",
      "Iteration 294, loss = 0.07430160\n",
      "Iteration 295, loss = 0.07241737\n",
      "Iteration 296, loss = 0.07118326\n",
      "Iteration 297, loss = 0.07059661\n",
      "Iteration 298, loss = 0.07017587\n",
      "Iteration 299, loss = 0.07029785\n",
      "Iteration 300, loss = 0.07060934\n",
      "Iteration 301, loss = 0.07082657\n",
      "Iteration 302, loss = 0.07061320\n",
      "Iteration 303, loss = 0.07031865\n",
      "Iteration 304, loss = 0.07011079\n",
      "Iteration 305, loss = 0.06846526\n",
      "Iteration 306, loss = 0.06990294\n",
      "Iteration 307, loss = 0.07270889\n",
      "Iteration 308, loss = 0.07576782\n",
      "Iteration 309, loss = 0.07607317\n",
      "Iteration 310, loss = 0.07429495\n",
      "Iteration 311, loss = 0.07225482\n",
      "Iteration 312, loss = 0.07004203\n",
      "Iteration 313, loss = 0.06813493\n",
      "Iteration 314, loss = 0.06710222\n",
      "Iteration 315, loss = 0.06609551\n",
      "Iteration 316, loss = 0.06553447\n",
      "Iteration 317, loss = 0.06716414\n",
      "Iteration 318, loss = 0.06463707\n",
      "Iteration 319, loss = 0.06493473\n",
      "Iteration 320, loss = 0.06871814\n",
      "Iteration 321, loss = 0.07166512\n",
      "Iteration 322, loss = 0.07183389\n",
      "Iteration 323, loss = 0.06923905\n",
      "Iteration 324, loss = 0.06595595\n",
      "Iteration 325, loss = 0.06334267\n",
      "Iteration 326, loss = 0.06267684\n",
      "Iteration 327, loss = 0.06213031\n",
      "Iteration 328, loss = 0.06164319\n",
      "Iteration 329, loss = 0.06090773\n",
      "Iteration 330, loss = 0.06422733\n",
      "Iteration 331, loss = 0.06623386\n",
      "Iteration 332, loss = 0.06418608\n",
      "Iteration 333, loss = 0.06068051\n",
      "Iteration 334, loss = 0.05983305\n",
      "Iteration 335, loss = 0.06278278\n",
      "Iteration 336, loss = 0.06611526\n",
      "Iteration 337, loss = 0.06348965\n",
      "Iteration 338, loss = 0.06002489\n",
      "Iteration 339, loss = 0.05930038\n",
      "Iteration 340, loss = 0.05947522\n",
      "Iteration 341, loss = 0.05957453\n",
      "Iteration 342, loss = 0.05930942\n",
      "Iteration 343, loss = 0.05785103\n",
      "Iteration 344, loss = 0.05693067\n",
      "Iteration 345, loss = 0.05587720\n",
      "Iteration 346, loss = 0.05562457\n",
      "Iteration 347, loss = 0.05541017\n",
      "Iteration 348, loss = 0.05552422\n",
      "Iteration 349, loss = 0.05662269\n",
      "Iteration 350, loss = 0.05844674\n",
      "Iteration 351, loss = 0.05981646\n",
      "Iteration 352, loss = 0.05986135\n",
      "Iteration 353, loss = 0.05925795\n",
      "Iteration 354, loss = 0.05823601\n",
      "Iteration 355, loss = 0.05637158\n",
      "Iteration 356, loss = 0.05491286\n",
      "Iteration 357, loss = 0.05573866\n",
      "Iteration 358, loss = 0.05879490\n",
      "Iteration 359, loss = 0.06273415\n",
      "Iteration 360, loss = 0.06262624\n",
      "Iteration 361, loss = 0.05907707\n",
      "Iteration 362, loss = 0.05549892\n",
      "Iteration 363, loss = 0.05375827\n",
      "Iteration 364, loss = 0.05327480\n",
      "Iteration 365, loss = 0.05384464\n",
      "Iteration 366, loss = 0.05374462\n",
      "Iteration 367, loss = 0.05367571\n",
      "Iteration 368, loss = 0.05341823\n",
      "Iteration 369, loss = 0.05298151\n",
      "Iteration 370, loss = 0.05218958\n",
      "Iteration 371, loss = 0.05138601\n",
      "Iteration 372, loss = 0.05018343\n",
      "Iteration 373, loss = 0.04929249\n",
      "Iteration 374, loss = 0.04947545\n",
      "Iteration 375, loss = 0.04996195\n",
      "Iteration 376, loss = 0.05053759\n",
      "Iteration 377, loss = 0.04989019\n",
      "Iteration 378, loss = 0.04897875\n",
      "Iteration 379, loss = 0.04953414\n",
      "Iteration 380, loss = 0.05038710\n",
      "Iteration 381, loss = 0.05129815\n",
      "Iteration 382, loss = 0.05176399\n",
      "Iteration 383, loss = 0.05099346\n",
      "Iteration 384, loss = 0.04922460\n",
      "Iteration 385, loss = 0.04740309\n",
      "Iteration 386, loss = 0.04596430\n",
      "Iteration 387, loss = 0.04509381\n",
      "Iteration 388, loss = 0.04456901\n",
      "Iteration 389, loss = 0.04449308\n",
      "Iteration 390, loss = 0.04447080\n",
      "Iteration 391, loss = 0.04443994\n",
      "Iteration 392, loss = 0.04435366\n",
      "Iteration 393, loss = 0.04451441\n",
      "Iteration 394, loss = 0.04445705\n",
      "Iteration 395, loss = 0.04472802\n",
      "Iteration 396, loss = 0.04464694\n",
      "Iteration 397, loss = 0.04412133\n",
      "Iteration 398, loss = 0.04342249\n",
      "Iteration 399, loss = 0.04277738\n",
      "Iteration 400, loss = 0.04230699\n",
      "Iteration 401, loss = 0.04219850\n",
      "Iteration 402, loss = 0.04213270\n",
      "Iteration 403, loss = 0.04216407\n",
      "Iteration 404, loss = 0.04193105\n",
      "Iteration 405, loss = 0.04187834\n",
      "Iteration 406, loss = 0.04175217\n",
      "Iteration 407, loss = 0.04112337\n",
      "Iteration 408, loss = 0.04063553\n",
      "Iteration 409, loss = 0.04096218\n",
      "Iteration 410, loss = 0.04137607\n",
      "Iteration 411, loss = 0.04168717\n",
      "Iteration 412, loss = 0.04121288\n",
      "Iteration 413, loss = 0.04061852\n",
      "Iteration 414, loss = 0.04069057\n",
      "Iteration 415, loss = 0.04078388\n",
      "Iteration 416, loss = 0.04072296\n",
      "Iteration 417, loss = 0.04061882\n",
      "Iteration 418, loss = 0.04133981\n",
      "Iteration 419, loss = 0.04134538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier4 = MLPClassifier(max_iter=2000,activation='relu', verbose=True, hidden_layer_sizes=(15,),random_state=1,learning_rate_init=0.01)\n",
    "mlpClassifier4.fit(X_train, y_train)\n",
    "y_pred4 = mlpClassifier4.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e608c02",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d6f327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.26881720430107"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad7b34cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde3616e8e0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 4')\n",
    "plt.plot(range(len(mlpClassifier4.loss_curve_)),mlpClassifier4.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549ea0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix4 = metrics.confusion_matrix(y_test,y_pred4)\n",
    "sns.heatmap(pd.DataFrame(confMatrix4),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13151b0a",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "295b1225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79650353\n",
      "Iteration 2, loss = 0.78696497\n",
      "Iteration 3, loss = 0.77849627\n",
      "Iteration 4, loss = 0.77070068\n",
      "Iteration 5, loss = 0.76320091\n",
      "Iteration 6, loss = 0.75609833\n",
      "Iteration 7, loss = 0.74941939\n",
      "Iteration 8, loss = 0.74287381\n",
      "Iteration 9, loss = 0.73671752\n",
      "Iteration 10, loss = 0.73095124\n",
      "Iteration 11, loss = 0.72530121\n",
      "Iteration 12, loss = 0.72028822\n",
      "Iteration 13, loss = 0.71492184\n",
      "Iteration 14, loss = 0.71006057\n",
      "Iteration 15, loss = 0.70510713\n",
      "Iteration 16, loss = 0.70050926\n",
      "Iteration 17, loss = 0.69581469\n",
      "Iteration 18, loss = 0.69161255\n",
      "Iteration 19, loss = 0.68731577\n",
      "Iteration 20, loss = 0.68316527\n",
      "Iteration 21, loss = 0.67899828\n",
      "Iteration 22, loss = 0.67485210\n",
      "Iteration 23, loss = 0.67092627\n",
      "Iteration 24, loss = 0.66686230\n",
      "Iteration 25, loss = 0.66298534\n",
      "Iteration 26, loss = 0.65927977\n",
      "Iteration 27, loss = 0.65557346\n",
      "Iteration 28, loss = 0.65195820\n",
      "Iteration 29, loss = 0.64841033\n",
      "Iteration 30, loss = 0.64476506\n",
      "Iteration 31, loss = 0.64136171\n",
      "Iteration 32, loss = 0.63795962\n",
      "Iteration 33, loss = 0.63481846\n",
      "Iteration 34, loss = 0.63175016\n",
      "Iteration 35, loss = 0.62869931\n",
      "Iteration 36, loss = 0.62578279\n",
      "Iteration 37, loss = 0.62297322\n",
      "Iteration 38, loss = 0.62003707\n",
      "Iteration 39, loss = 0.61730684\n",
      "Iteration 40, loss = 0.61459624\n",
      "Iteration 41, loss = 0.61188154\n",
      "Iteration 42, loss = 0.60924504\n",
      "Iteration 43, loss = 0.60658682\n",
      "Iteration 44, loss = 0.60396066\n",
      "Iteration 45, loss = 0.60145934\n",
      "Iteration 46, loss = 0.59893729\n",
      "Iteration 47, loss = 0.59656277\n",
      "Iteration 48, loss = 0.59432583\n",
      "Iteration 49, loss = 0.59204259\n",
      "Iteration 50, loss = 0.58985433\n",
      "Iteration 51, loss = 0.58768246\n",
      "Iteration 52, loss = 0.58544514\n",
      "Iteration 53, loss = 0.58319379\n",
      "Iteration 54, loss = 0.58098521\n",
      "Iteration 55, loss = 0.57868202\n",
      "Iteration 56, loss = 0.57636786\n",
      "Iteration 57, loss = 0.57414772\n",
      "Iteration 58, loss = 0.57184246\n",
      "Iteration 59, loss = 0.56951440\n",
      "Iteration 60, loss = 0.56732846\n",
      "Iteration 61, loss = 0.56507344\n",
      "Iteration 62, loss = 0.56292518\n",
      "Iteration 63, loss = 0.56075019\n",
      "Iteration 64, loss = 0.55861387\n",
      "Iteration 65, loss = 0.55647855\n",
      "Iteration 66, loss = 0.55426918\n",
      "Iteration 67, loss = 0.55205485\n",
      "Iteration 68, loss = 0.54997661\n",
      "Iteration 69, loss = 0.54773520\n",
      "Iteration 70, loss = 0.54558169\n",
      "Iteration 71, loss = 0.54325458\n",
      "Iteration 72, loss = 0.54102472\n",
      "Iteration 73, loss = 0.53887003\n",
      "Iteration 74, loss = 0.53660269\n",
      "Iteration 75, loss = 0.53432241\n",
      "Iteration 76, loss = 0.53202108\n",
      "Iteration 77, loss = 0.52976204\n",
      "Iteration 78, loss = 0.52737463\n",
      "Iteration 79, loss = 0.52499860\n",
      "Iteration 80, loss = 0.52265275\n",
      "Iteration 81, loss = 0.52032420\n",
      "Iteration 82, loss = 0.51799158\n",
      "Iteration 83, loss = 0.51569327\n",
      "Iteration 84, loss = 0.51328026\n",
      "Iteration 85, loss = 0.51094073\n",
      "Iteration 86, loss = 0.50853682\n",
      "Iteration 87, loss = 0.50616147\n",
      "Iteration 88, loss = 0.50392414\n",
      "Iteration 89, loss = 0.50174560\n",
      "Iteration 90, loss = 0.49968710\n",
      "Iteration 91, loss = 0.49766734\n",
      "Iteration 92, loss = 0.49571578\n",
      "Iteration 93, loss = 0.49379862\n",
      "Iteration 94, loss = 0.49203074\n",
      "Iteration 95, loss = 0.49016900\n",
      "Iteration 96, loss = 0.48840462\n",
      "Iteration 97, loss = 0.48658826\n",
      "Iteration 98, loss = 0.48485093\n",
      "Iteration 99, loss = 0.48305783\n",
      "Iteration 100, loss = 0.48130362\n",
      "Iteration 101, loss = 0.47957841\n",
      "Iteration 102, loss = 0.47779637\n",
      "Iteration 103, loss = 0.47611454\n",
      "Iteration 104, loss = 0.47440801\n",
      "Iteration 105, loss = 0.47279083\n",
      "Iteration 106, loss = 0.47112092\n",
      "Iteration 107, loss = 0.46956426\n",
      "Iteration 108, loss = 0.46793454\n",
      "Iteration 109, loss = 0.46634209\n",
      "Iteration 110, loss = 0.46473413\n",
      "Iteration 111, loss = 0.46313171\n",
      "Iteration 112, loss = 0.46153873\n",
      "Iteration 113, loss = 0.45998567\n",
      "Iteration 114, loss = 0.45857346\n",
      "Iteration 115, loss = 0.45710609\n",
      "Iteration 116, loss = 0.45570408\n",
      "Iteration 117, loss = 0.45435810\n",
      "Iteration 118, loss = 0.45301078\n",
      "Iteration 119, loss = 0.45166852\n",
      "Iteration 120, loss = 0.45036753\n",
      "Iteration 121, loss = 0.44910392\n",
      "Iteration 122, loss = 0.44779986\n",
      "Iteration 123, loss = 0.44661238\n",
      "Iteration 124, loss = 0.44529320\n",
      "Iteration 125, loss = 0.44409449\n",
      "Iteration 126, loss = 0.44289364\n",
      "Iteration 127, loss = 0.44173794\n",
      "Iteration 128, loss = 0.44055716\n",
      "Iteration 129, loss = 0.43943328\n",
      "Iteration 130, loss = 0.43833142\n",
      "Iteration 131, loss = 0.43729006\n",
      "Iteration 132, loss = 0.43624277\n",
      "Iteration 133, loss = 0.43514595\n",
      "Iteration 134, loss = 0.43414222\n",
      "Iteration 135, loss = 0.43312370\n",
      "Iteration 136, loss = 0.43210267\n",
      "Iteration 137, loss = 0.43102324\n",
      "Iteration 138, loss = 0.42995127\n",
      "Iteration 139, loss = 0.42895529\n",
      "Iteration 140, loss = 0.42796548\n",
      "Iteration 141, loss = 0.42698715\n",
      "Iteration 142, loss = 0.42608524\n",
      "Iteration 143, loss = 0.42513206\n",
      "Iteration 144, loss = 0.42424474\n",
      "Iteration 145, loss = 0.42329083\n",
      "Iteration 146, loss = 0.42236271\n",
      "Iteration 147, loss = 0.42143328\n",
      "Iteration 148, loss = 0.42046077\n",
      "Iteration 149, loss = 0.41943868\n",
      "Iteration 150, loss = 0.41848411\n",
      "Iteration 151, loss = 0.41754819\n",
      "Iteration 152, loss = 0.41664469\n",
      "Iteration 153, loss = 0.41567989\n",
      "Iteration 154, loss = 0.41467470\n",
      "Iteration 155, loss = 0.41370693\n",
      "Iteration 156, loss = 0.41267496\n",
      "Iteration 157, loss = 0.41162346\n",
      "Iteration 158, loss = 0.41059547\n",
      "Iteration 159, loss = 0.40959504\n",
      "Iteration 160, loss = 0.40859995\n",
      "Iteration 161, loss = 0.40760672\n",
      "Iteration 162, loss = 0.40652470\n",
      "Iteration 163, loss = 0.40555051\n",
      "Iteration 164, loss = 0.40455757\n",
      "Iteration 165, loss = 0.40359889\n",
      "Iteration 166, loss = 0.40269749\n",
      "Iteration 167, loss = 0.40175439\n",
      "Iteration 168, loss = 0.40077393\n",
      "Iteration 169, loss = 0.39977096\n",
      "Iteration 170, loss = 0.39882978\n",
      "Iteration 171, loss = 0.39788594\n",
      "Iteration 172, loss = 0.39693053\n",
      "Iteration 173, loss = 0.39603114\n",
      "Iteration 174, loss = 0.39518242\n",
      "Iteration 175, loss = 0.39438684\n",
      "Iteration 176, loss = 0.39357840\n",
      "Iteration 177, loss = 0.39271219\n",
      "Iteration 178, loss = 0.39183396\n",
      "Iteration 179, loss = 0.39097128\n",
      "Iteration 180, loss = 0.39015184\n",
      "Iteration 181, loss = 0.38915780\n",
      "Iteration 182, loss = 0.38822036\n",
      "Iteration 183, loss = 0.38726844\n",
      "Iteration 184, loss = 0.38630280\n",
      "Iteration 185, loss = 0.38529308\n",
      "Iteration 186, loss = 0.38433188\n",
      "Iteration 187, loss = 0.38348196\n",
      "Iteration 188, loss = 0.38263620\n",
      "Iteration 189, loss = 0.38182746\n",
      "Iteration 190, loss = 0.38098306\n",
      "Iteration 191, loss = 0.38014991\n",
      "Iteration 192, loss = 0.37920473\n",
      "Iteration 193, loss = 0.37834312\n",
      "Iteration 194, loss = 0.37748865\n",
      "Iteration 195, loss = 0.37648083\n",
      "Iteration 196, loss = 0.37547293\n",
      "Iteration 197, loss = 0.37441194\n",
      "Iteration 198, loss = 0.37345017\n",
      "Iteration 199, loss = 0.37253319\n",
      "Iteration 200, loss = 0.37163335\n",
      "Iteration 201, loss = 0.37069161\n",
      "Iteration 202, loss = 0.36983560\n",
      "Iteration 203, loss = 0.36891776\n",
      "Iteration 204, loss = 0.36800536\n",
      "Iteration 205, loss = 0.36713639\n",
      "Iteration 206, loss = 0.36623985\n",
      "Iteration 207, loss = 0.36540330\n",
      "Iteration 208, loss = 0.36448919\n",
      "Iteration 209, loss = 0.36356718\n",
      "Iteration 210, loss = 0.36275316\n",
      "Iteration 211, loss = 0.36189717\n",
      "Iteration 212, loss = 0.36105195\n",
      "Iteration 213, loss = 0.36027244\n",
      "Iteration 214, loss = 0.35951350\n",
      "Iteration 215, loss = 0.35874488\n",
      "Iteration 216, loss = 0.35793971\n",
      "Iteration 217, loss = 0.35704585\n",
      "Iteration 218, loss = 0.35619966\n",
      "Iteration 219, loss = 0.35534316\n",
      "Iteration 220, loss = 0.35439922\n",
      "Iteration 221, loss = 0.35351977\n",
      "Iteration 222, loss = 0.35255380\n",
      "Iteration 223, loss = 0.35167704\n",
      "Iteration 224, loss = 0.35080992\n",
      "Iteration 225, loss = 0.35007220\n",
      "Iteration 226, loss = 0.34922921\n",
      "Iteration 227, loss = 0.34835831\n",
      "Iteration 228, loss = 0.34748116\n",
      "Iteration 229, loss = 0.34666607\n",
      "Iteration 230, loss = 0.34592121\n",
      "Iteration 231, loss = 0.34516927\n",
      "Iteration 232, loss = 0.34442605\n",
      "Iteration 233, loss = 0.34371578\n",
      "Iteration 234, loss = 0.34290468\n",
      "Iteration 235, loss = 0.34214224\n",
      "Iteration 236, loss = 0.34134611\n",
      "Iteration 237, loss = 0.34053053\n",
      "Iteration 238, loss = 0.33969359\n",
      "Iteration 239, loss = 0.33891180\n",
      "Iteration 240, loss = 0.33811595\n",
      "Iteration 241, loss = 0.33733286\n",
      "Iteration 242, loss = 0.33656774\n",
      "Iteration 243, loss = 0.33580057\n",
      "Iteration 244, loss = 0.33502623\n",
      "Iteration 245, loss = 0.33427147\n",
      "Iteration 246, loss = 0.33346018\n",
      "Iteration 247, loss = 0.33271940\n",
      "Iteration 248, loss = 0.33189831\n",
      "Iteration 249, loss = 0.33112296\n",
      "Iteration 250, loss = 0.33035017\n",
      "Iteration 251, loss = 0.32952517\n",
      "Iteration 252, loss = 0.32879521\n",
      "Iteration 253, loss = 0.32800906\n",
      "Iteration 254, loss = 0.32725255\n",
      "Iteration 255, loss = 0.32637707\n",
      "Iteration 256, loss = 0.32552679\n",
      "Iteration 257, loss = 0.32463559\n",
      "Iteration 258, loss = 0.32387007\n",
      "Iteration 259, loss = 0.32301139\n",
      "Iteration 260, loss = 0.32230814\n",
      "Iteration 261, loss = 0.32142723\n",
      "Iteration 262, loss = 0.32077890\n",
      "Iteration 263, loss = 0.32015832\n",
      "Iteration 264, loss = 0.31931493\n",
      "Iteration 265, loss = 0.31851149\n",
      "Iteration 266, loss = 0.31772562\n",
      "Iteration 267, loss = 0.31701649\n",
      "Iteration 268, loss = 0.31636305\n",
      "Iteration 269, loss = 0.31569399\n",
      "Iteration 270, loss = 0.31495588\n",
      "Iteration 271, loss = 0.31436075\n",
      "Iteration 272, loss = 0.31359108\n",
      "Iteration 273, loss = 0.31291174\n",
      "Iteration 274, loss = 0.31215936\n",
      "Iteration 275, loss = 0.31150407\n",
      "Iteration 276, loss = 0.31079381\n",
      "Iteration 277, loss = 0.31013111\n",
      "Iteration 278, loss = 0.30938997\n",
      "Iteration 279, loss = 0.30868451\n",
      "Iteration 280, loss = 0.30801921\n",
      "Iteration 281, loss = 0.30745570\n",
      "Iteration 282, loss = 0.30672846\n",
      "Iteration 283, loss = 0.30592213\n",
      "Iteration 284, loss = 0.30506587\n",
      "Iteration 285, loss = 0.30423819\n",
      "Iteration 286, loss = 0.30354220\n",
      "Iteration 287, loss = 0.30279965\n",
      "Iteration 288, loss = 0.30207300\n",
      "Iteration 289, loss = 0.30135158\n",
      "Iteration 290, loss = 0.30067171\n",
      "Iteration 291, loss = 0.29997726\n",
      "Iteration 292, loss = 0.29932360\n",
      "Iteration 293, loss = 0.29872054\n",
      "Iteration 294, loss = 0.29812862\n",
      "Iteration 295, loss = 0.29746638\n",
      "Iteration 296, loss = 0.29686946\n",
      "Iteration 297, loss = 0.29619886\n",
      "Iteration 298, loss = 0.29551547\n",
      "Iteration 299, loss = 0.29487277\n",
      "Iteration 300, loss = 0.29427474\n",
      "Iteration 301, loss = 0.29360992\n",
      "Iteration 302, loss = 0.29288918\n",
      "Iteration 303, loss = 0.29218358\n",
      "Iteration 304, loss = 0.29143865\n",
      "Iteration 305, loss = 0.29068487\n",
      "Iteration 306, loss = 0.29003068\n",
      "Iteration 307, loss = 0.28934951\n",
      "Iteration 308, loss = 0.28871900\n",
      "Iteration 309, loss = 0.28811431\n",
      "Iteration 310, loss = 0.28752129\n",
      "Iteration 311, loss = 0.28689573\n",
      "Iteration 312, loss = 0.28628348\n",
      "Iteration 313, loss = 0.28560086\n",
      "Iteration 314, loss = 0.28494224\n",
      "Iteration 315, loss = 0.28428564\n",
      "Iteration 316, loss = 0.28372034\n",
      "Iteration 317, loss = 0.28293803\n",
      "Iteration 318, loss = 0.28222872\n",
      "Iteration 319, loss = 0.28161662\n",
      "Iteration 320, loss = 0.28100322\n",
      "Iteration 321, loss = 0.28042284\n",
      "Iteration 322, loss = 0.27983845\n",
      "Iteration 323, loss = 0.27934889\n",
      "Iteration 324, loss = 0.27888044\n",
      "Iteration 325, loss = 0.27832295\n",
      "Iteration 326, loss = 0.27774166\n",
      "Iteration 327, loss = 0.27730055\n",
      "Iteration 328, loss = 0.27681951\n",
      "Iteration 329, loss = 0.27633189\n",
      "Iteration 330, loss = 0.27574363\n",
      "Iteration 331, loss = 0.27518334\n",
      "Iteration 332, loss = 0.27464254\n",
      "Iteration 333, loss = 0.27409115\n",
      "Iteration 334, loss = 0.27354943\n",
      "Iteration 335, loss = 0.27301502\n",
      "Iteration 336, loss = 0.27251891\n",
      "Iteration 337, loss = 0.27201326\n",
      "Iteration 338, loss = 0.27129679\n",
      "Iteration 339, loss = 0.27074626\n",
      "Iteration 340, loss = 0.27023291\n",
      "Iteration 341, loss = 0.26971100\n",
      "Iteration 342, loss = 0.26899399\n",
      "Iteration 343, loss = 0.26836349\n",
      "Iteration 344, loss = 0.26775398\n",
      "Iteration 345, loss = 0.26707342\n",
      "Iteration 346, loss = 0.26648347\n",
      "Iteration 347, loss = 0.26598934\n",
      "Iteration 348, loss = 0.26566012\n",
      "Iteration 349, loss = 0.26502226\n",
      "Iteration 350, loss = 0.26441302\n",
      "Iteration 351, loss = 0.26381793\n",
      "Iteration 352, loss = 0.26330683\n",
      "Iteration 353, loss = 0.26269609\n",
      "Iteration 354, loss = 0.26216872\n",
      "Iteration 355, loss = 0.26159947\n",
      "Iteration 356, loss = 0.26105278\n",
      "Iteration 357, loss = 0.26054000\n",
      "Iteration 358, loss = 0.26003414\n",
      "Iteration 359, loss = 0.25941813\n",
      "Iteration 360, loss = 0.25888463\n",
      "Iteration 361, loss = 0.25846652\n",
      "Iteration 362, loss = 0.25801643\n",
      "Iteration 363, loss = 0.25752469\n",
      "Iteration 364, loss = 0.25700603\n",
      "Iteration 365, loss = 0.25653177\n",
      "Iteration 366, loss = 0.25600666\n",
      "Iteration 367, loss = 0.25556944\n",
      "Iteration 368, loss = 0.25498776\n",
      "Iteration 369, loss = 0.25443321\n",
      "Iteration 370, loss = 0.25393857\n",
      "Iteration 371, loss = 0.25342608\n",
      "Iteration 372, loss = 0.25293796\n",
      "Iteration 373, loss = 0.25242630\n",
      "Iteration 374, loss = 0.25189155\n",
      "Iteration 375, loss = 0.25140966\n",
      "Iteration 376, loss = 0.25091977\n",
      "Iteration 377, loss = 0.25040774\n",
      "Iteration 378, loss = 0.24987237\n",
      "Iteration 379, loss = 0.24941775\n",
      "Iteration 380, loss = 0.24887400\n",
      "Iteration 381, loss = 0.24832719\n",
      "Iteration 382, loss = 0.24777359\n",
      "Iteration 383, loss = 0.24717964\n",
      "Iteration 384, loss = 0.24644930\n",
      "Iteration 385, loss = 0.24595864\n",
      "Iteration 386, loss = 0.24581870\n",
      "Iteration 387, loss = 0.24516414\n",
      "Iteration 388, loss = 0.24468136\n",
      "Iteration 389, loss = 0.24409807\n",
      "Iteration 390, loss = 0.24347275\n",
      "Iteration 391, loss = 0.24296661\n",
      "Iteration 392, loss = 0.24235816\n",
      "Iteration 393, loss = 0.24172907\n",
      "Iteration 394, loss = 0.24113529\n",
      "Iteration 395, loss = 0.24052312\n",
      "Iteration 396, loss = 0.24001239\n",
      "Iteration 397, loss = 0.23939879\n",
      "Iteration 398, loss = 0.23881052\n",
      "Iteration 399, loss = 0.23824139\n",
      "Iteration 400, loss = 0.23761235\n",
      "Iteration 401, loss = 0.23707992\n",
      "Iteration 402, loss = 0.23653374\n",
      "Iteration 403, loss = 0.23606960\n",
      "Iteration 404, loss = 0.23554108\n",
      "Iteration 405, loss = 0.23493294\n",
      "Iteration 406, loss = 0.23431179\n",
      "Iteration 407, loss = 0.23377533\n",
      "Iteration 408, loss = 0.23323534\n",
      "Iteration 409, loss = 0.23265463\n",
      "Iteration 410, loss = 0.23231235\n",
      "Iteration 411, loss = 0.23184923\n",
      "Iteration 412, loss = 0.23143667\n",
      "Iteration 413, loss = 0.23095852\n",
      "Iteration 414, loss = 0.23055204\n",
      "Iteration 415, loss = 0.23013051\n",
      "Iteration 416, loss = 0.22965290\n",
      "Iteration 417, loss = 0.22909195\n",
      "Iteration 418, loss = 0.22851794\n",
      "Iteration 419, loss = 0.22795652\n",
      "Iteration 420, loss = 0.22737550\n",
      "Iteration 421, loss = 0.22677641\n",
      "Iteration 422, loss = 0.22630818\n",
      "Iteration 423, loss = 0.22585563\n",
      "Iteration 424, loss = 0.22550721\n",
      "Iteration 425, loss = 0.22516548\n",
      "Iteration 426, loss = 0.22460972\n",
      "Iteration 427, loss = 0.22407775\n",
      "Iteration 428, loss = 0.22348881\n",
      "Iteration 429, loss = 0.22288929\n",
      "Iteration 430, loss = 0.22227216\n",
      "Iteration 431, loss = 0.22175749\n",
      "Iteration 432, loss = 0.22120776\n",
      "Iteration 433, loss = 0.22069872\n",
      "Iteration 434, loss = 0.22024470\n",
      "Iteration 435, loss = 0.21979282\n",
      "Iteration 436, loss = 0.21931765\n",
      "Iteration 437, loss = 0.21890501\n",
      "Iteration 438, loss = 0.21843121\n",
      "Iteration 439, loss = 0.21801261\n",
      "Iteration 440, loss = 0.21751622\n",
      "Iteration 441, loss = 0.21697591\n",
      "Iteration 442, loss = 0.21647620\n",
      "Iteration 443, loss = 0.21599195\n",
      "Iteration 444, loss = 0.21552452\n",
      "Iteration 445, loss = 0.21505690\n",
      "Iteration 446, loss = 0.21463513\n",
      "Iteration 447, loss = 0.21416789\n",
      "Iteration 448, loss = 0.21377314\n",
      "Iteration 449, loss = 0.21333390\n",
      "Iteration 450, loss = 0.21290977\n",
      "Iteration 451, loss = 0.21245778\n",
      "Iteration 452, loss = 0.21198317\n",
      "Iteration 453, loss = 0.21159672\n",
      "Iteration 454, loss = 0.21111939\n",
      "Iteration 455, loss = 0.21059211\n",
      "Iteration 456, loss = 0.21010433\n",
      "Iteration 457, loss = 0.20967381\n",
      "Iteration 458, loss = 0.20921621\n",
      "Iteration 459, loss = 0.20875712\n",
      "Iteration 460, loss = 0.20825665\n",
      "Iteration 461, loss = 0.20779312\n",
      "Iteration 462, loss = 0.20732023\n",
      "Iteration 463, loss = 0.20695471\n",
      "Iteration 464, loss = 0.20639794\n",
      "Iteration 465, loss = 0.20596244\n",
      "Iteration 466, loss = 0.20554532\n",
      "Iteration 467, loss = 0.20516826\n",
      "Iteration 468, loss = 0.20468075\n",
      "Iteration 469, loss = 0.20419840\n",
      "Iteration 470, loss = 0.20378756\n",
      "Iteration 471, loss = 0.20331889\n",
      "Iteration 472, loss = 0.20294914\n",
      "Iteration 473, loss = 0.20265861\n",
      "Iteration 474, loss = 0.20231815\n",
      "Iteration 475, loss = 0.20196884\n",
      "Iteration 476, loss = 0.20161110\n",
      "Iteration 477, loss = 0.20139675\n",
      "Iteration 478, loss = 0.20115598\n",
      "Iteration 479, loss = 0.20085164\n",
      "Iteration 480, loss = 0.20047665\n",
      "Iteration 481, loss = 0.20009653\n",
      "Iteration 482, loss = 0.19973462\n",
      "Iteration 483, loss = 0.19931048\n",
      "Iteration 484, loss = 0.19887539\n",
      "Iteration 485, loss = 0.19841275\n",
      "Iteration 486, loss = 0.19794627\n",
      "Iteration 487, loss = 0.19759569\n",
      "Iteration 488, loss = 0.19723170\n",
      "Iteration 489, loss = 0.19689095\n",
      "Iteration 490, loss = 0.19671257\n",
      "Iteration 491, loss = 0.19654687\n",
      "Iteration 492, loss = 0.19606027\n",
      "Iteration 493, loss = 0.19569120\n",
      "Iteration 494, loss = 0.19535054\n",
      "Iteration 495, loss = 0.19493917\n",
      "Iteration 496, loss = 0.19466639\n",
      "Iteration 497, loss = 0.19428711\n",
      "Iteration 498, loss = 0.19396160\n",
      "Iteration 499, loss = 0.19364709\n",
      "Iteration 500, loss = 0.19338698\n",
      "Iteration 501, loss = 0.19288302\n",
      "Iteration 502, loss = 0.19244716\n",
      "Iteration 503, loss = 0.19200623\n",
      "Iteration 504, loss = 0.19164663\n",
      "Iteration 505, loss = 0.19130962\n",
      "Iteration 506, loss = 0.19096663\n",
      "Iteration 507, loss = 0.19065796\n",
      "Iteration 508, loss = 0.19030758\n",
      "Iteration 509, loss = 0.18994285\n",
      "Iteration 510, loss = 0.18960834\n",
      "Iteration 511, loss = 0.18922496\n",
      "Iteration 512, loss = 0.18887211\n",
      "Iteration 513, loss = 0.18847665\n",
      "Iteration 514, loss = 0.18816897\n",
      "Iteration 515, loss = 0.18779412\n",
      "Iteration 516, loss = 0.18743329\n",
      "Iteration 517, loss = 0.18717700\n",
      "Iteration 518, loss = 0.18695869\n",
      "Iteration 519, loss = 0.18689540\n",
      "Iteration 520, loss = 0.18664590\n",
      "Iteration 521, loss = 0.18639447\n",
      "Iteration 522, loss = 0.18611423\n",
      "Iteration 523, loss = 0.18584437\n",
      "Iteration 524, loss = 0.18558769\n",
      "Iteration 525, loss = 0.18534354\n",
      "Iteration 526, loss = 0.18508543\n",
      "Iteration 527, loss = 0.18482206\n",
      "Iteration 528, loss = 0.18454922\n",
      "Iteration 529, loss = 0.18439494\n",
      "Iteration 530, loss = 0.18422727\n",
      "Iteration 531, loss = 0.18414095\n",
      "Iteration 532, loss = 0.18399482\n",
      "Iteration 533, loss = 0.18396498\n",
      "Iteration 534, loss = 0.18381300\n",
      "Iteration 535, loss = 0.18363472\n",
      "Iteration 536, loss = 0.18338320\n",
      "Iteration 537, loss = 0.18304154\n",
      "Iteration 538, loss = 0.18263557\n",
      "Iteration 539, loss = 0.18224307\n",
      "Iteration 540, loss = 0.18187396\n",
      "Iteration 541, loss = 0.18156959\n",
      "Iteration 542, loss = 0.18121000\n",
      "Iteration 543, loss = 0.18098237\n",
      "Iteration 544, loss = 0.18073545\n",
      "Iteration 545, loss = 0.18055037\n",
      "Iteration 546, loss = 0.18032109\n",
      "Iteration 547, loss = 0.18011765\n",
      "Iteration 548, loss = 0.17992386\n",
      "Iteration 549, loss = 0.17961471\n",
      "Iteration 550, loss = 0.17906648\n",
      "Iteration 551, loss = 0.17884579\n",
      "Iteration 552, loss = 0.17851883\n",
      "Iteration 553, loss = 0.17834240\n",
      "Iteration 554, loss = 0.17823388\n",
      "Iteration 555, loss = 0.17814786\n",
      "Iteration 556, loss = 0.17806756\n",
      "Iteration 557, loss = 0.17796758\n",
      "Iteration 558, loss = 0.17785221\n",
      "Iteration 559, loss = 0.17780266\n",
      "Iteration 560, loss = 0.17757161\n",
      "Iteration 561, loss = 0.17720992\n",
      "Iteration 562, loss = 0.17675718\n",
      "Iteration 563, loss = 0.17632011\n",
      "Iteration 564, loss = 0.17590232\n",
      "Iteration 565, loss = 0.17569942\n",
      "Iteration 566, loss = 0.17523509\n",
      "Iteration 567, loss = 0.17482341\n",
      "Iteration 568, loss = 0.17444282\n",
      "Iteration 569, loss = 0.17409141\n",
      "Iteration 570, loss = 0.17384488\n",
      "Iteration 571, loss = 0.17363439\n",
      "Iteration 572, loss = 0.17328681\n",
      "Iteration 573, loss = 0.17300736\n",
      "Iteration 574, loss = 0.17274804\n",
      "Iteration 575, loss = 0.17254724\n",
      "Iteration 576, loss = 0.17236980\n",
      "Iteration 577, loss = 0.17219831\n",
      "Iteration 578, loss = 0.17194565\n",
      "Iteration 579, loss = 0.17176888\n",
      "Iteration 580, loss = 0.17153761\n",
      "Iteration 581, loss = 0.17151377\n",
      "Iteration 582, loss = 0.17155037\n",
      "Iteration 583, loss = 0.17127857\n",
      "Iteration 584, loss = 0.17090157\n",
      "Iteration 585, loss = 0.17053478\n",
      "Iteration 586, loss = 0.17021478\n",
      "Iteration 587, loss = 0.16987188\n",
      "Iteration 588, loss = 0.16961387\n",
      "Iteration 589, loss = 0.16942679\n",
      "Iteration 590, loss = 0.16917639\n",
      "Iteration 591, loss = 0.16893464\n",
      "Iteration 592, loss = 0.16870847\n",
      "Iteration 593, loss = 0.16851640\n",
      "Iteration 594, loss = 0.16828327\n",
      "Iteration 595, loss = 0.16809037\n",
      "Iteration 596, loss = 0.16765632\n",
      "Iteration 597, loss = 0.16728456\n",
      "Iteration 598, loss = 0.16696120\n",
      "Iteration 599, loss = 0.16673637\n",
      "Iteration 600, loss = 0.16652915\n",
      "Iteration 601, loss = 0.16619778\n",
      "Iteration 602, loss = 0.16591666\n",
      "Iteration 603, loss = 0.16562926\n",
      "Iteration 604, loss = 0.16533240\n",
      "Iteration 605, loss = 0.16501590\n",
      "Iteration 606, loss = 0.16503299\n",
      "Iteration 607, loss = 0.16478473\n",
      "Iteration 608, loss = 0.16459224\n",
      "Iteration 609, loss = 0.16438777\n",
      "Iteration 610, loss = 0.16419800\n",
      "Iteration 611, loss = 0.16408330\n",
      "Iteration 612, loss = 0.16415158\n",
      "Iteration 613, loss = 0.16425750\n",
      "Iteration 614, loss = 0.16419295\n",
      "Iteration 615, loss = 0.16401343\n",
      "Iteration 616, loss = 0.16378246\n",
      "Iteration 617, loss = 0.16339604\n",
      "Iteration 618, loss = 0.16310331\n",
      "Iteration 619, loss = 0.16256889\n",
      "Iteration 620, loss = 0.16222645\n",
      "Iteration 621, loss = 0.16192955\n",
      "Iteration 622, loss = 0.16180788\n",
      "Iteration 623, loss = 0.16149842\n",
      "Iteration 624, loss = 0.16137025\n",
      "Iteration 625, loss = 0.16110734\n",
      "Iteration 626, loss = 0.16088634\n",
      "Iteration 627, loss = 0.16067962\n",
      "Iteration 628, loss = 0.16051564\n",
      "Iteration 629, loss = 0.16053862\n",
      "Iteration 630, loss = 0.16041512\n",
      "Iteration 631, loss = 0.16031741\n",
      "Iteration 632, loss = 0.16016348\n",
      "Iteration 633, loss = 0.16001054\n",
      "Iteration 634, loss = 0.15977057\n",
      "Iteration 635, loss = 0.15940045\n",
      "Iteration 636, loss = 0.15911145\n",
      "Iteration 637, loss = 0.15888554\n",
      "Iteration 638, loss = 0.15840974\n",
      "Iteration 639, loss = 0.15798215\n",
      "Iteration 640, loss = 0.15785917\n",
      "Iteration 641, loss = 0.15788686\n",
      "Iteration 642, loss = 0.15782288\n",
      "Iteration 643, loss = 0.15783130\n",
      "Iteration 644, loss = 0.15772397\n",
      "Iteration 645, loss = 0.15775976\n",
      "Iteration 646, loss = 0.15786589\n",
      "Iteration 647, loss = 0.15791601\n",
      "Iteration 648, loss = 0.15779359\n",
      "Iteration 649, loss = 0.15752014\n",
      "Iteration 650, loss = 0.15727561\n",
      "Iteration 651, loss = 0.15716321\n",
      "Iteration 652, loss = 0.15696574\n",
      "Iteration 653, loss = 0.15683585\n",
      "Iteration 654, loss = 0.15635076\n",
      "Iteration 655, loss = 0.15590166\n",
      "Iteration 656, loss = 0.15538241\n",
      "Iteration 657, loss = 0.15503462\n",
      "Iteration 658, loss = 0.15502768\n",
      "Iteration 659, loss = 0.15503715\n",
      "Iteration 660, loss = 0.15511558\n",
      "Iteration 661, loss = 0.15505585\n",
      "Iteration 662, loss = 0.15496374\n",
      "Iteration 663, loss = 0.15472927\n",
      "Iteration 664, loss = 0.15458217\n",
      "Iteration 665, loss = 0.15438579\n",
      "Iteration 666, loss = 0.15411482\n",
      "Iteration 667, loss = 0.15382633\n",
      "Iteration 668, loss = 0.15347248\n",
      "Iteration 669, loss = 0.15329927\n",
      "Iteration 670, loss = 0.15314172\n",
      "Iteration 671, loss = 0.15286518\n",
      "Iteration 672, loss = 0.15252359\n",
      "Iteration 673, loss = 0.15220818\n",
      "Iteration 674, loss = 0.15197955\n",
      "Iteration 675, loss = 0.15193604\n",
      "Iteration 676, loss = 0.15201558\n",
      "Iteration 677, loss = 0.15224383\n",
      "Iteration 678, loss = 0.15217334\n",
      "Iteration 679, loss = 0.15182932\n",
      "Iteration 680, loss = 0.15138528\n",
      "Iteration 681, loss = 0.15098429\n",
      "Iteration 682, loss = 0.15067667\n",
      "Iteration 683, loss = 0.15041556\n",
      "Iteration 684, loss = 0.15017161\n",
      "Iteration 685, loss = 0.14993539\n",
      "Iteration 686, loss = 0.14974899\n",
      "Iteration 687, loss = 0.14961524\n",
      "Iteration 688, loss = 0.14941980\n",
      "Iteration 689, loss = 0.14921407\n",
      "Iteration 690, loss = 0.14895084\n",
      "Iteration 691, loss = 0.14873411\n",
      "Iteration 692, loss = 0.14865289\n",
      "Iteration 693, loss = 0.14864056\n",
      "Iteration 694, loss = 0.14865330\n",
      "Iteration 695, loss = 0.14865136\n",
      "Iteration 696, loss = 0.14851681\n",
      "Iteration 697, loss = 0.14843076\n",
      "Iteration 698, loss = 0.14816645\n",
      "Iteration 699, loss = 0.14785889\n",
      "Iteration 700, loss = 0.14776585\n",
      "Iteration 701, loss = 0.14759652\n",
      "Iteration 702, loss = 0.14749119\n",
      "Iteration 703, loss = 0.14736553\n",
      "Iteration 704, loss = 0.14735037\n",
      "Iteration 705, loss = 0.14710915\n",
      "Iteration 706, loss = 0.14701832\n",
      "Iteration 707, loss = 0.14681756\n",
      "Iteration 708, loss = 0.14660389\n",
      "Iteration 709, loss = 0.14638094\n",
      "Iteration 710, loss = 0.14618323\n",
      "Iteration 711, loss = 0.14597332\n",
      "Iteration 712, loss = 0.14574249\n",
      "Iteration 713, loss = 0.14561189\n",
      "Iteration 714, loss = 0.14539510\n",
      "Iteration 715, loss = 0.14523894\n",
      "Iteration 716, loss = 0.14504383\n",
      "Iteration 717, loss = 0.14494383\n",
      "Iteration 718, loss = 0.14466756\n",
      "Iteration 719, loss = 0.14442984\n",
      "Iteration 720, loss = 0.14424844\n",
      "Iteration 721, loss = 0.14407268\n",
      "Iteration 722, loss = 0.14392862\n",
      "Iteration 723, loss = 0.14376664\n",
      "Iteration 724, loss = 0.14362816\n",
      "Iteration 725, loss = 0.14343082\n",
      "Iteration 726, loss = 0.14319994\n",
      "Iteration 727, loss = 0.14299781\n",
      "Iteration 728, loss = 0.14283850\n",
      "Iteration 729, loss = 0.14278479\n",
      "Iteration 730, loss = 0.14251462\n",
      "Iteration 731, loss = 0.14231515\n",
      "Iteration 732, loss = 0.14203306\n",
      "Iteration 733, loss = 0.14194172\n",
      "Iteration 734, loss = 0.14176092\n",
      "Iteration 735, loss = 0.14162898\n",
      "Iteration 736, loss = 0.14142168\n",
      "Iteration 737, loss = 0.14113247\n",
      "Iteration 738, loss = 0.14089255\n",
      "Iteration 739, loss = 0.14070062\n",
      "Iteration 740, loss = 0.14061930\n",
      "Iteration 741, loss = 0.14056866\n",
      "Iteration 742, loss = 0.14123932\n",
      "Iteration 743, loss = 0.14119364\n",
      "Iteration 744, loss = 0.14109422\n",
      "Iteration 745, loss = 0.14107256\n",
      "Iteration 746, loss = 0.14103781\n",
      "Iteration 747, loss = 0.14100496\n",
      "Iteration 748, loss = 0.14090173\n",
      "Iteration 749, loss = 0.14080706\n",
      "Iteration 750, loss = 0.14071272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier5 = MLPClassifier(max_iter=2000,activation='relu', verbose=True, hidden_layer_sizes=(15,3),random_state=1,learning_rate_init=0.001)\n",
    "mlpClassifier5.fit(X_train, y_train)\n",
    "y_pred5 = mlpClassifier5.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b0dada7",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78d034e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.64516129032258"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "188f5e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde362e4070>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 5')\n",
    "plt.plot(range(len(mlpClassifier5.loss_curve_)),mlpClassifier5.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7dbd110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix5 = metrics.confusion_matrix(y_test,y_pred5)\n",
    "sns.heatmap(pd.DataFrame(confMatrix5),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654e01b8",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "705daebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74244995\n",
      "Iteration 2, loss = 0.73334325\n",
      "Iteration 3, loss = 0.72484460\n",
      "Iteration 4, loss = 0.71707606\n",
      "Iteration 5, loss = 0.70953435\n",
      "Iteration 6, loss = 0.70218292\n",
      "Iteration 7, loss = 0.69544779\n",
      "Iteration 8, loss = 0.68870399\n",
      "Iteration 9, loss = 0.68257413\n",
      "Iteration 10, loss = 0.67681234\n",
      "Iteration 11, loss = 0.67135059\n",
      "Iteration 12, loss = 0.66621138\n",
      "Iteration 13, loss = 0.66136879\n",
      "Iteration 14, loss = 0.65680814\n",
      "Iteration 15, loss = 0.65267354\n",
      "Iteration 16, loss = 0.64833030\n",
      "Iteration 17, loss = 0.64471035\n",
      "Iteration 18, loss = 0.64100206\n",
      "Iteration 19, loss = 0.63749502\n",
      "Iteration 20, loss = 0.63410725\n",
      "Iteration 21, loss = 0.63093299\n",
      "Iteration 22, loss = 0.62794854\n",
      "Iteration 23, loss = 0.62490905\n",
      "Iteration 24, loss = 0.62190452\n",
      "Iteration 25, loss = 0.61893084\n",
      "Iteration 26, loss = 0.61608321\n",
      "Iteration 27, loss = 0.61308852\n",
      "Iteration 28, loss = 0.61032659\n",
      "Iteration 29, loss = 0.60754311\n",
      "Iteration 30, loss = 0.60521176\n",
      "Iteration 31, loss = 0.60272921\n",
      "Iteration 32, loss = 0.60047461\n",
      "Iteration 33, loss = 0.59851525\n",
      "Iteration 34, loss = 0.59633502\n",
      "Iteration 35, loss = 0.59421667\n",
      "Iteration 36, loss = 0.59219900\n",
      "Iteration 37, loss = 0.59008938\n",
      "Iteration 38, loss = 0.58808880\n",
      "Iteration 39, loss = 0.58606137\n",
      "Iteration 40, loss = 0.58411147\n",
      "Iteration 41, loss = 0.58208197\n",
      "Iteration 42, loss = 0.58010767\n",
      "Iteration 43, loss = 0.57828172\n",
      "Iteration 44, loss = 0.57667694\n",
      "Iteration 45, loss = 0.57465109\n",
      "Iteration 46, loss = 0.57284325\n",
      "Iteration 47, loss = 0.57118061\n",
      "Iteration 48, loss = 0.56933602\n",
      "Iteration 49, loss = 0.56750225\n",
      "Iteration 50, loss = 0.56569182\n",
      "Iteration 51, loss = 0.56385219\n",
      "Iteration 52, loss = 0.56193023\n",
      "Iteration 53, loss = 0.56005253\n",
      "Iteration 54, loss = 0.55821770\n",
      "Iteration 55, loss = 0.55637036\n",
      "Iteration 56, loss = 0.55456892\n",
      "Iteration 57, loss = 0.55278711\n",
      "Iteration 58, loss = 0.55100724\n",
      "Iteration 59, loss = 0.54927678\n",
      "Iteration 60, loss = 0.54761190\n",
      "Iteration 61, loss = 0.54591220\n",
      "Iteration 62, loss = 0.54424344\n",
      "Iteration 63, loss = 0.54245786\n",
      "Iteration 64, loss = 0.54061621\n",
      "Iteration 65, loss = 0.53877735\n",
      "Iteration 66, loss = 0.53681889\n",
      "Iteration 67, loss = 0.53491526\n",
      "Iteration 68, loss = 0.53308235\n",
      "Iteration 69, loss = 0.53123740\n",
      "Iteration 70, loss = 0.52936915\n",
      "Iteration 71, loss = 0.52762522\n",
      "Iteration 72, loss = 0.52594445\n",
      "Iteration 73, loss = 0.52418554\n",
      "Iteration 74, loss = 0.52259352\n",
      "Iteration 75, loss = 0.52094549\n",
      "Iteration 76, loss = 0.51935053\n",
      "Iteration 77, loss = 0.51767418\n",
      "Iteration 78, loss = 0.51602555\n",
      "Iteration 79, loss = 0.51436681\n",
      "Iteration 80, loss = 0.51268180\n",
      "Iteration 81, loss = 0.51097274\n",
      "Iteration 82, loss = 0.50933488\n",
      "Iteration 83, loss = 0.50771018\n",
      "Iteration 84, loss = 0.50600825\n",
      "Iteration 85, loss = 0.50439246\n",
      "Iteration 86, loss = 0.50276937\n",
      "Iteration 87, loss = 0.50109369\n",
      "Iteration 88, loss = 0.49958789\n",
      "Iteration 89, loss = 0.49802390\n",
      "Iteration 90, loss = 0.49646936\n",
      "Iteration 91, loss = 0.49486031\n",
      "Iteration 92, loss = 0.49330065\n",
      "Iteration 93, loss = 0.49174906\n",
      "Iteration 94, loss = 0.49016883\n",
      "Iteration 95, loss = 0.48854673\n",
      "Iteration 96, loss = 0.48704463\n",
      "Iteration 97, loss = 0.48587463\n",
      "Iteration 98, loss = 0.48435962\n",
      "Iteration 99, loss = 0.48301817\n",
      "Iteration 100, loss = 0.48162272\n",
      "Iteration 101, loss = 0.48012777\n",
      "Iteration 102, loss = 0.47863050\n",
      "Iteration 103, loss = 0.47712668\n",
      "Iteration 104, loss = 0.47560534\n",
      "Iteration 105, loss = 0.47413126\n",
      "Iteration 106, loss = 0.47264607\n",
      "Iteration 107, loss = 0.47121046\n",
      "Iteration 108, loss = 0.46968833\n",
      "Iteration 109, loss = 0.46829710\n",
      "Iteration 110, loss = 0.46681730\n",
      "Iteration 111, loss = 0.46539249\n",
      "Iteration 112, loss = 0.46399997\n",
      "Iteration 113, loss = 0.46254469\n",
      "Iteration 114, loss = 0.46107996\n",
      "Iteration 115, loss = 0.45960248\n",
      "Iteration 116, loss = 0.45816724\n",
      "Iteration 117, loss = 0.45671807\n",
      "Iteration 118, loss = 0.45523900\n",
      "Iteration 119, loss = 0.45385565\n",
      "Iteration 120, loss = 0.45245840\n",
      "Iteration 121, loss = 0.45118359\n",
      "Iteration 122, loss = 0.44979878\n",
      "Iteration 123, loss = 0.44848965\n",
      "Iteration 124, loss = 0.44720108\n",
      "Iteration 125, loss = 0.44579318\n",
      "Iteration 126, loss = 0.44443107\n",
      "Iteration 127, loss = 0.44302552\n",
      "Iteration 128, loss = 0.44192150\n",
      "Iteration 129, loss = 0.44044263\n",
      "Iteration 130, loss = 0.43914795\n",
      "Iteration 131, loss = 0.43771049\n",
      "Iteration 132, loss = 0.43625901\n",
      "Iteration 133, loss = 0.43476157\n",
      "Iteration 134, loss = 0.43340074\n",
      "Iteration 135, loss = 0.43201685\n",
      "Iteration 136, loss = 0.43049265\n",
      "Iteration 137, loss = 0.42910457\n",
      "Iteration 138, loss = 0.42780844\n",
      "Iteration 139, loss = 0.42644179\n",
      "Iteration 140, loss = 0.42522538\n",
      "Iteration 141, loss = 0.42394468\n",
      "Iteration 142, loss = 0.42270023\n",
      "Iteration 143, loss = 0.42129186\n",
      "Iteration 144, loss = 0.41996318\n",
      "Iteration 145, loss = 0.41869620\n",
      "Iteration 146, loss = 0.41733397\n",
      "Iteration 147, loss = 0.41606517\n",
      "Iteration 148, loss = 0.41477999\n",
      "Iteration 149, loss = 0.41348908\n",
      "Iteration 150, loss = 0.41233283\n",
      "Iteration 151, loss = 0.41113585\n",
      "Iteration 152, loss = 0.40992688\n",
      "Iteration 153, loss = 0.40870082\n",
      "Iteration 154, loss = 0.40745918\n",
      "Iteration 155, loss = 0.40614890\n",
      "Iteration 156, loss = 0.40489224\n",
      "Iteration 157, loss = 0.40369212\n",
      "Iteration 158, loss = 0.40239047\n",
      "Iteration 159, loss = 0.40114432\n",
      "Iteration 160, loss = 0.39989434\n",
      "Iteration 161, loss = 0.39863940\n",
      "Iteration 162, loss = 0.39746855\n",
      "Iteration 163, loss = 0.39627570\n",
      "Iteration 164, loss = 0.39504698\n",
      "Iteration 165, loss = 0.39392739\n",
      "Iteration 166, loss = 0.39284665\n",
      "Iteration 167, loss = 0.39171721\n",
      "Iteration 168, loss = 0.39052866\n",
      "Iteration 169, loss = 0.38937424\n",
      "Iteration 170, loss = 0.38814975\n",
      "Iteration 171, loss = 0.38705988\n",
      "Iteration 172, loss = 0.38584272\n",
      "Iteration 173, loss = 0.38474014\n",
      "Iteration 174, loss = 0.38357159\n",
      "Iteration 175, loss = 0.38256018\n",
      "Iteration 176, loss = 0.38157369\n",
      "Iteration 177, loss = 0.38054932\n",
      "Iteration 178, loss = 0.37946168\n",
      "Iteration 179, loss = 0.37838786\n",
      "Iteration 180, loss = 0.37729002\n",
      "Iteration 181, loss = 0.37616718\n",
      "Iteration 182, loss = 0.37506979\n",
      "Iteration 183, loss = 0.37404570\n",
      "Iteration 184, loss = 0.37296382\n",
      "Iteration 185, loss = 0.37191487\n",
      "Iteration 186, loss = 0.37103455\n",
      "Iteration 187, loss = 0.36995638\n",
      "Iteration 188, loss = 0.36889653\n",
      "Iteration 189, loss = 0.36780988\n",
      "Iteration 190, loss = 0.36670655\n",
      "Iteration 191, loss = 0.36567674\n",
      "Iteration 192, loss = 0.36463746\n",
      "Iteration 193, loss = 0.36361727\n",
      "Iteration 194, loss = 0.36252507\n",
      "Iteration 195, loss = 0.36143759\n",
      "Iteration 196, loss = 0.36037365\n",
      "Iteration 197, loss = 0.35947407\n",
      "Iteration 198, loss = 0.35846773\n",
      "Iteration 199, loss = 0.35743620\n",
      "Iteration 200, loss = 0.35641237\n",
      "Iteration 201, loss = 0.35533064\n",
      "Iteration 202, loss = 0.35432969\n",
      "Iteration 203, loss = 0.35321308\n",
      "Iteration 204, loss = 0.35212724\n",
      "Iteration 205, loss = 0.35104163\n",
      "Iteration 206, loss = 0.34989260\n",
      "Iteration 207, loss = 0.34888898\n",
      "Iteration 208, loss = 0.34780918\n",
      "Iteration 209, loss = 0.34678701\n",
      "Iteration 210, loss = 0.34587892\n",
      "Iteration 211, loss = 0.34472610\n",
      "Iteration 212, loss = 0.34379945\n",
      "Iteration 213, loss = 0.34273143\n",
      "Iteration 214, loss = 0.34181883\n",
      "Iteration 215, loss = 0.34079242\n",
      "Iteration 216, loss = 0.33984632\n",
      "Iteration 217, loss = 0.33889037\n",
      "Iteration 218, loss = 0.33791769\n",
      "Iteration 219, loss = 0.33697285\n",
      "Iteration 220, loss = 0.33602282\n",
      "Iteration 221, loss = 0.33517524\n",
      "Iteration 222, loss = 0.33423897\n",
      "Iteration 223, loss = 0.33338828\n",
      "Iteration 224, loss = 0.33253097\n",
      "Iteration 225, loss = 0.33160879\n",
      "Iteration 226, loss = 0.33071663\n",
      "Iteration 227, loss = 0.32974457\n",
      "Iteration 228, loss = 0.32881090\n",
      "Iteration 229, loss = 0.32799495\n",
      "Iteration 230, loss = 0.32718830\n",
      "Iteration 231, loss = 0.32654093\n",
      "Iteration 232, loss = 0.32592363\n",
      "Iteration 233, loss = 0.32546557\n",
      "Iteration 234, loss = 0.32472589\n",
      "Iteration 235, loss = 0.32399646\n",
      "Iteration 236, loss = 0.32322543\n",
      "Iteration 237, loss = 0.32231945\n",
      "Iteration 238, loss = 0.32141986\n",
      "Iteration 239, loss = 0.32044995\n",
      "Iteration 240, loss = 0.31955101\n",
      "Iteration 241, loss = 0.31856847\n",
      "Iteration 242, loss = 0.31775640\n",
      "Iteration 243, loss = 0.31689984\n",
      "Iteration 244, loss = 0.31612573\n",
      "Iteration 245, loss = 0.31538992\n",
      "Iteration 246, loss = 0.31474651\n",
      "Iteration 247, loss = 0.31394904\n",
      "Iteration 248, loss = 0.31314772\n",
      "Iteration 249, loss = 0.31241660\n",
      "Iteration 250, loss = 0.31161573\n",
      "Iteration 251, loss = 0.31084712\n",
      "Iteration 252, loss = 0.31006991\n",
      "Iteration 253, loss = 0.30921975\n",
      "Iteration 254, loss = 0.30849473\n",
      "Iteration 255, loss = 0.30772407\n",
      "Iteration 256, loss = 0.30699854\n",
      "Iteration 257, loss = 0.30626286\n",
      "Iteration 258, loss = 0.30544774\n",
      "Iteration 259, loss = 0.30462355\n",
      "Iteration 260, loss = 0.30388243\n",
      "Iteration 261, loss = 0.30303621\n",
      "Iteration 262, loss = 0.30238320\n",
      "Iteration 263, loss = 0.30171946\n",
      "Iteration 264, loss = 0.30109719\n",
      "Iteration 265, loss = 0.30055957\n",
      "Iteration 266, loss = 0.29999647\n",
      "Iteration 267, loss = 0.29942112\n",
      "Iteration 268, loss = 0.29892943\n",
      "Iteration 269, loss = 0.29834389\n",
      "Iteration 270, loss = 0.29755070\n",
      "Iteration 271, loss = 0.29672648\n",
      "Iteration 272, loss = 0.29597031\n",
      "Iteration 273, loss = 0.29536112\n",
      "Iteration 274, loss = 0.29465086\n",
      "Iteration 275, loss = 0.29412436\n",
      "Iteration 276, loss = 0.29349913\n",
      "Iteration 277, loss = 0.29277033\n",
      "Iteration 278, loss = 0.29210404\n",
      "Iteration 279, loss = 0.29137211\n",
      "Iteration 280, loss = 0.29069496\n",
      "Iteration 281, loss = 0.28998924\n",
      "Iteration 282, loss = 0.28927309\n",
      "Iteration 283, loss = 0.28854635\n",
      "Iteration 284, loss = 0.28779103\n",
      "Iteration 285, loss = 0.28705037\n",
      "Iteration 286, loss = 0.28642326\n",
      "Iteration 287, loss = 0.28566828\n",
      "Iteration 288, loss = 0.28511134\n",
      "Iteration 289, loss = 0.28449731\n",
      "Iteration 290, loss = 0.28396333\n",
      "Iteration 291, loss = 0.28342291\n",
      "Iteration 292, loss = 0.28280873\n",
      "Iteration 293, loss = 0.28225465\n",
      "Iteration 294, loss = 0.28169344\n",
      "Iteration 295, loss = 0.28119469\n",
      "Iteration 296, loss = 0.28072217\n",
      "Iteration 297, loss = 0.28027455\n",
      "Iteration 298, loss = 0.27977878\n",
      "Iteration 299, loss = 0.27910882\n",
      "Iteration 300, loss = 0.27855845\n",
      "Iteration 301, loss = 0.27809754\n",
      "Iteration 302, loss = 0.27741279\n",
      "Iteration 303, loss = 0.27675917\n",
      "Iteration 304, loss = 0.27642281\n",
      "Iteration 305, loss = 0.27585968\n",
      "Iteration 306, loss = 0.27526861\n",
      "Iteration 307, loss = 0.27481052\n",
      "Iteration 308, loss = 0.27427641\n",
      "Iteration 309, loss = 0.27362845\n",
      "Iteration 310, loss = 0.27282873\n",
      "Iteration 311, loss = 0.27201046\n",
      "Iteration 312, loss = 0.27126338\n",
      "Iteration 313, loss = 0.27043423\n",
      "Iteration 314, loss = 0.26984072\n",
      "Iteration 315, loss = 0.26920686\n",
      "Iteration 316, loss = 0.26880195\n",
      "Iteration 317, loss = 0.26836237\n",
      "Iteration 318, loss = 0.26781025\n",
      "Iteration 319, loss = 0.26723920\n",
      "Iteration 320, loss = 0.26664949\n",
      "Iteration 321, loss = 0.26602994\n",
      "Iteration 322, loss = 0.26541533\n",
      "Iteration 323, loss = 0.26486685\n",
      "Iteration 324, loss = 0.26429829\n",
      "Iteration 325, loss = 0.26369678\n",
      "Iteration 326, loss = 0.26311129\n",
      "Iteration 327, loss = 0.26254140\n",
      "Iteration 328, loss = 0.26195941\n",
      "Iteration 329, loss = 0.26134297\n",
      "Iteration 330, loss = 0.26076802\n",
      "Iteration 331, loss = 0.26018531\n",
      "Iteration 332, loss = 0.25961344\n",
      "Iteration 333, loss = 0.25901704\n",
      "Iteration 334, loss = 0.25852784\n",
      "Iteration 335, loss = 0.25797133\n",
      "Iteration 336, loss = 0.25749560\n",
      "Iteration 337, loss = 0.25700328\n",
      "Iteration 338, loss = 0.25655764\n",
      "Iteration 339, loss = 0.25609068\n",
      "Iteration 340, loss = 0.25565721\n",
      "Iteration 341, loss = 0.25512931\n",
      "Iteration 342, loss = 0.25465270\n",
      "Iteration 343, loss = 0.25407212\n",
      "Iteration 344, loss = 0.25346611\n",
      "Iteration 345, loss = 0.25278586\n",
      "Iteration 346, loss = 0.25225519\n",
      "Iteration 347, loss = 0.25200420\n",
      "Iteration 348, loss = 0.25147770\n",
      "Iteration 349, loss = 0.25097095\n",
      "Iteration 350, loss = 0.25040090\n",
      "Iteration 351, loss = 0.24987609\n",
      "Iteration 352, loss = 0.24932860\n",
      "Iteration 353, loss = 0.24871663\n",
      "Iteration 354, loss = 0.24836806\n",
      "Iteration 355, loss = 0.24821773\n",
      "Iteration 356, loss = 0.24811645\n",
      "Iteration 357, loss = 0.24775467\n",
      "Iteration 358, loss = 0.24747369\n",
      "Iteration 359, loss = 0.24711567\n",
      "Iteration 360, loss = 0.24677871\n",
      "Iteration 361, loss = 0.24642190\n",
      "Iteration 362, loss = 0.24602032\n",
      "Iteration 363, loss = 0.24535335\n",
      "Iteration 364, loss = 0.24463399\n",
      "Iteration 365, loss = 0.24402856\n",
      "Iteration 366, loss = 0.24344301\n",
      "Iteration 367, loss = 0.24302699\n",
      "Iteration 368, loss = 0.24233198\n",
      "Iteration 369, loss = 0.24174046\n",
      "Iteration 370, loss = 0.24126095\n",
      "Iteration 371, loss = 0.24070979\n",
      "Iteration 372, loss = 0.24020828\n",
      "Iteration 373, loss = 0.23973974\n",
      "Iteration 374, loss = 0.23968409\n",
      "Iteration 375, loss = 0.23968039\n",
      "Iteration 376, loss = 0.23970429\n",
      "Iteration 377, loss = 0.23914126\n",
      "Iteration 378, loss = 0.23834245\n",
      "Iteration 379, loss = 0.23751844\n",
      "Iteration 380, loss = 0.23667313\n",
      "Iteration 381, loss = 0.23593531\n",
      "Iteration 382, loss = 0.23534260\n",
      "Iteration 383, loss = 0.23475674\n",
      "Iteration 384, loss = 0.23426958\n",
      "Iteration 385, loss = 0.23380080\n",
      "Iteration 386, loss = 0.23348606\n",
      "Iteration 387, loss = 0.23279300\n",
      "Iteration 388, loss = 0.23232882\n",
      "Iteration 389, loss = 0.23217957\n",
      "Iteration 390, loss = 0.23221766\n",
      "Iteration 391, loss = 0.23201316\n",
      "Iteration 392, loss = 0.23162482\n",
      "Iteration 393, loss = 0.23115068\n",
      "Iteration 394, loss = 0.23073502\n",
      "Iteration 395, loss = 0.23019418\n",
      "Iteration 396, loss = 0.22974130\n",
      "Iteration 397, loss = 0.22929388\n",
      "Iteration 398, loss = 0.22881714\n",
      "Iteration 399, loss = 0.22844953\n",
      "Iteration 400, loss = 0.22793649\n",
      "Iteration 401, loss = 0.22743559\n",
      "Iteration 402, loss = 0.22703662\n",
      "Iteration 403, loss = 0.22670322\n",
      "Iteration 404, loss = 0.22663682\n",
      "Iteration 405, loss = 0.22637327\n",
      "Iteration 406, loss = 0.22601352\n",
      "Iteration 407, loss = 0.22558132\n",
      "Iteration 408, loss = 0.22502964\n",
      "Iteration 409, loss = 0.22441448\n",
      "Iteration 410, loss = 0.22375433\n",
      "Iteration 411, loss = 0.22303209\n",
      "Iteration 412, loss = 0.22231262\n",
      "Iteration 413, loss = 0.22165834\n",
      "Iteration 414, loss = 0.22106734\n",
      "Iteration 415, loss = 0.22065229\n",
      "Iteration 416, loss = 0.22011777\n",
      "Iteration 417, loss = 0.21957095\n",
      "Iteration 418, loss = 0.21906184\n",
      "Iteration 419, loss = 0.21859128\n",
      "Iteration 420, loss = 0.21813844\n",
      "Iteration 421, loss = 0.21771737\n",
      "Iteration 422, loss = 0.21719213\n",
      "Iteration 423, loss = 0.21672186\n",
      "Iteration 424, loss = 0.21620761\n",
      "Iteration 425, loss = 0.21576187\n",
      "Iteration 426, loss = 0.21524921\n",
      "Iteration 427, loss = 0.21488306\n",
      "Iteration 428, loss = 0.21446961\n",
      "Iteration 429, loss = 0.21403435\n",
      "Iteration 430, loss = 0.21364298\n",
      "Iteration 431, loss = 0.21317451\n",
      "Iteration 432, loss = 0.21274340\n",
      "Iteration 433, loss = 0.21237918\n",
      "Iteration 434, loss = 0.21202755\n",
      "Iteration 435, loss = 0.21166622\n",
      "Iteration 436, loss = 0.21130386\n",
      "Iteration 437, loss = 0.21094380\n",
      "Iteration 438, loss = 0.21069040\n",
      "Iteration 439, loss = 0.21034736\n",
      "Iteration 440, loss = 0.20999197\n",
      "Iteration 441, loss = 0.20984433\n",
      "Iteration 442, loss = 0.20957458\n",
      "Iteration 443, loss = 0.20901962\n",
      "Iteration 444, loss = 0.20831883\n",
      "Iteration 445, loss = 0.20773682\n",
      "Iteration 446, loss = 0.20777404\n",
      "Iteration 447, loss = 0.20723917\n",
      "Iteration 448, loss = 0.20720868\n",
      "Iteration 449, loss = 0.20691561\n",
      "Iteration 450, loss = 0.20672794\n",
      "Iteration 451, loss = 0.20634261\n",
      "Iteration 452, loss = 0.20591780\n",
      "Iteration 453, loss = 0.20547323\n",
      "Iteration 454, loss = 0.20518272\n",
      "Iteration 455, loss = 0.20474935\n",
      "Iteration 456, loss = 0.20429781\n",
      "Iteration 457, loss = 0.20371210\n",
      "Iteration 458, loss = 0.20339799\n",
      "Iteration 459, loss = 0.20306009\n",
      "Iteration 460, loss = 0.20268906\n",
      "Iteration 461, loss = 0.20252362\n",
      "Iteration 462, loss = 0.20240318\n",
      "Iteration 463, loss = 0.20200503\n",
      "Iteration 464, loss = 0.20146036\n",
      "Iteration 465, loss = 0.20105896\n",
      "Iteration 466, loss = 0.20055543\n",
      "Iteration 467, loss = 0.20028413\n",
      "Iteration 468, loss = 0.19983454\n",
      "Iteration 469, loss = 0.19938879\n",
      "Iteration 470, loss = 0.19890000\n",
      "Iteration 471, loss = 0.19868179\n",
      "Iteration 472, loss = 0.19837665\n",
      "Iteration 473, loss = 0.19807042\n",
      "Iteration 474, loss = 0.19773879\n",
      "Iteration 475, loss = 0.19729140\n",
      "Iteration 476, loss = 0.19673029\n",
      "Iteration 477, loss = 0.19617697\n",
      "Iteration 478, loss = 0.19571254\n",
      "Iteration 479, loss = 0.19549172\n",
      "Iteration 480, loss = 0.19525393\n",
      "Iteration 481, loss = 0.19495780\n",
      "Iteration 482, loss = 0.19468334\n",
      "Iteration 483, loss = 0.19433444\n",
      "Iteration 484, loss = 0.19385981\n",
      "Iteration 485, loss = 0.19358022\n",
      "Iteration 486, loss = 0.19342283\n",
      "Iteration 487, loss = 0.19329813\n",
      "Iteration 488, loss = 0.19327657\n",
      "Iteration 489, loss = 0.19329118\n",
      "Iteration 490, loss = 0.19321243\n",
      "Iteration 491, loss = 0.19290847\n",
      "Iteration 492, loss = 0.19255711\n",
      "Iteration 493, loss = 0.19225580\n",
      "Iteration 494, loss = 0.19186851\n",
      "Iteration 495, loss = 0.19129052\n",
      "Iteration 496, loss = 0.19097789\n",
      "Iteration 497, loss = 0.19059042\n",
      "Iteration 498, loss = 0.19007995\n",
      "Iteration 499, loss = 0.18978153\n",
      "Iteration 500, loss = 0.18949416\n",
      "Iteration 501, loss = 0.18915025\n",
      "Iteration 502, loss = 0.18871105\n",
      "Iteration 503, loss = 0.18838455\n",
      "Iteration 504, loss = 0.18800960\n",
      "Iteration 505, loss = 0.18770729\n",
      "Iteration 506, loss = 0.18784876\n",
      "Iteration 507, loss = 0.18781317\n",
      "Iteration 508, loss = 0.18787999\n",
      "Iteration 509, loss = 0.18774003\n",
      "Iteration 510, loss = 0.18768159\n",
      "Iteration 511, loss = 0.18780788\n",
      "Iteration 512, loss = 0.18743298\n",
      "Iteration 513, loss = 0.18669439\n",
      "Iteration 514, loss = 0.18576335\n",
      "Iteration 515, loss = 0.18508091\n",
      "Iteration 516, loss = 0.18428176\n",
      "Iteration 517, loss = 0.18367233\n",
      "Iteration 518, loss = 0.18305459\n",
      "Iteration 519, loss = 0.18262543\n",
      "Iteration 520, loss = 0.18219608\n",
      "Iteration 521, loss = 0.18182939\n",
      "Iteration 522, loss = 0.18189401\n",
      "Iteration 523, loss = 0.18204562\n",
      "Iteration 524, loss = 0.18185533\n",
      "Iteration 525, loss = 0.18195436\n",
      "Iteration 526, loss = 0.18180618\n",
      "Iteration 527, loss = 0.18141186\n",
      "Iteration 528, loss = 0.18097376\n",
      "Iteration 529, loss = 0.18061276\n",
      "Iteration 530, loss = 0.18018798\n",
      "Iteration 531, loss = 0.17998701\n",
      "Iteration 532, loss = 0.18004276\n",
      "Iteration 533, loss = 0.18004534\n",
      "Iteration 534, loss = 0.17981766\n",
      "Iteration 535, loss = 0.17945090\n",
      "Iteration 536, loss = 0.17905157\n",
      "Iteration 537, loss = 0.17790592\n",
      "Iteration 538, loss = 0.17695322\n",
      "Iteration 539, loss = 0.17591233\n",
      "Iteration 540, loss = 0.17533291\n",
      "Iteration 541, loss = 0.17510078\n",
      "Iteration 542, loss = 0.17481398\n",
      "Iteration 543, loss = 0.17459555\n",
      "Iteration 544, loss = 0.17442242\n",
      "Iteration 545, loss = 0.17425985\n",
      "Iteration 546, loss = 0.17396602\n",
      "Iteration 547, loss = 0.17353681\n",
      "Iteration 548, loss = 0.17336704\n",
      "Iteration 549, loss = 0.17309449\n",
      "Iteration 550, loss = 0.17282930\n",
      "Iteration 551, loss = 0.17255200\n",
      "Iteration 552, loss = 0.17227182\n",
      "Iteration 553, loss = 0.17191411\n",
      "Iteration 554, loss = 0.17159853\n",
      "Iteration 555, loss = 0.17115795\n",
      "Iteration 556, loss = 0.17116331\n",
      "Iteration 557, loss = 0.17081614\n",
      "Iteration 558, loss = 0.17037667\n",
      "Iteration 559, loss = 0.16993612\n",
      "Iteration 560, loss = 0.16963905\n",
      "Iteration 561, loss = 0.16936985\n",
      "Iteration 562, loss = 0.16908527\n",
      "Iteration 563, loss = 0.16884645\n",
      "Iteration 564, loss = 0.16859216\n",
      "Iteration 565, loss = 0.16827401\n",
      "Iteration 566, loss = 0.16793920\n",
      "Iteration 567, loss = 0.16760666\n",
      "Iteration 568, loss = 0.16730448\n",
      "Iteration 569, loss = 0.16685401\n",
      "Iteration 570, loss = 0.16654299\n",
      "Iteration 571, loss = 0.16618332\n",
      "Iteration 572, loss = 0.16577283\n",
      "Iteration 573, loss = 0.16552548\n",
      "Iteration 574, loss = 0.16521156\n",
      "Iteration 575, loss = 0.16498508\n",
      "Iteration 576, loss = 0.16476663\n",
      "Iteration 577, loss = 0.16445356\n",
      "Iteration 578, loss = 0.16421115\n",
      "Iteration 579, loss = 0.16393081\n",
      "Iteration 580, loss = 0.16377532\n",
      "Iteration 581, loss = 0.16353736\n",
      "Iteration 582, loss = 0.16334019\n",
      "Iteration 583, loss = 0.16334088\n",
      "Iteration 584, loss = 0.16376715\n",
      "Iteration 585, loss = 0.16390028\n",
      "Iteration 586, loss = 0.16434435\n",
      "Iteration 587, loss = 0.16476158\n",
      "Iteration 588, loss = 0.16482984\n",
      "Iteration 589, loss = 0.16456803\n",
      "Iteration 590, loss = 0.16395176\n",
      "Iteration 591, loss = 0.16313813\n",
      "Iteration 592, loss = 0.16241630\n",
      "Iteration 593, loss = 0.16169090\n",
      "Iteration 594, loss = 0.16107152\n",
      "Iteration 595, loss = 0.16067046\n",
      "Iteration 596, loss = 0.16021770\n",
      "Iteration 597, loss = 0.15983395\n",
      "Iteration 598, loss = 0.15942377\n",
      "Iteration 599, loss = 0.15904951\n",
      "Iteration 600, loss = 0.15874322\n",
      "Iteration 601, loss = 0.15840438\n",
      "Iteration 602, loss = 0.15811268\n",
      "Iteration 603, loss = 0.15779290\n",
      "Iteration 604, loss = 0.15748464\n",
      "Iteration 605, loss = 0.15715755\n",
      "Iteration 606, loss = 0.15683231\n",
      "Iteration 607, loss = 0.15654964\n",
      "Iteration 608, loss = 0.15623529\n",
      "Iteration 609, loss = 0.15603004\n",
      "Iteration 610, loss = 0.15580837\n",
      "Iteration 611, loss = 0.15563624\n",
      "Iteration 612, loss = 0.15543651\n",
      "Iteration 613, loss = 0.15542176\n",
      "Iteration 614, loss = 0.15538402\n",
      "Iteration 615, loss = 0.15544588\n",
      "Iteration 616, loss = 0.15572470\n",
      "Iteration 617, loss = 0.15603297\n",
      "Iteration 618, loss = 0.15614794\n",
      "Iteration 619, loss = 0.15616372\n",
      "Iteration 620, loss = 0.15600634\n",
      "Iteration 621, loss = 0.15553148\n",
      "Iteration 622, loss = 0.15509097\n",
      "Iteration 623, loss = 0.15470847\n",
      "Iteration 624, loss = 0.15415246\n",
      "Iteration 625, loss = 0.15359506\n",
      "Iteration 626, loss = 0.15283981\n",
      "Iteration 627, loss = 0.15230691\n",
      "Iteration 628, loss = 0.15172195\n",
      "Iteration 629, loss = 0.15121685\n",
      "Iteration 630, loss = 0.15087826\n",
      "Iteration 631, loss = 0.15041578\n",
      "Iteration 632, loss = 0.15014601\n",
      "Iteration 633, loss = 0.15001370\n",
      "Iteration 634, loss = 0.14964879\n",
      "Iteration 635, loss = 0.14931659\n",
      "Iteration 636, loss = 0.14896360\n",
      "Iteration 637, loss = 0.14875226\n",
      "Iteration 638, loss = 0.14843924\n",
      "Iteration 639, loss = 0.14810517\n",
      "Iteration 640, loss = 0.14780845\n",
      "Iteration 641, loss = 0.14753007\n",
      "Iteration 642, loss = 0.14730075\n",
      "Iteration 643, loss = 0.14728735\n",
      "Iteration 644, loss = 0.14742147\n",
      "Iteration 645, loss = 0.14786998\n",
      "Iteration 646, loss = 0.14835296\n",
      "Iteration 647, loss = 0.14839005\n",
      "Iteration 648, loss = 0.14812822\n",
      "Iteration 649, loss = 0.14771975\n",
      "Iteration 650, loss = 0.14695603\n",
      "Iteration 651, loss = 0.14649863\n",
      "Iteration 652, loss = 0.14607487\n",
      "Iteration 653, loss = 0.14555986\n",
      "Iteration 654, loss = 0.14502278\n",
      "Iteration 655, loss = 0.14461025\n",
      "Iteration 656, loss = 0.14414951\n",
      "Iteration 657, loss = 0.14379696\n",
      "Iteration 658, loss = 0.14358471\n",
      "Iteration 659, loss = 0.14330994\n",
      "Iteration 660, loss = 0.14306287\n",
      "Iteration 661, loss = 0.14277200\n",
      "Iteration 662, loss = 0.14258454\n",
      "Iteration 663, loss = 0.14236056\n",
      "Iteration 664, loss = 0.14218732\n",
      "Iteration 665, loss = 0.14204813\n",
      "Iteration 666, loss = 0.14166296\n",
      "Iteration 667, loss = 0.14129982\n",
      "Iteration 668, loss = 0.14109110\n",
      "Iteration 669, loss = 0.14063543\n",
      "Iteration 670, loss = 0.14048073\n",
      "Iteration 671, loss = 0.14033654\n",
      "Iteration 672, loss = 0.14006360\n",
      "Iteration 673, loss = 0.13975977\n",
      "Iteration 674, loss = 0.13927909\n",
      "Iteration 675, loss = 0.13890869\n",
      "Iteration 676, loss = 0.13863363\n",
      "Iteration 677, loss = 0.13838078\n",
      "Iteration 678, loss = 0.13811512\n",
      "Iteration 679, loss = 0.13793850\n",
      "Iteration 680, loss = 0.13767210\n",
      "Iteration 681, loss = 0.13748197\n",
      "Iteration 682, loss = 0.13731244\n",
      "Iteration 683, loss = 0.13718386\n",
      "Iteration 684, loss = 0.13702607\n",
      "Iteration 685, loss = 0.13704483\n",
      "Iteration 686, loss = 0.13661170\n",
      "Iteration 687, loss = 0.13619518\n",
      "Iteration 688, loss = 0.13625086\n",
      "Iteration 689, loss = 0.13603412\n",
      "Iteration 690, loss = 0.13577124\n",
      "Iteration 691, loss = 0.13569656\n",
      "Iteration 692, loss = 0.13549848\n",
      "Iteration 693, loss = 0.13533041\n",
      "Iteration 694, loss = 0.13516653\n",
      "Iteration 695, loss = 0.13507990\n",
      "Iteration 696, loss = 0.13492945\n",
      "Iteration 697, loss = 0.13492397\n",
      "Iteration 698, loss = 0.13494205\n",
      "Iteration 699, loss = 0.13487627\n",
      "Iteration 700, loss = 0.13458304\n",
      "Iteration 701, loss = 0.13421462\n",
      "Iteration 702, loss = 0.13381438\n",
      "Iteration 703, loss = 0.13340550\n",
      "Iteration 704, loss = 0.13332540\n",
      "Iteration 705, loss = 0.13315850\n",
      "Iteration 706, loss = 0.13289599\n",
      "Iteration 707, loss = 0.13264553\n",
      "Iteration 708, loss = 0.13237965\n",
      "Iteration 709, loss = 0.13188515\n",
      "Iteration 710, loss = 0.13132473\n",
      "Iteration 711, loss = 0.13099619\n",
      "Iteration 712, loss = 0.13064836\n",
      "Iteration 713, loss = 0.13044863\n",
      "Iteration 714, loss = 0.13027474\n",
      "Iteration 715, loss = 0.12991868\n",
      "Iteration 716, loss = 0.12955376\n",
      "Iteration 717, loss = 0.12922321\n",
      "Iteration 718, loss = 0.12880211\n",
      "Iteration 719, loss = 0.12857242\n",
      "Iteration 720, loss = 0.12841784\n",
      "Iteration 721, loss = 0.12837546\n",
      "Iteration 722, loss = 0.12826027\n",
      "Iteration 723, loss = 0.12822221\n",
      "Iteration 724, loss = 0.12816833\n",
      "Iteration 725, loss = 0.12805055\n",
      "Iteration 726, loss = 0.12748353\n",
      "Iteration 727, loss = 0.12697444\n",
      "Iteration 728, loss = 0.12676043\n",
      "Iteration 729, loss = 0.12663956\n",
      "Iteration 730, loss = 0.12662410\n",
      "Iteration 731, loss = 0.12649114\n",
      "Iteration 732, loss = 0.12633120\n",
      "Iteration 733, loss = 0.12625102\n",
      "Iteration 734, loss = 0.12640144\n",
      "Iteration 735, loss = 0.12639210\n",
      "Iteration 736, loss = 0.12606722\n",
      "Iteration 737, loss = 0.12569402\n",
      "Iteration 738, loss = 0.12539962\n",
      "Iteration 739, loss = 0.12514479\n",
      "Iteration 740, loss = 0.12497962\n",
      "Iteration 741, loss = 0.12479936\n",
      "Iteration 742, loss = 0.12457113\n",
      "Iteration 743, loss = 0.12442991\n",
      "Iteration 744, loss = 0.12429522\n",
      "Iteration 745, loss = 0.12422421\n",
      "Iteration 746, loss = 0.12386028\n",
      "Iteration 747, loss = 0.12350047\n",
      "Iteration 748, loss = 0.12314750\n",
      "Iteration 749, loss = 0.12285349\n",
      "Iteration 750, loss = 0.12262904\n",
      "Iteration 751, loss = 0.12250569\n",
      "Iteration 752, loss = 0.12228920\n",
      "Iteration 753, loss = 0.12198582\n",
      "Iteration 754, loss = 0.12172812\n",
      "Iteration 755, loss = 0.12150964\n",
      "Iteration 756, loss = 0.12136590\n",
      "Iteration 757, loss = 0.12106079\n",
      "Iteration 758, loss = 0.12075322\n",
      "Iteration 759, loss = 0.12061979\n",
      "Iteration 760, loss = 0.12051932\n",
      "Iteration 761, loss = 0.12044631\n",
      "Iteration 762, loss = 0.12024171\n",
      "Iteration 763, loss = 0.12009770\n",
      "Iteration 764, loss = 0.11997629\n",
      "Iteration 765, loss = 0.11957002\n",
      "Iteration 766, loss = 0.11921436\n",
      "Iteration 767, loss = 0.11895073\n",
      "Iteration 768, loss = 0.11883516\n",
      "Iteration 769, loss = 0.11870652\n",
      "Iteration 770, loss = 0.11857140\n",
      "Iteration 771, loss = 0.11845162\n",
      "Iteration 772, loss = 0.11814211\n",
      "Iteration 773, loss = 0.11784330\n",
      "Iteration 774, loss = 0.11785945\n",
      "Iteration 775, loss = 0.11779410\n",
      "Iteration 776, loss = 0.11794732\n",
      "Iteration 777, loss = 0.11785022\n",
      "Iteration 778, loss = 0.11772055\n",
      "Iteration 779, loss = 0.11761820\n",
      "Iteration 780, loss = 0.11740832\n",
      "Iteration 781, loss = 0.11692157\n",
      "Iteration 782, loss = 0.11642872\n",
      "Iteration 783, loss = 0.11590705\n",
      "Iteration 784, loss = 0.11557319\n",
      "Iteration 785, loss = 0.11541805\n",
      "Iteration 786, loss = 0.11495985\n",
      "Iteration 787, loss = 0.11455038\n",
      "Iteration 788, loss = 0.11423855\n",
      "Iteration 789, loss = 0.11398159\n",
      "Iteration 790, loss = 0.11377062\n",
      "Iteration 791, loss = 0.11371220\n",
      "Iteration 792, loss = 0.11354590\n",
      "Iteration 793, loss = 0.11346529\n",
      "Iteration 794, loss = 0.11344487\n",
      "Iteration 795, loss = 0.11403168\n",
      "Iteration 796, loss = 0.11412995\n",
      "Iteration 797, loss = 0.11396711\n",
      "Iteration 798, loss = 0.11360005\n",
      "Iteration 799, loss = 0.11317240\n",
      "Iteration 800, loss = 0.11267653\n",
      "Iteration 801, loss = 0.11236727\n",
      "Iteration 802, loss = 0.11210102\n",
      "Iteration 803, loss = 0.11187750\n",
      "Iteration 804, loss = 0.11161969\n",
      "Iteration 805, loss = 0.11138342\n",
      "Iteration 806, loss = 0.11121045\n",
      "Iteration 807, loss = 0.11111651\n",
      "Iteration 808, loss = 0.11121884\n",
      "Iteration 809, loss = 0.11131973\n",
      "Iteration 810, loss = 0.11103766\n",
      "Iteration 811, loss = 0.11075031\n",
      "Iteration 812, loss = 0.11023729\n",
      "Iteration 813, loss = 0.10959683\n",
      "Iteration 814, loss = 0.10927322\n",
      "Iteration 815, loss = 0.10893459\n",
      "Iteration 816, loss = 0.10868652\n",
      "Iteration 817, loss = 0.10850085\n",
      "Iteration 818, loss = 0.10830003\n",
      "Iteration 819, loss = 0.10803528\n",
      "Iteration 820, loss = 0.10796689\n",
      "Iteration 821, loss = 0.10778075\n",
      "Iteration 822, loss = 0.10764779\n",
      "Iteration 823, loss = 0.10746562\n",
      "Iteration 824, loss = 0.10738776\n",
      "Iteration 825, loss = 0.10715277\n",
      "Iteration 826, loss = 0.10708464\n",
      "Iteration 827, loss = 0.10674463\n",
      "Iteration 828, loss = 0.10644633\n",
      "Iteration 829, loss = 0.10612563\n",
      "Iteration 830, loss = 0.10587802\n",
      "Iteration 831, loss = 0.10578095\n",
      "Iteration 832, loss = 0.10569784\n",
      "Iteration 833, loss = 0.10560542\n",
      "Iteration 834, loss = 0.10553595\n",
      "Iteration 835, loss = 0.10580581\n",
      "Iteration 836, loss = 0.10600394\n",
      "Iteration 837, loss = 0.10578373\n",
      "Iteration 838, loss = 0.10540416\n",
      "Iteration 839, loss = 0.10493757\n",
      "Iteration 840, loss = 0.10459510\n",
      "Iteration 841, loss = 0.10429778\n",
      "Iteration 842, loss = 0.10406997\n",
      "Iteration 843, loss = 0.10382535\n",
      "Iteration 844, loss = 0.10361959\n",
      "Iteration 845, loss = 0.10339592\n",
      "Iteration 846, loss = 0.10303317\n",
      "Iteration 847, loss = 0.10272865\n",
      "Iteration 848, loss = 0.10250961\n",
      "Iteration 849, loss = 0.10241676\n",
      "Iteration 850, loss = 0.10220190\n",
      "Iteration 851, loss = 0.10217424\n",
      "Iteration 852, loss = 0.10228461\n",
      "Iteration 853, loss = 0.10229040\n",
      "Iteration 854, loss = 0.10246732\n",
      "Iteration 855, loss = 0.10254397\n",
      "Iteration 856, loss = 0.10246161\n",
      "Iteration 857, loss = 0.10238279\n",
      "Iteration 858, loss = 0.10215649\n",
      "Iteration 859, loss = 0.10184315\n",
      "Iteration 860, loss = 0.10163498\n",
      "Iteration 861, loss = 0.10148623\n",
      "Iteration 862, loss = 0.10142567\n",
      "Iteration 863, loss = 0.10118154\n",
      "Iteration 864, loss = 0.10079158\n",
      "Iteration 865, loss = 0.10046138\n",
      "Iteration 866, loss = 0.10005756\n",
      "Iteration 867, loss = 0.09968499\n",
      "Iteration 868, loss = 0.09956076\n",
      "Iteration 869, loss = 0.09936266\n",
      "Iteration 870, loss = 0.09945868\n",
      "Iteration 871, loss = 0.09952780\n",
      "Iteration 872, loss = 0.09930354\n",
      "Iteration 873, loss = 0.09887119\n",
      "Iteration 874, loss = 0.09835595\n",
      "Iteration 875, loss = 0.09824690\n",
      "Iteration 876, loss = 0.09775789\n",
      "Iteration 877, loss = 0.09740889\n",
      "Iteration 878, loss = 0.09720597\n",
      "Iteration 879, loss = 0.09711495\n",
      "Iteration 880, loss = 0.09697803\n",
      "Iteration 881, loss = 0.09683206\n",
      "Iteration 882, loss = 0.09654211\n",
      "Iteration 883, loss = 0.09610999\n",
      "Iteration 884, loss = 0.09586383\n",
      "Iteration 885, loss = 0.09555097\n",
      "Iteration 886, loss = 0.09527027\n",
      "Iteration 887, loss = 0.09499680\n",
      "Iteration 888, loss = 0.09486108\n",
      "Iteration 889, loss = 0.09495903\n",
      "Iteration 890, loss = 0.09457931\n",
      "Iteration 891, loss = 0.09416404\n",
      "Iteration 892, loss = 0.09390563\n",
      "Iteration 893, loss = 0.09395098\n",
      "Iteration 894, loss = 0.09403393\n",
      "Iteration 895, loss = 0.09438668\n",
      "Iteration 896, loss = 0.09450613\n",
      "Iteration 897, loss = 0.09426492\n",
      "Iteration 898, loss = 0.09379809\n",
      "Iteration 899, loss = 0.09331900\n",
      "Iteration 900, loss = 0.09305272\n",
      "Iteration 901, loss = 0.09275196\n",
      "Iteration 902, loss = 0.09253591\n",
      "Iteration 903, loss = 0.09236851\n",
      "Iteration 904, loss = 0.09227215\n",
      "Iteration 905, loss = 0.09227835\n",
      "Iteration 906, loss = 0.09208436\n",
      "Iteration 907, loss = 0.09191553\n",
      "Iteration 908, loss = 0.09167715\n",
      "Iteration 909, loss = 0.09144856\n",
      "Iteration 910, loss = 0.09118675\n",
      "Iteration 911, loss = 0.09101170\n",
      "Iteration 912, loss = 0.09062846\n",
      "Iteration 913, loss = 0.09034176\n",
      "Iteration 914, loss = 0.09015436\n",
      "Iteration 915, loss = 0.08997163\n",
      "Iteration 916, loss = 0.09005681\n",
      "Iteration 917, loss = 0.09001537\n",
      "Iteration 918, loss = 0.08981900\n",
      "Iteration 919, loss = 0.08933681\n",
      "Iteration 920, loss = 0.08916050\n",
      "Iteration 921, loss = 0.08878952\n",
      "Iteration 922, loss = 0.08865237\n",
      "Iteration 923, loss = 0.08837780\n",
      "Iteration 924, loss = 0.08826060\n",
      "Iteration 925, loss = 0.08814147\n",
      "Iteration 926, loss = 0.08807456\n",
      "Iteration 927, loss = 0.08787872\n",
      "Iteration 928, loss = 0.08767929\n",
      "Iteration 929, loss = 0.08747518\n",
      "Iteration 930, loss = 0.08731434\n",
      "Iteration 931, loss = 0.08701832\n",
      "Iteration 932, loss = 0.08682985\n",
      "Iteration 933, loss = 0.08667718\n",
      "Iteration 934, loss = 0.08640084\n",
      "Iteration 935, loss = 0.08613493\n",
      "Iteration 936, loss = 0.08586154\n",
      "Iteration 937, loss = 0.08575663\n",
      "Iteration 938, loss = 0.08561589\n",
      "Iteration 939, loss = 0.08548717\n",
      "Iteration 940, loss = 0.08535554\n",
      "Iteration 941, loss = 0.08512859\n",
      "Iteration 942, loss = 0.08492177\n",
      "Iteration 943, loss = 0.08467141\n",
      "Iteration 944, loss = 0.08448568\n",
      "Iteration 945, loss = 0.08433205\n",
      "Iteration 946, loss = 0.08414544\n",
      "Iteration 947, loss = 0.08405549\n",
      "Iteration 948, loss = 0.08398124\n",
      "Iteration 949, loss = 0.08384463\n",
      "Iteration 950, loss = 0.08370799\n",
      "Iteration 951, loss = 0.08359069\n",
      "Iteration 952, loss = 0.08347723\n",
      "Iteration 953, loss = 0.08332608\n",
      "Iteration 954, loss = 0.08316755\n",
      "Iteration 955, loss = 0.08298117\n",
      "Iteration 956, loss = 0.08289484\n",
      "Iteration 957, loss = 0.08270706\n",
      "Iteration 958, loss = 0.08245259\n",
      "Iteration 959, loss = 0.08225117\n",
      "Iteration 960, loss = 0.08202605\n",
      "Iteration 961, loss = 0.08197837\n",
      "Iteration 962, loss = 0.08202650\n",
      "Iteration 963, loss = 0.08190003\n",
      "Iteration 964, loss = 0.08172716\n",
      "Iteration 965, loss = 0.08154325\n",
      "Iteration 966, loss = 0.08131159\n",
      "Iteration 967, loss = 0.08119893\n",
      "Iteration 968, loss = 0.08106664\n",
      "Iteration 969, loss = 0.08093750\n",
      "Iteration 970, loss = 0.08084422\n",
      "Iteration 971, loss = 0.08072936\n",
      "Iteration 972, loss = 0.08084759\n",
      "Iteration 973, loss = 0.08129978\n",
      "Iteration 974, loss = 0.08097681\n",
      "Iteration 975, loss = 0.08056064\n",
      "Iteration 976, loss = 0.08005078\n",
      "Iteration 977, loss = 0.07979550\n",
      "Iteration 978, loss = 0.07999353\n",
      "Iteration 979, loss = 0.08005901\n",
      "Iteration 980, loss = 0.08021500\n",
      "Iteration 981, loss = 0.08026158\n",
      "Iteration 982, loss = 0.08007074\n",
      "Iteration 983, loss = 0.07973854\n",
      "Iteration 984, loss = 0.07945125\n",
      "Iteration 985, loss = 0.07910243\n",
      "Iteration 986, loss = 0.07882961\n",
      "Iteration 987, loss = 0.07847305\n",
      "Iteration 988, loss = 0.07833707\n",
      "Iteration 989, loss = 0.07830383\n",
      "Iteration 990, loss = 0.07801035\n",
      "Iteration 991, loss = 0.07782980\n",
      "Iteration 992, loss = 0.07778015\n",
      "Iteration 993, loss = 0.07736959\n",
      "Iteration 994, loss = 0.07726003\n",
      "Iteration 995, loss = 0.07741826\n",
      "Iteration 996, loss = 0.07743898\n",
      "Iteration 997, loss = 0.07721278\n",
      "Iteration 998, loss = 0.07691619\n",
      "Iteration 999, loss = 0.07649474\n",
      "Iteration 1000, loss = 0.07619996\n",
      "Iteration 1001, loss = 0.07595328\n",
      "Iteration 1002, loss = 0.07580942\n",
      "Iteration 1003, loss = 0.07560036\n",
      "Iteration 1004, loss = 0.07565687\n",
      "Iteration 1005, loss = 0.07605391\n",
      "Iteration 1006, loss = 0.07637055\n",
      "Iteration 1007, loss = 0.07644138\n",
      "Iteration 1008, loss = 0.07582727\n",
      "Iteration 1009, loss = 0.07522069\n",
      "Iteration 1010, loss = 0.07477782\n",
      "Iteration 1011, loss = 0.07462525\n",
      "Iteration 1012, loss = 0.07438474\n",
      "Iteration 1013, loss = 0.07437145\n",
      "Iteration 1014, loss = 0.07422972\n",
      "Iteration 1015, loss = 0.07431958\n",
      "Iteration 1016, loss = 0.07442520\n",
      "Iteration 1017, loss = 0.07467016\n",
      "Iteration 1018, loss = 0.07469297\n",
      "Iteration 1019, loss = 0.07458723\n",
      "Iteration 1020, loss = 0.07460521\n",
      "Iteration 1021, loss = 0.07461822\n",
      "Iteration 1022, loss = 0.07458450\n",
      "Iteration 1023, loss = 0.07470679\n",
      "Iteration 1024, loss = 0.07490745\n",
      "Iteration 1025, loss = 0.07486263\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier6 = MLPClassifier(max_iter=2000,activation='relu', verbose=True, hidden_layer_sizes=(15,6),random_state=1,learning_rate_init=0.001)\n",
    "mlpClassifier6.fit(X_train, y_train)\n",
    "y_pred6 = mlpClassifier6.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95c428e6",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41122bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56989247311827"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e693fbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde364f0ac0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 6')\n",
    "plt.plot(range(len(mlpClassifier6.loss_curve_)),mlpClassifier6.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9a7e93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix6 = metrics.confusion_matrix(y_test,y_pred6)\n",
    "sns.heatmap(pd.DataFrame(confMatrix6),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b41baa6",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a17746d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63837118\n",
      "Iteration 2, loss = 0.61485584\n",
      "Iteration 3, loss = 0.59497575\n",
      "Iteration 4, loss = 0.57803542\n",
      "Iteration 5, loss = 0.56274258\n",
      "Iteration 6, loss = 0.54978820\n",
      "Iteration 7, loss = 0.53810396\n",
      "Iteration 8, loss = 0.52796557\n",
      "Iteration 9, loss = 0.51869406\n",
      "Iteration 10, loss = 0.51085741\n",
      "Iteration 11, loss = 0.50326898\n",
      "Iteration 12, loss = 0.49699277\n",
      "Iteration 13, loss = 0.49121515\n",
      "Iteration 14, loss = 0.48571616\n",
      "Iteration 15, loss = 0.48095659\n",
      "Iteration 16, loss = 0.47601943\n",
      "Iteration 17, loss = 0.47152075\n",
      "Iteration 18, loss = 0.46690501\n",
      "Iteration 19, loss = 0.46223107\n",
      "Iteration 20, loss = 0.45757267\n",
      "Iteration 21, loss = 0.45317530\n",
      "Iteration 22, loss = 0.44876030\n",
      "Iteration 23, loss = 0.44461829\n",
      "Iteration 24, loss = 0.44063107\n",
      "Iteration 25, loss = 0.43656671\n",
      "Iteration 26, loss = 0.43287694\n",
      "Iteration 27, loss = 0.42951171\n",
      "Iteration 28, loss = 0.42600997\n",
      "Iteration 29, loss = 0.42297505\n",
      "Iteration 30, loss = 0.42038960\n",
      "Iteration 31, loss = 0.41749683\n",
      "Iteration 32, loss = 0.41463263\n",
      "Iteration 33, loss = 0.41182889\n",
      "Iteration 34, loss = 0.40883069\n",
      "Iteration 35, loss = 0.40573625\n",
      "Iteration 36, loss = 0.40260144\n",
      "Iteration 37, loss = 0.39972128\n",
      "Iteration 38, loss = 0.39691889\n",
      "Iteration 39, loss = 0.39383087\n",
      "Iteration 40, loss = 0.39076203\n",
      "Iteration 41, loss = 0.38761891\n",
      "Iteration 42, loss = 0.38473826\n",
      "Iteration 43, loss = 0.38185499\n",
      "Iteration 44, loss = 0.37915177\n",
      "Iteration 45, loss = 0.37647897\n",
      "Iteration 46, loss = 0.37399295\n",
      "Iteration 47, loss = 0.37140080\n",
      "Iteration 48, loss = 0.36884047\n",
      "Iteration 49, loss = 0.36641080\n",
      "Iteration 50, loss = 0.36411334\n",
      "Iteration 51, loss = 0.36193315\n",
      "Iteration 52, loss = 0.35953565\n",
      "Iteration 53, loss = 0.35745664\n",
      "Iteration 54, loss = 0.35578999\n",
      "Iteration 55, loss = 0.35364320\n",
      "Iteration 56, loss = 0.35152241\n",
      "Iteration 57, loss = 0.34928765\n",
      "Iteration 58, loss = 0.34663205\n",
      "Iteration 59, loss = 0.34403799\n",
      "Iteration 60, loss = 0.34131950\n",
      "Iteration 61, loss = 0.33882031\n",
      "Iteration 62, loss = 0.33661328\n",
      "Iteration 63, loss = 0.33437729\n",
      "Iteration 64, loss = 0.33221762\n",
      "Iteration 65, loss = 0.33018670\n",
      "Iteration 66, loss = 0.32830030\n",
      "Iteration 67, loss = 0.32636086\n",
      "Iteration 68, loss = 0.32453965\n",
      "Iteration 69, loss = 0.32271811\n",
      "Iteration 70, loss = 0.32082771\n",
      "Iteration 71, loss = 0.31904043\n",
      "Iteration 72, loss = 0.31722466\n",
      "Iteration 73, loss = 0.31543196\n",
      "Iteration 74, loss = 0.31354161\n",
      "Iteration 75, loss = 0.31162899\n",
      "Iteration 76, loss = 0.30971850\n",
      "Iteration 77, loss = 0.30775799\n",
      "Iteration 78, loss = 0.30573367\n",
      "Iteration 79, loss = 0.30405070\n",
      "Iteration 80, loss = 0.30215154\n",
      "Iteration 81, loss = 0.30053396\n",
      "Iteration 82, loss = 0.29889707\n",
      "Iteration 83, loss = 0.29688561\n",
      "Iteration 84, loss = 0.29495107\n",
      "Iteration 85, loss = 0.29281532\n",
      "Iteration 86, loss = 0.29085711\n",
      "Iteration 87, loss = 0.28894406\n",
      "Iteration 88, loss = 0.28723778\n",
      "Iteration 89, loss = 0.28576991\n",
      "Iteration 90, loss = 0.28477408\n",
      "Iteration 91, loss = 0.28348259\n",
      "Iteration 92, loss = 0.28218014\n",
      "Iteration 93, loss = 0.28065135\n",
      "Iteration 94, loss = 0.27916946\n",
      "Iteration 95, loss = 0.27764211\n",
      "Iteration 96, loss = 0.27633817\n",
      "Iteration 97, loss = 0.27461352\n",
      "Iteration 98, loss = 0.27339812\n",
      "Iteration 99, loss = 0.27196193\n",
      "Iteration 100, loss = 0.27113475\n",
      "Iteration 101, loss = 0.26975882\n",
      "Iteration 102, loss = 0.26872652\n",
      "Iteration 103, loss = 0.26737732\n",
      "Iteration 104, loss = 0.26602899\n",
      "Iteration 105, loss = 0.26422341\n",
      "Iteration 106, loss = 0.26203728\n",
      "Iteration 107, loss = 0.26071053\n",
      "Iteration 108, loss = 0.25990081\n",
      "Iteration 109, loss = 0.25969182\n",
      "Iteration 110, loss = 0.25939583\n",
      "Iteration 111, loss = 0.25900400\n",
      "Iteration 112, loss = 0.25790297\n",
      "Iteration 113, loss = 0.25671754\n",
      "Iteration 114, loss = 0.25549178\n",
      "Iteration 115, loss = 0.25379382\n",
      "Iteration 116, loss = 0.25181705\n",
      "Iteration 117, loss = 0.24983206\n",
      "Iteration 118, loss = 0.24753588\n",
      "Iteration 119, loss = 0.24599062\n",
      "Iteration 120, loss = 0.24450114\n",
      "Iteration 121, loss = 0.24344226\n",
      "Iteration 122, loss = 0.24275976\n",
      "Iteration 123, loss = 0.24205664\n",
      "Iteration 124, loss = 0.24168379\n",
      "Iteration 125, loss = 0.24101028\n",
      "Iteration 126, loss = 0.23982819\n",
      "Iteration 127, loss = 0.23873598\n",
      "Iteration 128, loss = 0.23752856\n",
      "Iteration 129, loss = 0.23645795\n",
      "Iteration 130, loss = 0.23535778\n",
      "Iteration 131, loss = 0.23422613\n",
      "Iteration 132, loss = 0.23308288\n",
      "Iteration 133, loss = 0.23173472\n",
      "Iteration 134, loss = 0.23003522\n",
      "Iteration 135, loss = 0.22882122\n",
      "Iteration 136, loss = 0.22757840\n",
      "Iteration 137, loss = 0.22691900\n",
      "Iteration 138, loss = 0.22601453\n",
      "Iteration 139, loss = 0.22521126\n",
      "Iteration 140, loss = 0.22427024\n",
      "Iteration 141, loss = 0.22327160\n",
      "Iteration 142, loss = 0.22231814\n",
      "Iteration 143, loss = 0.22133592\n",
      "Iteration 144, loss = 0.22046373\n",
      "Iteration 145, loss = 0.22019008\n",
      "Iteration 146, loss = 0.22016362\n",
      "Iteration 147, loss = 0.21943343\n",
      "Iteration 148, loss = 0.21861822\n",
      "Iteration 149, loss = 0.21756926\n",
      "Iteration 150, loss = 0.21678330\n",
      "Iteration 151, loss = 0.21567580\n",
      "Iteration 152, loss = 0.21493955\n",
      "Iteration 153, loss = 0.21416861\n",
      "Iteration 154, loss = 0.21355778\n",
      "Iteration 155, loss = 0.21279314\n",
      "Iteration 156, loss = 0.21176946\n",
      "Iteration 157, loss = 0.21095556\n",
      "Iteration 158, loss = 0.21010195\n",
      "Iteration 159, loss = 0.20911517\n",
      "Iteration 160, loss = 0.20815652\n",
      "Iteration 161, loss = 0.20733879\n",
      "Iteration 162, loss = 0.20667479\n",
      "Iteration 163, loss = 0.20608147\n",
      "Iteration 164, loss = 0.20522109\n",
      "Iteration 165, loss = 0.20418887\n",
      "Iteration 166, loss = 0.20335799\n",
      "Iteration 167, loss = 0.20245956\n",
      "Iteration 168, loss = 0.20164195\n",
      "Iteration 169, loss = 0.20085688\n",
      "Iteration 170, loss = 0.20027193\n",
      "Iteration 171, loss = 0.19945171\n",
      "Iteration 172, loss = 0.19865541\n",
      "Iteration 173, loss = 0.19807010\n",
      "Iteration 174, loss = 0.19741980\n",
      "Iteration 175, loss = 0.19683950\n",
      "Iteration 176, loss = 0.19648748\n",
      "Iteration 177, loss = 0.19604985\n",
      "Iteration 178, loss = 0.19544403\n",
      "Iteration 179, loss = 0.19482734\n",
      "Iteration 180, loss = 0.19419285\n",
      "Iteration 181, loss = 0.19367290\n",
      "Iteration 182, loss = 0.19312518\n",
      "Iteration 183, loss = 0.19257055\n",
      "Iteration 184, loss = 0.19195399\n",
      "Iteration 185, loss = 0.19141265\n",
      "Iteration 186, loss = 0.19100201\n",
      "Iteration 187, loss = 0.19033200\n",
      "Iteration 188, loss = 0.18970842\n",
      "Iteration 189, loss = 0.18890827\n",
      "Iteration 190, loss = 0.18800722\n",
      "Iteration 191, loss = 0.18706076\n",
      "Iteration 192, loss = 0.18629161\n",
      "Iteration 193, loss = 0.18570302\n",
      "Iteration 194, loss = 0.18501385\n",
      "Iteration 195, loss = 0.18447546\n",
      "Iteration 196, loss = 0.18389193\n",
      "Iteration 197, loss = 0.18329934\n",
      "Iteration 198, loss = 0.18280180\n",
      "Iteration 199, loss = 0.18282278\n",
      "Iteration 200, loss = 0.18230436\n",
      "Iteration 201, loss = 0.18137618\n",
      "Iteration 202, loss = 0.18035707\n",
      "Iteration 203, loss = 0.17941706\n",
      "Iteration 204, loss = 0.17845274\n",
      "Iteration 205, loss = 0.17769847\n",
      "Iteration 206, loss = 0.17719356\n",
      "Iteration 207, loss = 0.17640228\n",
      "Iteration 208, loss = 0.17571752\n",
      "Iteration 209, loss = 0.17521561\n",
      "Iteration 210, loss = 0.17454499\n",
      "Iteration 211, loss = 0.17382962\n",
      "Iteration 212, loss = 0.17322395\n",
      "Iteration 213, loss = 0.17280607\n",
      "Iteration 214, loss = 0.17249093\n",
      "Iteration 215, loss = 0.17225148\n",
      "Iteration 216, loss = 0.17215259\n",
      "Iteration 217, loss = 0.17183528\n",
      "Iteration 218, loss = 0.17117850\n",
      "Iteration 219, loss = 0.17038006\n",
      "Iteration 220, loss = 0.16969816\n",
      "Iteration 221, loss = 0.16906800\n",
      "Iteration 222, loss = 0.16826073\n",
      "Iteration 223, loss = 0.16758469\n",
      "Iteration 224, loss = 0.16708670\n",
      "Iteration 225, loss = 0.16650927\n",
      "Iteration 226, loss = 0.16614059\n",
      "Iteration 227, loss = 0.16568293\n",
      "Iteration 228, loss = 0.16528600\n",
      "Iteration 229, loss = 0.16457543\n",
      "Iteration 230, loss = 0.16403037\n",
      "Iteration 231, loss = 0.16333717\n",
      "Iteration 232, loss = 0.16259335\n",
      "Iteration 233, loss = 0.16194779\n",
      "Iteration 234, loss = 0.16128990\n",
      "Iteration 235, loss = 0.16091692\n",
      "Iteration 236, loss = 0.16040983\n",
      "Iteration 237, loss = 0.16019260\n",
      "Iteration 238, loss = 0.15977858\n",
      "Iteration 239, loss = 0.15919252\n",
      "Iteration 240, loss = 0.15851469\n",
      "Iteration 241, loss = 0.15792193\n",
      "Iteration 242, loss = 0.15740308\n",
      "Iteration 243, loss = 0.15701764\n",
      "Iteration 244, loss = 0.15651539\n",
      "Iteration 245, loss = 0.15588903\n",
      "Iteration 246, loss = 0.15538929\n",
      "Iteration 247, loss = 0.15514152\n",
      "Iteration 248, loss = 0.15457999\n",
      "Iteration 249, loss = 0.15412814\n",
      "Iteration 250, loss = 0.15367145\n",
      "Iteration 251, loss = 0.15315496\n",
      "Iteration 252, loss = 0.15265747\n",
      "Iteration 253, loss = 0.15210227\n",
      "Iteration 254, loss = 0.15170287\n",
      "Iteration 255, loss = 0.15120397\n",
      "Iteration 256, loss = 0.15074317\n",
      "Iteration 257, loss = 0.15027606\n",
      "Iteration 258, loss = 0.14981092\n",
      "Iteration 259, loss = 0.14941084\n",
      "Iteration 260, loss = 0.14874232\n",
      "Iteration 261, loss = 0.14823281\n",
      "Iteration 262, loss = 0.14793821\n",
      "Iteration 263, loss = 0.14745289\n",
      "Iteration 264, loss = 0.14685687\n",
      "Iteration 265, loss = 0.14646597\n",
      "Iteration 266, loss = 0.14608698\n",
      "Iteration 267, loss = 0.14548348\n",
      "Iteration 268, loss = 0.14499678\n",
      "Iteration 269, loss = 0.14476461\n",
      "Iteration 270, loss = 0.14494506\n",
      "Iteration 271, loss = 0.14441380\n",
      "Iteration 272, loss = 0.14422361\n",
      "Iteration 273, loss = 0.14379799\n",
      "Iteration 274, loss = 0.14387659\n",
      "Iteration 275, loss = 0.14405343\n",
      "Iteration 276, loss = 0.14413308\n",
      "Iteration 277, loss = 0.14413793\n",
      "Iteration 278, loss = 0.14409706\n",
      "Iteration 279, loss = 0.14344545\n",
      "Iteration 280, loss = 0.14245531\n",
      "Iteration 281, loss = 0.14141601\n",
      "Iteration 282, loss = 0.14070506\n",
      "Iteration 283, loss = 0.14010020\n",
      "Iteration 284, loss = 0.13969708\n",
      "Iteration 285, loss = 0.13936683\n",
      "Iteration 286, loss = 0.13910689\n",
      "Iteration 287, loss = 0.13892670\n",
      "Iteration 288, loss = 0.13866213\n",
      "Iteration 289, loss = 0.13841552\n",
      "Iteration 290, loss = 0.13828258\n",
      "Iteration 291, loss = 0.13825616\n",
      "Iteration 292, loss = 0.13827392\n",
      "Iteration 293, loss = 0.13810248\n",
      "Iteration 294, loss = 0.13799508\n",
      "Iteration 295, loss = 0.13772581\n",
      "Iteration 296, loss = 0.13696777\n",
      "Iteration 297, loss = 0.13589115\n",
      "Iteration 298, loss = 0.13508538\n",
      "Iteration 299, loss = 0.13430889\n",
      "Iteration 300, loss = 0.13345051\n",
      "Iteration 301, loss = 0.13287611\n",
      "Iteration 302, loss = 0.13273943\n",
      "Iteration 303, loss = 0.13260252\n",
      "Iteration 304, loss = 0.13261628\n",
      "Iteration 305, loss = 0.13220525\n",
      "Iteration 306, loss = 0.13192194\n",
      "Iteration 307, loss = 0.13184057\n",
      "Iteration 308, loss = 0.13152175\n",
      "Iteration 309, loss = 0.13109512\n",
      "Iteration 310, loss = 0.13067279\n",
      "Iteration 311, loss = 0.13043643\n",
      "Iteration 312, loss = 0.12989619\n",
      "Iteration 313, loss = 0.12926700\n",
      "Iteration 314, loss = 0.12906092\n",
      "Iteration 315, loss = 0.12920686\n",
      "Iteration 316, loss = 0.12913645\n",
      "Iteration 317, loss = 0.12873635\n",
      "Iteration 318, loss = 0.12775245\n",
      "Iteration 319, loss = 0.12654198\n",
      "Iteration 320, loss = 0.12577856\n",
      "Iteration 321, loss = 0.12499287\n",
      "Iteration 322, loss = 0.12442453\n",
      "Iteration 323, loss = 0.12394730\n",
      "Iteration 324, loss = 0.12367651\n",
      "Iteration 325, loss = 0.12343271\n",
      "Iteration 326, loss = 0.12299677\n",
      "Iteration 327, loss = 0.12247194\n",
      "Iteration 328, loss = 0.12195463\n",
      "Iteration 329, loss = 0.12125212\n",
      "Iteration 330, loss = 0.12080690\n",
      "Iteration 331, loss = 0.12043271\n",
      "Iteration 332, loss = 0.12006934\n",
      "Iteration 333, loss = 0.11976361\n",
      "Iteration 334, loss = 0.11946161\n",
      "Iteration 335, loss = 0.11908855\n",
      "Iteration 336, loss = 0.11864407\n",
      "Iteration 337, loss = 0.11813558\n",
      "Iteration 338, loss = 0.11767134\n",
      "Iteration 339, loss = 0.11720750\n",
      "Iteration 340, loss = 0.11684785\n",
      "Iteration 341, loss = 0.11651759\n",
      "Iteration 342, loss = 0.11630254\n",
      "Iteration 343, loss = 0.11599071\n",
      "Iteration 344, loss = 0.11585734\n",
      "Iteration 345, loss = 0.11565263\n",
      "Iteration 346, loss = 0.11524658\n",
      "Iteration 347, loss = 0.11475457\n",
      "Iteration 348, loss = 0.11424513\n",
      "Iteration 349, loss = 0.11386539\n",
      "Iteration 350, loss = 0.11382849\n",
      "Iteration 351, loss = 0.11427540\n",
      "Iteration 352, loss = 0.11384563\n",
      "Iteration 353, loss = 0.11339809\n",
      "Iteration 354, loss = 0.11287480\n",
      "Iteration 355, loss = 0.11239088\n",
      "Iteration 356, loss = 0.11204001\n",
      "Iteration 357, loss = 0.11145822\n",
      "Iteration 358, loss = 0.11112252\n",
      "Iteration 359, loss = 0.11059224\n",
      "Iteration 360, loss = 0.10997739\n",
      "Iteration 361, loss = 0.10970055\n",
      "Iteration 362, loss = 0.10919736\n",
      "Iteration 363, loss = 0.10899201\n",
      "Iteration 364, loss = 0.10890417\n",
      "Iteration 365, loss = 0.10886891\n",
      "Iteration 366, loss = 0.10884484\n",
      "Iteration 367, loss = 0.10830465\n",
      "Iteration 368, loss = 0.10798682\n",
      "Iteration 369, loss = 0.10787054\n",
      "Iteration 370, loss = 0.10788231\n",
      "Iteration 371, loss = 0.10737477\n",
      "Iteration 372, loss = 0.10679341\n",
      "Iteration 373, loss = 0.10631118\n",
      "Iteration 374, loss = 0.10604351\n",
      "Iteration 375, loss = 0.10576508\n",
      "Iteration 376, loss = 0.10558342\n",
      "Iteration 377, loss = 0.10547879\n",
      "Iteration 378, loss = 0.10513021\n",
      "Iteration 379, loss = 0.10469019\n",
      "Iteration 380, loss = 0.10504871\n",
      "Iteration 381, loss = 0.10494584\n",
      "Iteration 382, loss = 0.10463451\n",
      "Iteration 383, loss = 0.10370016\n",
      "Iteration 384, loss = 0.10282861\n",
      "Iteration 385, loss = 0.10225825\n",
      "Iteration 386, loss = 0.10209527\n",
      "Iteration 387, loss = 0.10204818\n",
      "Iteration 388, loss = 0.10210084\n",
      "Iteration 389, loss = 0.10211001\n",
      "Iteration 390, loss = 0.10196624\n",
      "Iteration 391, loss = 0.10173091\n",
      "Iteration 392, loss = 0.10122789\n",
      "Iteration 393, loss = 0.10051486\n",
      "Iteration 394, loss = 0.10022701\n",
      "Iteration 395, loss = 0.10014401\n",
      "Iteration 396, loss = 0.10000112\n",
      "Iteration 397, loss = 0.09967403\n",
      "Iteration 398, loss = 0.09924086\n",
      "Iteration 399, loss = 0.09871047\n",
      "Iteration 400, loss = 0.09819770\n",
      "Iteration 401, loss = 0.09766259\n",
      "Iteration 402, loss = 0.09721655\n",
      "Iteration 403, loss = 0.09681710\n",
      "Iteration 404, loss = 0.09651945\n",
      "Iteration 405, loss = 0.09612390\n",
      "Iteration 406, loss = 0.09595208\n",
      "Iteration 407, loss = 0.09589089\n",
      "Iteration 408, loss = 0.09577798\n",
      "Iteration 409, loss = 0.09556527\n",
      "Iteration 410, loss = 0.09523575\n",
      "Iteration 411, loss = 0.09486260\n",
      "Iteration 412, loss = 0.09440269\n",
      "Iteration 413, loss = 0.09394651\n",
      "Iteration 414, loss = 0.09341508\n",
      "Iteration 415, loss = 0.09315000\n",
      "Iteration 416, loss = 0.09293927\n",
      "Iteration 417, loss = 0.09332207\n",
      "Iteration 418, loss = 0.09316954\n",
      "Iteration 419, loss = 0.09282474\n",
      "Iteration 420, loss = 0.09249573\n",
      "Iteration 421, loss = 0.09194915\n",
      "Iteration 422, loss = 0.09134055\n",
      "Iteration 423, loss = 0.09076354\n",
      "Iteration 424, loss = 0.09040197\n",
      "Iteration 425, loss = 0.09003539\n",
      "Iteration 426, loss = 0.08990837\n",
      "Iteration 427, loss = 0.08994569\n",
      "Iteration 428, loss = 0.08973092\n",
      "Iteration 429, loss = 0.08970261\n",
      "Iteration 430, loss = 0.08926804\n",
      "Iteration 431, loss = 0.08870564\n",
      "Iteration 432, loss = 0.08831311\n",
      "Iteration 433, loss = 0.08786833\n",
      "Iteration 434, loss = 0.08755169\n",
      "Iteration 435, loss = 0.08735746\n",
      "Iteration 436, loss = 0.08695394\n",
      "Iteration 437, loss = 0.08656250\n",
      "Iteration 438, loss = 0.08618281\n",
      "Iteration 439, loss = 0.08585041\n",
      "Iteration 440, loss = 0.08567906\n",
      "Iteration 441, loss = 0.08591244\n",
      "Iteration 442, loss = 0.08620144\n",
      "Iteration 443, loss = 0.08612729\n",
      "Iteration 444, loss = 0.08587643\n",
      "Iteration 445, loss = 0.08535299\n",
      "Iteration 446, loss = 0.08467477\n",
      "Iteration 447, loss = 0.08421074\n",
      "Iteration 448, loss = 0.08397249\n",
      "Iteration 449, loss = 0.08364354\n",
      "Iteration 450, loss = 0.08319166\n",
      "Iteration 451, loss = 0.08267917\n",
      "Iteration 452, loss = 0.08248504\n",
      "Iteration 453, loss = 0.08244878\n",
      "Iteration 454, loss = 0.08308931\n",
      "Iteration 455, loss = 0.08298010\n",
      "Iteration 456, loss = 0.08270934\n",
      "Iteration 457, loss = 0.08280454\n",
      "Iteration 458, loss = 0.08274161\n",
      "Iteration 459, loss = 0.08226879\n",
      "Iteration 460, loss = 0.08158332\n",
      "Iteration 461, loss = 0.08098492\n",
      "Iteration 462, loss = 0.08066466\n",
      "Iteration 463, loss = 0.08025093\n",
      "Iteration 464, loss = 0.08002014\n",
      "Iteration 465, loss = 0.07989798\n",
      "Iteration 466, loss = 0.07956612\n",
      "Iteration 467, loss = 0.07955646\n",
      "Iteration 468, loss = 0.07948069\n",
      "Iteration 469, loss = 0.07924200\n",
      "Iteration 470, loss = 0.07873638\n",
      "Iteration 471, loss = 0.07819159\n",
      "Iteration 472, loss = 0.07775301\n",
      "Iteration 473, loss = 0.07759485\n",
      "Iteration 474, loss = 0.07723822\n",
      "Iteration 475, loss = 0.07700437\n",
      "Iteration 476, loss = 0.07684756\n",
      "Iteration 477, loss = 0.07678269\n",
      "Iteration 478, loss = 0.07674192\n",
      "Iteration 479, loss = 0.07669612\n",
      "Iteration 480, loss = 0.07662816\n",
      "Iteration 481, loss = 0.07654275\n",
      "Iteration 482, loss = 0.07638517\n",
      "Iteration 483, loss = 0.07646727\n",
      "Iteration 484, loss = 0.07678411\n",
      "Iteration 485, loss = 0.07658124\n",
      "Iteration 486, loss = 0.07619955\n",
      "Iteration 487, loss = 0.07582087\n",
      "Iteration 488, loss = 0.07520496\n",
      "Iteration 489, loss = 0.07464239\n",
      "Iteration 490, loss = 0.07411759\n",
      "Iteration 491, loss = 0.07383687\n",
      "Iteration 492, loss = 0.07387653\n",
      "Iteration 493, loss = 0.07369026\n",
      "Iteration 494, loss = 0.07335358\n",
      "Iteration 495, loss = 0.07293077\n",
      "Iteration 496, loss = 0.07244126\n",
      "Iteration 497, loss = 0.07207962\n",
      "Iteration 498, loss = 0.07176695\n",
      "Iteration 499, loss = 0.07146457\n",
      "Iteration 500, loss = 0.07126749\n",
      "Iteration 501, loss = 0.07106588\n",
      "Iteration 502, loss = 0.07096312\n",
      "Iteration 503, loss = 0.07089854\n",
      "Iteration 504, loss = 0.07082612\n",
      "Iteration 505, loss = 0.07061375\n",
      "Iteration 506, loss = 0.07050807\n",
      "Iteration 507, loss = 0.07049766\n",
      "Iteration 508, loss = 0.07028059\n",
      "Iteration 509, loss = 0.06992033\n",
      "Iteration 510, loss = 0.06959219\n",
      "Iteration 511, loss = 0.06928070\n",
      "Iteration 512, loss = 0.06901783\n",
      "Iteration 513, loss = 0.06872043\n",
      "Iteration 514, loss = 0.06839893\n",
      "Iteration 515, loss = 0.06820601\n",
      "Iteration 516, loss = 0.06817923\n",
      "Iteration 517, loss = 0.06826006\n",
      "Iteration 518, loss = 0.06828659\n",
      "Iteration 519, loss = 0.06840847\n",
      "Iteration 520, loss = 0.06847451\n",
      "Iteration 521, loss = 0.06830639\n",
      "Iteration 522, loss = 0.06811856\n",
      "Iteration 523, loss = 0.06802275\n",
      "Iteration 524, loss = 0.06794322\n",
      "Iteration 525, loss = 0.06773676\n",
      "Iteration 526, loss = 0.06735908\n",
      "Iteration 527, loss = 0.06694705\n",
      "Iteration 528, loss = 0.06660390\n",
      "Iteration 529, loss = 0.06616295\n",
      "Iteration 530, loss = 0.06582127\n",
      "Iteration 531, loss = 0.06567972\n",
      "Iteration 532, loss = 0.06570660\n",
      "Iteration 533, loss = 0.06586616\n",
      "Iteration 534, loss = 0.06571929\n",
      "Iteration 535, loss = 0.06550513\n",
      "Iteration 536, loss = 0.06525335\n",
      "Iteration 537, loss = 0.06490010\n",
      "Iteration 538, loss = 0.06456852\n",
      "Iteration 539, loss = 0.06433889\n",
      "Iteration 540, loss = 0.06401884\n",
      "Iteration 541, loss = 0.06372021\n",
      "Iteration 542, loss = 0.06348484\n",
      "Iteration 543, loss = 0.06338923\n",
      "Iteration 544, loss = 0.06333088\n",
      "Iteration 545, loss = 0.06311459\n",
      "Iteration 546, loss = 0.06292687\n",
      "Iteration 547, loss = 0.06283960\n",
      "Iteration 548, loss = 0.06262252\n",
      "Iteration 549, loss = 0.06234618\n",
      "Iteration 550, loss = 0.06214600\n",
      "Iteration 551, loss = 0.06180591\n",
      "Iteration 552, loss = 0.06157696\n",
      "Iteration 553, loss = 0.06137455\n",
      "Iteration 554, loss = 0.06120844\n",
      "Iteration 555, loss = 0.06111562\n",
      "Iteration 556, loss = 0.06110486\n",
      "Iteration 557, loss = 0.06104672\n",
      "Iteration 558, loss = 0.06088971\n",
      "Iteration 559, loss = 0.06067498\n",
      "Iteration 560, loss = 0.06044714\n",
      "Iteration 561, loss = 0.06020599\n",
      "Iteration 562, loss = 0.06017631\n",
      "Iteration 563, loss = 0.06004625\n",
      "Iteration 564, loss = 0.05976506\n",
      "Iteration 565, loss = 0.05927759\n",
      "Iteration 566, loss = 0.05893713\n",
      "Iteration 567, loss = 0.05865064\n",
      "Iteration 568, loss = 0.05859480\n",
      "Iteration 569, loss = 0.05857630\n",
      "Iteration 570, loss = 0.05857166\n",
      "Iteration 571, loss = 0.05837225\n",
      "Iteration 572, loss = 0.05816263\n",
      "Iteration 573, loss = 0.05805701\n",
      "Iteration 574, loss = 0.05817947\n",
      "Iteration 575, loss = 0.05830709\n",
      "Iteration 576, loss = 0.05796776\n",
      "Iteration 577, loss = 0.05751578\n",
      "Iteration 578, loss = 0.05719126\n",
      "Iteration 579, loss = 0.05710720\n",
      "Iteration 580, loss = 0.05698621\n",
      "Iteration 581, loss = 0.05690802\n",
      "Iteration 582, loss = 0.05677459\n",
      "Iteration 583, loss = 0.05673074\n",
      "Iteration 584, loss = 0.05644682\n",
      "Iteration 585, loss = 0.05596163\n",
      "Iteration 586, loss = 0.05550992\n",
      "Iteration 587, loss = 0.05518098\n",
      "Iteration 588, loss = 0.05511753\n",
      "Iteration 589, loss = 0.05513255\n",
      "Iteration 590, loss = 0.05502350\n",
      "Iteration 591, loss = 0.05479737\n",
      "Iteration 592, loss = 0.05470022\n",
      "Iteration 593, loss = 0.05463050\n",
      "Iteration 594, loss = 0.05465658\n",
      "Iteration 595, loss = 0.05466163\n",
      "Iteration 596, loss = 0.05464020\n",
      "Iteration 597, loss = 0.05459942\n",
      "Iteration 598, loss = 0.05460028\n",
      "Iteration 599, loss = 0.05466898\n",
      "Iteration 600, loss = 0.05483568\n",
      "Iteration 601, loss = 0.05468215\n",
      "Iteration 602, loss = 0.05432046\n",
      "Iteration 603, loss = 0.05380686\n",
      "Iteration 604, loss = 0.05338089\n",
      "Iteration 605, loss = 0.05290431\n",
      "Iteration 606, loss = 0.05259990\n",
      "Iteration 607, loss = 0.05236788\n",
      "Iteration 608, loss = 0.05213785\n",
      "Iteration 609, loss = 0.05227626\n",
      "Iteration 610, loss = 0.05235153\n",
      "Iteration 611, loss = 0.05226706\n",
      "Iteration 612, loss = 0.05211008\n",
      "Iteration 613, loss = 0.05192723\n",
      "Iteration 614, loss = 0.05158244\n",
      "Iteration 615, loss = 0.05139395\n",
      "Iteration 616, loss = 0.05117460\n",
      "Iteration 617, loss = 0.05106934\n",
      "Iteration 618, loss = 0.05083057\n",
      "Iteration 619, loss = 0.05063429\n",
      "Iteration 620, loss = 0.05043591\n",
      "Iteration 621, loss = 0.05028547\n",
      "Iteration 622, loss = 0.05015378\n",
      "Iteration 623, loss = 0.05006766\n",
      "Iteration 624, loss = 0.04995009\n",
      "Iteration 625, loss = 0.05019233\n",
      "Iteration 626, loss = 0.05049246\n",
      "Iteration 627, loss = 0.05105269\n",
      "Iteration 628, loss = 0.05116373\n",
      "Iteration 629, loss = 0.05095630\n",
      "Iteration 630, loss = 0.05054536\n",
      "Iteration 631, loss = 0.04958962\n",
      "Iteration 632, loss = 0.04885800\n",
      "Iteration 633, loss = 0.04875635\n",
      "Iteration 634, loss = 0.04901049\n",
      "Iteration 635, loss = 0.04897845\n",
      "Iteration 636, loss = 0.04894793\n",
      "Iteration 637, loss = 0.04864241\n",
      "Iteration 638, loss = 0.04824138\n",
      "Iteration 639, loss = 0.04767061\n",
      "Iteration 640, loss = 0.04740993\n",
      "Iteration 641, loss = 0.04727462\n",
      "Iteration 642, loss = 0.04713383\n",
      "Iteration 643, loss = 0.04697966\n",
      "Iteration 644, loss = 0.04685890\n",
      "Iteration 645, loss = 0.04674731\n",
      "Iteration 646, loss = 0.04660369\n",
      "Iteration 647, loss = 0.04645523\n",
      "Iteration 648, loss = 0.04636530\n",
      "Iteration 649, loss = 0.04617879\n",
      "Iteration 650, loss = 0.04603434\n",
      "Iteration 651, loss = 0.04605384\n",
      "Iteration 652, loss = 0.04620028\n",
      "Iteration 653, loss = 0.04623686\n",
      "Iteration 654, loss = 0.04621857\n",
      "Iteration 655, loss = 0.04642833\n",
      "Iteration 656, loss = 0.04659298\n",
      "Iteration 657, loss = 0.04622931\n",
      "Iteration 658, loss = 0.04568394\n",
      "Iteration 659, loss = 0.04529842\n",
      "Iteration 660, loss = 0.04499681\n",
      "Iteration 661, loss = 0.04496057\n",
      "Iteration 662, loss = 0.04502156\n",
      "Iteration 663, loss = 0.04515475\n",
      "Iteration 664, loss = 0.04524307\n",
      "Iteration 665, loss = 0.04528130\n",
      "Iteration 666, loss = 0.04535125\n",
      "Iteration 667, loss = 0.04546643\n",
      "Iteration 668, loss = 0.04528878\n",
      "Iteration 669, loss = 0.04490589\n",
      "Iteration 670, loss = 0.04428351\n",
      "Iteration 671, loss = 0.04390654\n",
      "Iteration 672, loss = 0.04357348\n",
      "Iteration 673, loss = 0.04339039\n",
      "Iteration 674, loss = 0.04319672\n",
      "Iteration 675, loss = 0.04324044\n",
      "Iteration 676, loss = 0.04314771\n",
      "Iteration 677, loss = 0.04295877\n",
      "Iteration 678, loss = 0.04274303\n",
      "Iteration 679, loss = 0.04256777\n",
      "Iteration 680, loss = 0.04247203\n",
      "Iteration 681, loss = 0.04236197\n",
      "Iteration 682, loss = 0.04223866\n",
      "Iteration 683, loss = 0.04202000\n",
      "Iteration 684, loss = 0.04184512\n",
      "Iteration 685, loss = 0.04172295\n",
      "Iteration 686, loss = 0.04184448\n",
      "Iteration 687, loss = 0.04217744\n",
      "Iteration 688, loss = 0.04227039\n",
      "Iteration 689, loss = 0.04226120\n",
      "Iteration 690, loss = 0.04215855\n",
      "Iteration 691, loss = 0.04199818\n",
      "Iteration 692, loss = 0.04186789\n",
      "Iteration 693, loss = 0.04168598\n",
      "Iteration 694, loss = 0.04156128\n",
      "Iteration 695, loss = 0.04132302\n",
      "Iteration 696, loss = 0.04114395\n",
      "Iteration 697, loss = 0.04102335\n",
      "Iteration 698, loss = 0.04081465\n",
      "Iteration 699, loss = 0.04059017\n",
      "Iteration 700, loss = 0.04033072\n",
      "Iteration 701, loss = 0.04005627\n",
      "Iteration 702, loss = 0.03985917\n",
      "Iteration 703, loss = 0.03971680\n",
      "Iteration 704, loss = 0.03952466\n",
      "Iteration 705, loss = 0.03931233\n",
      "Iteration 706, loss = 0.03921046\n",
      "Iteration 707, loss = 0.03906449\n",
      "Iteration 708, loss = 0.03894093\n",
      "Iteration 709, loss = 0.03877641\n",
      "Iteration 710, loss = 0.03869078\n",
      "Iteration 711, loss = 0.03862305\n",
      "Iteration 712, loss = 0.03856087\n",
      "Iteration 713, loss = 0.03862065\n",
      "Iteration 714, loss = 0.03861225\n",
      "Iteration 715, loss = 0.03851613\n",
      "Iteration 716, loss = 0.03835299\n",
      "Iteration 717, loss = 0.03813998\n",
      "Iteration 718, loss = 0.03807048\n",
      "Iteration 719, loss = 0.03816919\n",
      "Iteration 720, loss = 0.03816436\n",
      "Iteration 721, loss = 0.03795466\n",
      "Iteration 722, loss = 0.03767115\n",
      "Iteration 723, loss = 0.03748764\n",
      "Iteration 724, loss = 0.03730320\n",
      "Iteration 725, loss = 0.03715910\n",
      "Iteration 726, loss = 0.03697076\n",
      "Iteration 727, loss = 0.03685016\n",
      "Iteration 728, loss = 0.03670706\n",
      "Iteration 729, loss = 0.03655905\n",
      "Iteration 730, loss = 0.03644846\n",
      "Iteration 731, loss = 0.03638532\n",
      "Iteration 732, loss = 0.03625965\n",
      "Iteration 733, loss = 0.03624855\n",
      "Iteration 734, loss = 0.03612626\n",
      "Iteration 735, loss = 0.03600607\n",
      "Iteration 736, loss = 0.03593389\n",
      "Iteration 737, loss = 0.03579743\n",
      "Iteration 738, loss = 0.03567607\n",
      "Iteration 739, loss = 0.03555300\n",
      "Iteration 740, loss = 0.03541833\n",
      "Iteration 741, loss = 0.03526989\n",
      "Iteration 742, loss = 0.03514247\n",
      "Iteration 743, loss = 0.03513881\n",
      "Iteration 744, loss = 0.03509849\n",
      "Iteration 745, loss = 0.03522530\n",
      "Iteration 746, loss = 0.03513675\n",
      "Iteration 747, loss = 0.03512811\n",
      "Iteration 748, loss = 0.03506511\n",
      "Iteration 749, loss = 0.03496612\n",
      "Iteration 750, loss = 0.03488607\n",
      "Iteration 751, loss = 0.03474963\n",
      "Iteration 752, loss = 0.03462611\n",
      "Iteration 753, loss = 0.03448515\n",
      "Iteration 754, loss = 0.03435043\n",
      "Iteration 755, loss = 0.03406774\n",
      "Iteration 756, loss = 0.03388818\n",
      "Iteration 757, loss = 0.03376715\n",
      "Iteration 758, loss = 0.03363196\n",
      "Iteration 759, loss = 0.03361617\n",
      "Iteration 760, loss = 0.03362088\n",
      "Iteration 761, loss = 0.03377951\n",
      "Iteration 762, loss = 0.03368150\n",
      "Iteration 763, loss = 0.03354242\n",
      "Iteration 764, loss = 0.03334585\n",
      "Iteration 765, loss = 0.03321499\n",
      "Iteration 766, loss = 0.03307140\n",
      "Iteration 767, loss = 0.03311143\n",
      "Iteration 768, loss = 0.03300273\n",
      "Iteration 769, loss = 0.03284306\n",
      "Iteration 770, loss = 0.03276367\n",
      "Iteration 771, loss = 0.03269074\n",
      "Iteration 772, loss = 0.03267578\n",
      "Iteration 773, loss = 0.03266331\n",
      "Iteration 774, loss = 0.03260290\n",
      "Iteration 775, loss = 0.03247770\n",
      "Iteration 776, loss = 0.03227791\n",
      "Iteration 777, loss = 0.03213923\n",
      "Iteration 778, loss = 0.03205478\n",
      "Iteration 779, loss = 0.03197080\n",
      "Iteration 780, loss = 0.03185407\n",
      "Iteration 781, loss = 0.03177058\n",
      "Iteration 782, loss = 0.03166131\n",
      "Iteration 783, loss = 0.03154808\n",
      "Iteration 784, loss = 0.03144342\n",
      "Iteration 785, loss = 0.03132548\n",
      "Iteration 786, loss = 0.03126067\n",
      "Iteration 787, loss = 0.03119262\n",
      "Iteration 788, loss = 0.03111841\n",
      "Iteration 789, loss = 0.03103145\n",
      "Iteration 790, loss = 0.03095057\n",
      "Iteration 791, loss = 0.03083838\n",
      "Iteration 792, loss = 0.03074973\n",
      "Iteration 793, loss = 0.03066142\n",
      "Iteration 794, loss = 0.03057554\n",
      "Iteration 795, loss = 0.03051283\n",
      "Iteration 796, loss = 0.03046081\n",
      "Iteration 797, loss = 0.03052566\n",
      "Iteration 798, loss = 0.03051693\n",
      "Iteration 799, loss = 0.03058177\n",
      "Iteration 800, loss = 0.03051844\n",
      "Iteration 801, loss = 0.03042832\n",
      "Iteration 802, loss = 0.03027560\n",
      "Iteration 803, loss = 0.03021300\n",
      "Iteration 804, loss = 0.03008645\n",
      "Iteration 805, loss = 0.02998064\n",
      "Iteration 806, loss = 0.02985887\n",
      "Iteration 807, loss = 0.02978865\n",
      "Iteration 808, loss = 0.02971126\n",
      "Iteration 809, loss = 0.02956597\n",
      "Iteration 810, loss = 0.02951886\n",
      "Iteration 811, loss = 0.02952086\n",
      "Iteration 812, loss = 0.02952813\n",
      "Iteration 813, loss = 0.02949032\n",
      "Iteration 814, loss = 0.02942754\n",
      "Iteration 815, loss = 0.02932920\n",
      "Iteration 816, loss = 0.02898218\n",
      "Iteration 817, loss = 0.02875464\n",
      "Iteration 818, loss = 0.02871642\n",
      "Iteration 819, loss = 0.02879272\n",
      "Iteration 820, loss = 0.02894659\n",
      "Iteration 821, loss = 0.02888361\n",
      "Iteration 822, loss = 0.02878321\n",
      "Iteration 823, loss = 0.02869417\n",
      "Iteration 824, loss = 0.02848743\n",
      "Iteration 825, loss = 0.02825144\n",
      "Iteration 826, loss = 0.02812589\n",
      "Iteration 827, loss = 0.02811444\n",
      "Iteration 828, loss = 0.02807304\n",
      "Iteration 829, loss = 0.02798725\n",
      "Iteration 830, loss = 0.02792310\n",
      "Iteration 831, loss = 0.02775811\n",
      "Iteration 832, loss = 0.02772670\n",
      "Iteration 833, loss = 0.02771631\n",
      "Iteration 834, loss = 0.02770251\n",
      "Iteration 835, loss = 0.02759686\n",
      "Iteration 836, loss = 0.02745377\n",
      "Iteration 837, loss = 0.02742972\n",
      "Iteration 838, loss = 0.02741987\n",
      "Iteration 839, loss = 0.02738887\n",
      "Iteration 840, loss = 0.02729115\n",
      "Iteration 841, loss = 0.02718679\n",
      "Iteration 842, loss = 0.02703417\n",
      "Iteration 843, loss = 0.02692454\n",
      "Iteration 844, loss = 0.02683217\n",
      "Iteration 845, loss = 0.02673630\n",
      "Iteration 846, loss = 0.02680417\n",
      "Iteration 847, loss = 0.02673853\n",
      "Iteration 848, loss = 0.02667294\n",
      "Iteration 849, loss = 0.02650484\n",
      "Iteration 850, loss = 0.02634257\n",
      "Iteration 851, loss = 0.02636419\n",
      "Iteration 852, loss = 0.02626357\n",
      "Iteration 853, loss = 0.02613699\n",
      "Iteration 854, loss = 0.02603660\n",
      "Iteration 855, loss = 0.02595528\n",
      "Iteration 856, loss = 0.02586569\n",
      "Iteration 857, loss = 0.02579750\n",
      "Iteration 858, loss = 0.02574737\n",
      "Iteration 859, loss = 0.02570530\n",
      "Iteration 860, loss = 0.02567519\n",
      "Iteration 861, loss = 0.02570289\n",
      "Iteration 862, loss = 0.02572773\n",
      "Iteration 863, loss = 0.02579088\n",
      "Iteration 864, loss = 0.02579535\n",
      "Iteration 865, loss = 0.02573122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier7 = MLPClassifier(max_iter=2000,activation='relu', verbose=True, hidden_layer_sizes=(200,),random_state=1,learning_rate_init=0.001)\n",
    "mlpClassifier7.fit(X_train, y_train)\n",
    "y_pred7 = mlpClassifier7.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bd64de8",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88cccb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56989247311827"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e403853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde36703940>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 7')\n",
    "plt.plot(range(len(mlpClassifier7.loss_curve_)),mlpClassifier7.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3049bed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix7 = metrics.confusion_matrix(y_test,y_pred7)\n",
    "sns.heatmap(pd.DataFrame(confMatrix7),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81f0acbd",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c64e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63837118\n",
      "Iteration 2, loss = 0.61485584\n",
      "Iteration 3, loss = 0.59497575\n",
      "Iteration 4, loss = 0.57803542\n",
      "Iteration 5, loss = 0.56274258\n",
      "Iteration 6, loss = 0.54978820\n",
      "Iteration 7, loss = 0.53810396\n",
      "Iteration 8, loss = 0.52796557\n",
      "Iteration 9, loss = 0.51869406\n",
      "Iteration 10, loss = 0.51085741\n",
      "Iteration 11, loss = 0.50326898\n",
      "Iteration 12, loss = 0.49699277\n",
      "Iteration 13, loss = 0.49121515\n",
      "Iteration 14, loss = 0.48571616\n",
      "Iteration 15, loss = 0.48095659\n",
      "Iteration 16, loss = 0.47601943\n",
      "Iteration 17, loss = 0.47152075\n",
      "Iteration 18, loss = 0.46690501\n",
      "Iteration 19, loss = 0.46223107\n",
      "Iteration 20, loss = 0.45757267\n",
      "Iteration 21, loss = 0.45317530\n",
      "Iteration 22, loss = 0.44876030\n",
      "Iteration 23, loss = 0.44461829\n",
      "Iteration 24, loss = 0.44063107\n",
      "Iteration 25, loss = 0.43656671\n",
      "Iteration 26, loss = 0.43287694\n",
      "Iteration 27, loss = 0.42951171\n",
      "Iteration 28, loss = 0.42600997\n",
      "Iteration 29, loss = 0.42297505\n",
      "Iteration 30, loss = 0.42038960\n",
      "Iteration 31, loss = 0.41749683\n",
      "Iteration 32, loss = 0.41463263\n",
      "Iteration 33, loss = 0.41182889\n",
      "Iteration 34, loss = 0.40883069\n",
      "Iteration 35, loss = 0.40573625\n",
      "Iteration 36, loss = 0.40260144\n",
      "Iteration 37, loss = 0.39972128\n",
      "Iteration 38, loss = 0.39691889\n",
      "Iteration 39, loss = 0.39383087\n",
      "Iteration 40, loss = 0.39076203\n",
      "Iteration 41, loss = 0.38761891\n",
      "Iteration 42, loss = 0.38473826\n",
      "Iteration 43, loss = 0.38185499\n",
      "Iteration 44, loss = 0.37915177\n",
      "Iteration 45, loss = 0.37647897\n",
      "Iteration 46, loss = 0.37399295\n",
      "Iteration 47, loss = 0.37140080\n",
      "Iteration 48, loss = 0.36884047\n",
      "Iteration 49, loss = 0.36641080\n",
      "Iteration 50, loss = 0.36411334\n",
      "Iteration 51, loss = 0.36193315\n",
      "Iteration 52, loss = 0.35953565\n",
      "Iteration 53, loss = 0.35745664\n",
      "Iteration 54, loss = 0.35578999\n",
      "Iteration 55, loss = 0.35364320\n",
      "Iteration 56, loss = 0.35152241\n",
      "Iteration 57, loss = 0.34928765\n",
      "Iteration 58, loss = 0.34663205\n",
      "Iteration 59, loss = 0.34403799\n",
      "Iteration 60, loss = 0.34131950\n",
      "Iteration 61, loss = 0.33882031\n",
      "Iteration 62, loss = 0.33661328\n",
      "Iteration 63, loss = 0.33437729\n",
      "Iteration 64, loss = 0.33221762\n",
      "Iteration 65, loss = 0.33018670\n",
      "Iteration 66, loss = 0.32830030\n",
      "Iteration 67, loss = 0.32636086\n",
      "Iteration 68, loss = 0.32453965\n",
      "Iteration 69, loss = 0.32271811\n",
      "Iteration 70, loss = 0.32082771\n",
      "Iteration 71, loss = 0.31904043\n",
      "Iteration 72, loss = 0.31722466\n",
      "Iteration 73, loss = 0.31543196\n",
      "Iteration 74, loss = 0.31354161\n",
      "Iteration 75, loss = 0.31162899\n",
      "Iteration 76, loss = 0.30971850\n",
      "Iteration 77, loss = 0.30775799\n",
      "Iteration 78, loss = 0.30573367\n",
      "Iteration 79, loss = 0.30405070\n",
      "Iteration 80, loss = 0.30215154\n",
      "Iteration 81, loss = 0.30053396\n",
      "Iteration 82, loss = 0.29889707\n",
      "Iteration 83, loss = 0.29688561\n",
      "Iteration 84, loss = 0.29495107\n",
      "Iteration 85, loss = 0.29281532\n",
      "Iteration 86, loss = 0.29085711\n",
      "Iteration 87, loss = 0.28894406\n",
      "Iteration 88, loss = 0.28723778\n",
      "Iteration 89, loss = 0.28576991\n",
      "Iteration 90, loss = 0.28477408\n",
      "Iteration 91, loss = 0.28348259\n",
      "Iteration 92, loss = 0.28218014\n",
      "Iteration 93, loss = 0.28065135\n",
      "Iteration 94, loss = 0.27916946\n",
      "Iteration 95, loss = 0.27764211\n",
      "Iteration 96, loss = 0.27633817\n",
      "Iteration 97, loss = 0.27461352\n",
      "Iteration 98, loss = 0.27339812\n",
      "Iteration 99, loss = 0.27196193\n",
      "Iteration 100, loss = 0.27113475\n",
      "Iteration 101, loss = 0.26975882\n",
      "Iteration 102, loss = 0.26872652\n",
      "Iteration 103, loss = 0.26737732\n",
      "Iteration 104, loss = 0.26602899\n",
      "Iteration 105, loss = 0.26422341\n",
      "Iteration 106, loss = 0.26203728\n",
      "Iteration 107, loss = 0.26071053\n",
      "Iteration 108, loss = 0.25990081\n",
      "Iteration 109, loss = 0.25969182\n",
      "Iteration 110, loss = 0.25939583\n",
      "Iteration 111, loss = 0.25900400\n",
      "Iteration 112, loss = 0.25790297\n",
      "Iteration 113, loss = 0.25671754\n",
      "Iteration 114, loss = 0.25549178\n",
      "Iteration 115, loss = 0.25379382\n",
      "Iteration 116, loss = 0.25181705\n",
      "Iteration 117, loss = 0.24983206\n",
      "Iteration 118, loss = 0.24753588\n",
      "Iteration 119, loss = 0.24599062\n",
      "Iteration 120, loss = 0.24450114\n",
      "Iteration 121, loss = 0.24344226\n",
      "Iteration 122, loss = 0.24275976\n",
      "Iteration 123, loss = 0.24205664\n",
      "Iteration 124, loss = 0.24168379\n",
      "Iteration 125, loss = 0.24101028\n",
      "Iteration 126, loss = 0.23982819\n",
      "Iteration 127, loss = 0.23873598\n",
      "Iteration 128, loss = 0.23752856\n",
      "Iteration 129, loss = 0.23645795\n",
      "Iteration 130, loss = 0.23535778\n",
      "Iteration 131, loss = 0.23422613\n",
      "Iteration 132, loss = 0.23308288\n",
      "Iteration 133, loss = 0.23173472\n",
      "Iteration 134, loss = 0.23003522\n",
      "Iteration 135, loss = 0.22882122\n",
      "Iteration 136, loss = 0.22757840\n",
      "Iteration 137, loss = 0.22691900\n",
      "Iteration 138, loss = 0.22601453\n",
      "Iteration 139, loss = 0.22521126\n",
      "Iteration 140, loss = 0.22427024\n",
      "Iteration 141, loss = 0.22327160\n",
      "Iteration 142, loss = 0.22231814\n",
      "Iteration 143, loss = 0.22133592\n",
      "Iteration 144, loss = 0.22046373\n",
      "Iteration 145, loss = 0.22019008\n",
      "Iteration 146, loss = 0.22016362\n",
      "Iteration 147, loss = 0.21943343\n",
      "Iteration 148, loss = 0.21861822\n",
      "Iteration 149, loss = 0.21756926\n",
      "Iteration 150, loss = 0.21678330\n",
      "Iteration 151, loss = 0.21567580\n",
      "Iteration 152, loss = 0.21493955\n",
      "Iteration 153, loss = 0.21416861\n",
      "Iteration 154, loss = 0.21355778\n",
      "Iteration 155, loss = 0.21279314\n",
      "Iteration 156, loss = 0.21176946\n",
      "Iteration 157, loss = 0.21095556\n",
      "Iteration 158, loss = 0.21010195\n",
      "Iteration 159, loss = 0.20911517\n",
      "Iteration 160, loss = 0.20815652\n",
      "Iteration 161, loss = 0.20733879\n",
      "Iteration 162, loss = 0.20667479\n",
      "Iteration 163, loss = 0.20608147\n",
      "Iteration 164, loss = 0.20522109\n",
      "Iteration 165, loss = 0.20418887\n",
      "Iteration 166, loss = 0.20335799\n",
      "Iteration 167, loss = 0.20245956\n",
      "Iteration 168, loss = 0.20164195\n",
      "Iteration 169, loss = 0.20085688\n",
      "Iteration 170, loss = 0.20027193\n",
      "Iteration 171, loss = 0.19945171\n",
      "Iteration 172, loss = 0.19865541\n",
      "Iteration 173, loss = 0.19807010\n",
      "Iteration 174, loss = 0.19741980\n",
      "Iteration 175, loss = 0.19683950\n",
      "Iteration 176, loss = 0.19648748\n",
      "Iteration 177, loss = 0.19604985\n",
      "Iteration 178, loss = 0.19544403\n",
      "Iteration 179, loss = 0.19482734\n",
      "Iteration 180, loss = 0.19419285\n",
      "Iteration 181, loss = 0.19367290\n",
      "Iteration 182, loss = 0.19312518\n",
      "Iteration 183, loss = 0.19257055\n",
      "Iteration 184, loss = 0.19195399\n",
      "Iteration 185, loss = 0.19141265\n",
      "Iteration 186, loss = 0.19100201\n",
      "Iteration 187, loss = 0.19033200\n",
      "Iteration 188, loss = 0.18970842\n",
      "Iteration 189, loss = 0.18890827\n",
      "Iteration 190, loss = 0.18800722\n",
      "Iteration 191, loss = 0.18706076\n",
      "Iteration 192, loss = 0.18629161\n",
      "Iteration 193, loss = 0.18570302\n",
      "Iteration 194, loss = 0.18501385\n",
      "Iteration 195, loss = 0.18447546\n",
      "Iteration 196, loss = 0.18389193\n",
      "Iteration 197, loss = 0.18329934\n",
      "Iteration 198, loss = 0.18280180\n",
      "Iteration 199, loss = 0.18282278\n",
      "Iteration 200, loss = 0.18230436\n",
      "Iteration 201, loss = 0.18137618\n",
      "Iteration 202, loss = 0.18035707\n",
      "Iteration 203, loss = 0.17941706\n",
      "Iteration 204, loss = 0.17845274\n",
      "Iteration 205, loss = 0.17769847\n",
      "Iteration 206, loss = 0.17719356\n",
      "Iteration 207, loss = 0.17640228\n",
      "Iteration 208, loss = 0.17571752\n",
      "Iteration 209, loss = 0.17521561\n",
      "Iteration 210, loss = 0.17454499\n",
      "Iteration 211, loss = 0.17382962\n",
      "Iteration 212, loss = 0.17322395\n",
      "Iteration 213, loss = 0.17280607\n",
      "Iteration 214, loss = 0.17249093\n",
      "Iteration 215, loss = 0.17225148\n",
      "Iteration 216, loss = 0.17215259\n",
      "Iteration 217, loss = 0.17183528\n",
      "Iteration 218, loss = 0.17117850\n",
      "Iteration 219, loss = 0.17038006\n",
      "Iteration 220, loss = 0.16969816\n",
      "Iteration 221, loss = 0.16906800\n",
      "Iteration 222, loss = 0.16826073\n",
      "Iteration 223, loss = 0.16758469\n",
      "Iteration 224, loss = 0.16708670\n",
      "Iteration 225, loss = 0.16650927\n",
      "Iteration 226, loss = 0.16614059\n",
      "Iteration 227, loss = 0.16568293\n",
      "Iteration 228, loss = 0.16528600\n",
      "Iteration 229, loss = 0.16457543\n",
      "Iteration 230, loss = 0.16403037\n",
      "Iteration 231, loss = 0.16333717\n",
      "Iteration 232, loss = 0.16259335\n",
      "Iteration 233, loss = 0.16194779\n",
      "Iteration 234, loss = 0.16128990\n",
      "Iteration 235, loss = 0.16091692\n",
      "Iteration 236, loss = 0.16040983\n",
      "Iteration 237, loss = 0.16019260\n",
      "Iteration 238, loss = 0.15977858\n",
      "Iteration 239, loss = 0.15919252\n",
      "Iteration 240, loss = 0.15851469\n",
      "Iteration 241, loss = 0.15792193\n",
      "Iteration 242, loss = 0.15740308\n",
      "Iteration 243, loss = 0.15701764\n",
      "Iteration 244, loss = 0.15651539\n",
      "Iteration 245, loss = 0.15588903\n",
      "Iteration 246, loss = 0.15538929\n",
      "Iteration 247, loss = 0.15514152\n",
      "Iteration 248, loss = 0.15457999\n",
      "Iteration 249, loss = 0.15412814\n",
      "Iteration 250, loss = 0.15367145\n",
      "Iteration 251, loss = 0.15315496\n",
      "Iteration 252, loss = 0.15265747\n",
      "Iteration 253, loss = 0.15210227\n",
      "Iteration 254, loss = 0.15170287\n",
      "Iteration 255, loss = 0.15120397\n",
      "Iteration 256, loss = 0.15074317\n",
      "Iteration 257, loss = 0.15027606\n",
      "Iteration 258, loss = 0.14981092\n",
      "Iteration 259, loss = 0.14941084\n",
      "Iteration 260, loss = 0.14874232\n",
      "Iteration 261, loss = 0.14823281\n",
      "Iteration 262, loss = 0.14793821\n",
      "Iteration 263, loss = 0.14745289\n",
      "Iteration 264, loss = 0.14685687\n",
      "Iteration 265, loss = 0.14646597\n",
      "Iteration 266, loss = 0.14608698\n",
      "Iteration 267, loss = 0.14548348\n",
      "Iteration 268, loss = 0.14499678\n",
      "Iteration 269, loss = 0.14476461\n",
      "Iteration 270, loss = 0.14494506\n",
      "Iteration 271, loss = 0.14441380\n",
      "Iteration 272, loss = 0.14422361\n",
      "Iteration 273, loss = 0.14379799\n",
      "Iteration 274, loss = 0.14387659\n",
      "Iteration 275, loss = 0.14405343\n",
      "Iteration 276, loss = 0.14413308\n",
      "Iteration 277, loss = 0.14413793\n",
      "Iteration 278, loss = 0.14409706\n",
      "Iteration 279, loss = 0.14344545\n",
      "Iteration 280, loss = 0.14245531\n",
      "Iteration 281, loss = 0.14141601\n",
      "Iteration 282, loss = 0.14070506\n",
      "Iteration 283, loss = 0.14010020\n",
      "Iteration 284, loss = 0.13969708\n",
      "Iteration 285, loss = 0.13936683\n",
      "Iteration 286, loss = 0.13910689\n",
      "Iteration 287, loss = 0.13892670\n",
      "Iteration 288, loss = 0.13866213\n",
      "Iteration 289, loss = 0.13841552\n",
      "Iteration 290, loss = 0.13828258\n",
      "Iteration 291, loss = 0.13825616\n",
      "Iteration 292, loss = 0.13827392\n",
      "Iteration 293, loss = 0.13810248\n",
      "Iteration 294, loss = 0.13799508\n",
      "Iteration 295, loss = 0.13772581\n",
      "Iteration 296, loss = 0.13696777\n",
      "Iteration 297, loss = 0.13589115\n",
      "Iteration 298, loss = 0.13508538\n",
      "Iteration 299, loss = 0.13430889\n",
      "Iteration 300, loss = 0.13345051\n",
      "Iteration 301, loss = 0.13287611\n",
      "Iteration 302, loss = 0.13273943\n",
      "Iteration 303, loss = 0.13260252\n",
      "Iteration 304, loss = 0.13261628\n",
      "Iteration 305, loss = 0.13220525\n",
      "Iteration 306, loss = 0.13192194\n",
      "Iteration 307, loss = 0.13184057\n",
      "Iteration 308, loss = 0.13152175\n",
      "Iteration 309, loss = 0.13109512\n",
      "Iteration 310, loss = 0.13067279\n",
      "Iteration 311, loss = 0.13043643\n",
      "Iteration 312, loss = 0.12989619\n",
      "Iteration 313, loss = 0.12926700\n",
      "Iteration 314, loss = 0.12906092\n",
      "Iteration 315, loss = 0.12920686\n",
      "Iteration 316, loss = 0.12913645\n",
      "Iteration 317, loss = 0.12873635\n",
      "Iteration 318, loss = 0.12775245\n",
      "Iteration 319, loss = 0.12654198\n",
      "Iteration 320, loss = 0.12577856\n",
      "Iteration 321, loss = 0.12499287\n",
      "Iteration 322, loss = 0.12442453\n",
      "Iteration 323, loss = 0.12394730\n",
      "Iteration 324, loss = 0.12367651\n",
      "Iteration 325, loss = 0.12343271\n",
      "Iteration 326, loss = 0.12299677\n",
      "Iteration 327, loss = 0.12247194\n",
      "Iteration 328, loss = 0.12195463\n",
      "Iteration 329, loss = 0.12125212\n",
      "Iteration 330, loss = 0.12080690\n",
      "Iteration 331, loss = 0.12043271\n",
      "Iteration 332, loss = 0.12006934\n",
      "Iteration 333, loss = 0.11976361\n",
      "Iteration 334, loss = 0.11946161\n",
      "Iteration 335, loss = 0.11908855\n",
      "Iteration 336, loss = 0.11864407\n",
      "Iteration 337, loss = 0.11813558\n",
      "Iteration 338, loss = 0.11767134\n",
      "Iteration 339, loss = 0.11720750\n",
      "Iteration 340, loss = 0.11684785\n",
      "Iteration 341, loss = 0.11651759\n",
      "Iteration 342, loss = 0.11630254\n",
      "Iteration 343, loss = 0.11599071\n",
      "Iteration 344, loss = 0.11585734\n",
      "Iteration 345, loss = 0.11565263\n",
      "Iteration 346, loss = 0.11524658\n",
      "Iteration 347, loss = 0.11475457\n",
      "Iteration 348, loss = 0.11424513\n",
      "Iteration 349, loss = 0.11386539\n",
      "Iteration 350, loss = 0.11382849\n",
      "Iteration 351, loss = 0.11427540\n",
      "Iteration 352, loss = 0.11384563\n",
      "Iteration 353, loss = 0.11339809\n",
      "Iteration 354, loss = 0.11287480\n",
      "Iteration 355, loss = 0.11239088\n",
      "Iteration 356, loss = 0.11204001\n",
      "Iteration 357, loss = 0.11145822\n",
      "Iteration 358, loss = 0.11112252\n",
      "Iteration 359, loss = 0.11059224\n",
      "Iteration 360, loss = 0.10997739\n",
      "Iteration 361, loss = 0.10970055\n",
      "Iteration 362, loss = 0.10919736\n",
      "Iteration 363, loss = 0.10899201\n",
      "Iteration 364, loss = 0.10890417\n",
      "Iteration 365, loss = 0.10886891\n",
      "Iteration 366, loss = 0.10884484\n",
      "Iteration 367, loss = 0.10830465\n",
      "Iteration 368, loss = 0.10798682\n",
      "Iteration 369, loss = 0.10787054\n",
      "Iteration 370, loss = 0.10788231\n",
      "Iteration 371, loss = 0.10737477\n",
      "Iteration 372, loss = 0.10679341\n",
      "Iteration 373, loss = 0.10631118\n",
      "Iteration 374, loss = 0.10604351\n",
      "Iteration 375, loss = 0.10576508\n",
      "Iteration 376, loss = 0.10558342\n",
      "Iteration 377, loss = 0.10547879\n",
      "Iteration 378, loss = 0.10513021\n",
      "Iteration 379, loss = 0.10469019\n",
      "Iteration 380, loss = 0.10504871\n",
      "Iteration 381, loss = 0.10494584\n",
      "Iteration 382, loss = 0.10463451\n",
      "Iteration 383, loss = 0.10370016\n",
      "Iteration 384, loss = 0.10282861\n",
      "Iteration 385, loss = 0.10225825\n",
      "Iteration 386, loss = 0.10209527\n",
      "Iteration 387, loss = 0.10204818\n",
      "Iteration 388, loss = 0.10210084\n",
      "Iteration 389, loss = 0.10211001\n",
      "Iteration 390, loss = 0.10196624\n",
      "Iteration 391, loss = 0.10173091\n",
      "Iteration 392, loss = 0.10122789\n",
      "Iteration 393, loss = 0.10051486\n",
      "Iteration 394, loss = 0.10022701\n",
      "Iteration 395, loss = 0.10014401\n",
      "Iteration 396, loss = 0.10000112\n",
      "Iteration 397, loss = 0.09967403\n",
      "Iteration 398, loss = 0.09924086\n",
      "Iteration 399, loss = 0.09871047\n",
      "Iteration 400, loss = 0.09819770\n",
      "Iteration 401, loss = 0.09766259\n",
      "Iteration 402, loss = 0.09721655\n",
      "Iteration 403, loss = 0.09681710\n",
      "Iteration 404, loss = 0.09651945\n",
      "Iteration 405, loss = 0.09612390\n",
      "Iteration 406, loss = 0.09595208\n",
      "Iteration 407, loss = 0.09589089\n",
      "Iteration 408, loss = 0.09577798\n",
      "Iteration 409, loss = 0.09556527\n",
      "Iteration 410, loss = 0.09523575\n",
      "Iteration 411, loss = 0.09486260\n",
      "Iteration 412, loss = 0.09440269\n",
      "Iteration 413, loss = 0.09394651\n",
      "Iteration 414, loss = 0.09341508\n",
      "Iteration 415, loss = 0.09315000\n",
      "Iteration 416, loss = 0.09293927\n",
      "Iteration 417, loss = 0.09332207\n",
      "Iteration 418, loss = 0.09316954\n",
      "Iteration 419, loss = 0.09282474\n",
      "Iteration 420, loss = 0.09249573\n",
      "Iteration 421, loss = 0.09194915\n",
      "Iteration 422, loss = 0.09134055\n",
      "Iteration 423, loss = 0.09076354\n",
      "Iteration 424, loss = 0.09040197\n",
      "Iteration 425, loss = 0.09003539\n",
      "Iteration 426, loss = 0.08990837\n",
      "Iteration 427, loss = 0.08994569\n",
      "Iteration 428, loss = 0.08973092\n",
      "Iteration 429, loss = 0.08970261\n",
      "Iteration 430, loss = 0.08926804\n",
      "Iteration 431, loss = 0.08870564\n",
      "Iteration 432, loss = 0.08831311\n",
      "Iteration 433, loss = 0.08786833\n",
      "Iteration 434, loss = 0.08755169\n",
      "Iteration 435, loss = 0.08735746\n",
      "Iteration 436, loss = 0.08695394\n",
      "Iteration 437, loss = 0.08656250\n",
      "Iteration 438, loss = 0.08618281\n",
      "Iteration 439, loss = 0.08585041\n",
      "Iteration 440, loss = 0.08567906\n",
      "Iteration 441, loss = 0.08591244\n",
      "Iteration 442, loss = 0.08620144\n",
      "Iteration 443, loss = 0.08612729\n",
      "Iteration 444, loss = 0.08587643\n",
      "Iteration 445, loss = 0.08535299\n",
      "Iteration 446, loss = 0.08467477\n",
      "Iteration 447, loss = 0.08421074\n",
      "Iteration 448, loss = 0.08397249\n",
      "Iteration 449, loss = 0.08364354\n",
      "Iteration 450, loss = 0.08319166\n",
      "Iteration 451, loss = 0.08267917\n",
      "Iteration 452, loss = 0.08248504\n",
      "Iteration 453, loss = 0.08244878\n",
      "Iteration 454, loss = 0.08308931\n",
      "Iteration 455, loss = 0.08298010\n",
      "Iteration 456, loss = 0.08270934\n",
      "Iteration 457, loss = 0.08280454\n",
      "Iteration 458, loss = 0.08274161\n",
      "Iteration 459, loss = 0.08226879\n",
      "Iteration 460, loss = 0.08158332\n",
      "Iteration 461, loss = 0.08098492\n",
      "Iteration 462, loss = 0.08066466\n",
      "Iteration 463, loss = 0.08025093\n",
      "Iteration 464, loss = 0.08002014\n",
      "Iteration 465, loss = 0.07989798\n",
      "Iteration 466, loss = 0.07956612\n",
      "Iteration 467, loss = 0.07955646\n",
      "Iteration 468, loss = 0.07948069\n",
      "Iteration 469, loss = 0.07924200\n",
      "Iteration 470, loss = 0.07873638\n",
      "Iteration 471, loss = 0.07819159\n",
      "Iteration 472, loss = 0.07775301\n",
      "Iteration 473, loss = 0.07759485\n",
      "Iteration 474, loss = 0.07723822\n",
      "Iteration 475, loss = 0.07700437\n",
      "Iteration 476, loss = 0.07684756\n",
      "Iteration 477, loss = 0.07678269\n",
      "Iteration 478, loss = 0.07674192\n",
      "Iteration 479, loss = 0.07669612\n",
      "Iteration 480, loss = 0.07662816\n",
      "Iteration 481, loss = 0.07654275\n",
      "Iteration 482, loss = 0.07638517\n",
      "Iteration 483, loss = 0.07646727\n",
      "Iteration 484, loss = 0.07678411\n",
      "Iteration 485, loss = 0.07658124\n",
      "Iteration 486, loss = 0.07619955\n",
      "Iteration 487, loss = 0.07582087\n",
      "Iteration 488, loss = 0.07520496\n",
      "Iteration 489, loss = 0.07464239\n",
      "Iteration 490, loss = 0.07411759\n",
      "Iteration 491, loss = 0.07383687\n",
      "Iteration 492, loss = 0.07387653\n",
      "Iteration 493, loss = 0.07369026\n",
      "Iteration 494, loss = 0.07335358\n",
      "Iteration 495, loss = 0.07293077\n",
      "Iteration 496, loss = 0.07244126\n",
      "Iteration 497, loss = 0.07207962\n",
      "Iteration 498, loss = 0.07176695\n",
      "Iteration 499, loss = 0.07146457\n",
      "Iteration 500, loss = 0.07126749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier8 = MLPClassifier(max_iter=500,activation='relu', verbose=True, hidden_layer_sizes=(200,),random_state=1,learning_rate_init=0.001)\n",
    "mlpClassifier8.fit(X_train, y_train)\n",
    "y_pred8 = mlpClassifier8.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0afddad",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be5dced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.41935483870968"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a65cbd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde3693e4c0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAlEklEQVR4nO3deXhW5Z3/8fc3ewhJyAohIYQlIJus4oa7VbRWWutY7WJ1bJ1Oa5ep01ZnejkdO53pNm1tazs/a1u7qbVaK7Uq7lZRgYCIBAQCBEiAkAAhG0lI8v398TyhgbIEknDyPM/ndV3PRc45d8753hg+ub3PZu6OiIhEvrigCxARkf6hQBcRiRIKdBGRKKFAFxGJEgp0EZEooUAXEYkSCnQRkSihQJdAmVmlmV0a0LHnmtlTZlZvZnvMbKmZ3RxELYczs+vMbK2ZNZrZGjN7f9A1yeCnQJeYZGZnAy8CrwDjgRzgn4ErTnJ/8f1YWyHwW+CLQAbwJeBBM8vvr2NIdFKgy6BkZslm9gMz2x7+/MDMksPbcs3syR4j61fNLC687StmVh0e2a4zs0uOcojvAL9y92+5e52HLHf368L7ucnMXjusJjez8eGvHzCzn4ZH+M3Av5rZzp7BbmYfMLNV4a/jzOwOM9toZrvN7BEzyz5KbUVAvbs/Ha7rL0AzMO6k/0IlJijQZbD6d+AsYAYwHZgLfDW87XagCsgDhgP/BriZTQRuA85w93TgcqDy8B2b2RDgbODRPtb4YeAbQDpwD6HQvfiw7Q+Gv/4s8H7gAmAksBe49yj7LQPWmtnVZhYfnm5pA1b1sV6Jcgp0Gaw+Atzt7rvcvRb4T+Bj4W0HgAJgtLsfcPdXPfRQok4gGZhsZonuXunuG4+w7yxCP/s7+ljjE+6+2N273L0VeAi4AcDM0oErw+sAPgX8u7tXuXsb8DXgWjNLOHyn7t4J/JrQL4O28J//5O7NfaxXopwCXQarkcCWHstbwusgNF1SATxrZpvM7A4Ad68AvkAoLHeZ2cNmNpK/txfoIvRLoS+2Hbb8IHBNeGroGmCFu3f3YTTweHiaqB5YS+gX0PDDdxo+Sfxt4EIgidCo/n4zm9HHeiXKKdBlsNpOKAS7FYfX4e6N7n67u48Frga+2D1X7u4Puvu88Pc68K3Dd+zuLcAbwAePcfxmYEj3gpmNOEKbQx5V6u5rCP3iuYJDp1sgFP5XuPuwHp8Ud68+wn5nAH9197Lw6H8ZsAQI5GogiRwKdBkMEs0spccngdBUxVfNLM/McoG7CF35gZldZWbjzcyAfYRGul1mNtHMLg6PkFuB/YRG4kfyZeAmM/uSmeWE9zvdzB4Ob38bmGJmM8wshdCovzceBD4PnA/8ocf6/wO+YWajw8fKM7MFR9nHMuC87hG5mc0EzkNz6HIcCnQZDJ4iFL7dn68B/0Xo5OAq4B1gRXgdQCnwPNBEaKT9E3d/idD8+TeBOmAnkA/ceaQDuvvrhE5gXgxsMrM9wH3hWnD39cDd4eNsAF470n6O4CFCUyQvuntdj/X3AAsJTRM1Am8CZx6ltlfCfwePhts+Bvy3uz/byxokRplecCEiEh00QhcRiRIKdBGRKKFAFxGJEgp0EZEo8Xd3qZ0qubm5XlJSEtThRUQi0vLly+vcPe9I2wIL9JKSEsrKyoI6vIhIRDKzLUfbpikXEZEooUAXEYkSCnQRkSihQBcRiRIKdBGRKKFAFxGJEgp0EZEoEXGBvqxyD9965l30lEgRkUNFXKC/va2en768kYb9HUGXIiIyqERcoGenJQGwp6U94EpERAaXyA305raAKxERGVwiLtBz0pIB2N2kEbqISE8RF+hZaYkA7GlWoIuI9BRxgd49QtccuojIoSIu0FOT4klNjGePplxERA4RcYEOoROjmnIRETlU5Aa6plxERA4RuYGuEbqIyCF6FehmNt/M1plZhZndcZQ215nZGjMrN7MH+7fMQ2WnJemyRRGRwxz3naJmFg/cC7wHqAKWmdlCd1/To00pcCdwrrvvNbP8gSoYQoG+V1MuIiKH6M0IfS5Q4e6b3L0deBhYcFibTwL3uvteAHff1b9lHio7LYmW9k5aD3QO5GFERCJKbwK9ENjWY7kqvK6nCcAEM1tsZm+a2fz+KvBIum//3615dBGRg/rrpGgCUApcCNwA/MzMhh3eyMxuNbMyMyurra096YN1B/peBbqIyEG9CfRqYFSP5aLwup6qgIXufsDdNwPrCQX8Idz9Pnef4+5z8vLyTrZmcsKBXtukB3SJiHTrTaAvA0rNbIyZJQHXAwsPa/MnQqNzzCyX0BTMpv4r81AjMlMAqNnXOlCHEBGJOMcNdHfvAG4DFgFrgUfcvdzM7jazq8PNFgG7zWwN8BLwJXffPVBF56enYAY7FOgiIgcd97JFAHd/CnjqsHV39fjagS+GPwMuKSGOnLRkahoU6CIi3SLyTlGAgswUjdBFRHqI2EAfkZnCTgW6iMhBERvooRH6/qDLEBEZNCI20EdkptDQ2kFLe0fQpYiIDAqRG+gZoUsXNe0iIhISuYGeqUAXEekpYgO9IDMV0LXoIiLdIjbQu6dcdGJURCQkYgM9NSmevPRktuxuCboUEZFBIWIDHWBMTpoCXUQkLKIDfXTOEDbvbg66DBGRQSGiA70kN43axjaa23QtuohIZAd6ThqApl1ERIjwQB+dMwSASk27iIhEdqCX5IZG6Ap0EZEID/ShyQnkDk2msk6BLiIS0YEOMDY3jYpdTUGXISISuIgP9NMK0llf00RXlwddiohIoCI/0Edk0NTWQXW9HgEgIrEt4gN9UkE6AGt3NARciYhIsCI+0CcMT8cM3t3ZGHQpIiKBivhAT0tOYHT2EN7dqRG6iMS2iA90CM2jr92hEbqIxLaoCPRpRZlsrmumvqU96FJERAITFYE+qzgLgLe21gdbiIhIgKIi0GeMGkZ8nLF8y96gSxERCUyvAt3M5pvZOjOrMLM7jrD9JjOrNbOV4c8n+r/Uo0tNimfKyAwFuojEtOMGupnFA/cCVwCTgRvMbPIRmv7e3WeEP/f3c53HNas4i5Xb6uno7DrVhxYRGRR6M0KfC1S4+yZ3bwceBhYMbFkn7oySbPYf6OTtqn1BlyIiEojeBHohsK3HclV43eE+aGarzOxRMxt1pB2Z2a1mVmZmZbW1tSdR7tHNG59LnMEr63b1635FRCJFf50U/TNQ4u6nA88BvzpSI3e/z93nuPucvLy8fjp0SOaQRGaPzuJFBbqIxKjeBHo10HPEXRRed5C773b3tvDi/cDs/invxFx0Wj6rqxvY1dAaxOFFRALVm0BfBpSa2RgzSwKuBxb2bGBmBT0WrwbW9l+JvXfRxHwAXnhXo3QRiT3HDXR37wBuAxYRCupH3L3czO42s6vDzT5nZuVm9jbwOeCmgSr4WE4bkU5JzhCeXLU9iMOLiAQqoTeN3P0p4KnD1t3V4+s7gTv7t7QTZ2ZcdfpIfvJyBbWNbeSlJwddkojIKRMVd4r29L7pI+lyeHr1jqBLERE5paIu0CeOSGfi8HQef6v6+I1FRKJI1AU6wAdnF/LW1no21url0SISO6Iy0N8/o5D4OOOx5VVBlyIicspEZaDnZ6Rwfmkuf1xRTWeXB12OiMgpEZWBDnDt7FHsbGhlcUVd0KWIiJwSURvol0zKJzM1kT9o2kVEYkTUBnpKYjwfmFnIovKdejWdiMSEqA10gOvmjKK9o4s/6RJGEYkBUR3ok0dmMK0wk9+XVeGuk6MiEt2iOtABrptTxNodDayubgi6FBGRARX1gX71jEKSE+J4pGzb8RuLiESwqA/0zNRErpxWwONvVdPYeiDockREBkzUBzrATeeU0NTWwSNluoRRRKJXTAT69FHDmDM6iwde36w7R0UkasVEoAPcMm8M2/bs57k1NUGXIiIyIGIm0C+bMoKirFR+8drmoEsRERkQMRPo8XHGTeeUsLRyDyu31QddjohIv4uZQAf40BmjyBqSyP8+uy7oUkRE+l1MBXp6SiKfuWg8r26o4/WNegqjiESXmAp0gI+eNZqCzBS+/cw6PQ5ARKJKzAV6SmI8/3LpBFZuq+exFXpol4hEj5gLdIBrZxcxZ3QWX39yDXVNbUGXIyLSL2Iy0OPijG9+cBot7R188+l3gy5HRKRfxGSgA4zPT+cT543l0eVVLN+yN+hyRET6LGYDHeC2i8YzIiOFu55YrUcCiEjE61Wgm9l8M1tnZhVmdscx2n3QzNzM5vRfiQMnLTmBf3/vJMq3N/Dg0q1BlyMi0ifHDXQziwfuBa4AJgM3mNnkI7RLBz4PLOnvIgfSVacXcPbYHL67aB17mvXuURGJXL0Zoc8FKtx9k7u3Aw8DC47Q7uvAt4DWfqxvwJkZ/7lgCk1tHXxnke4gFZHI1ZtALwR6vu6nKrzuIDObBYxy978ca0dmdquZlZlZWW1t7QkXO1AmDE/npnNKeHjZVlZV1QddjojISenzSVEziwO+B9x+vLbufp+7z3H3OXl5eX09dL/6wqWl5A5N5q4nyunSCVIRiUC9CfRqYFSP5aLwum7pwFTgZTOrBM4CFkbKidFu6SmJfPnyiazcVs+fVuoOUhGJPL0J9GVAqZmNMbMk4HpgYfdGd9/n7rnuXuLuJcCbwNXuXjYgFQ+gD84qYvqoYXzz6XdpausIuhwRkRNy3EB39w7gNmARsBZ4xN3LzexuM7t6oAs8leLijP9432R2NbZx70sVQZcjInJCEnrTyN2fAp46bN1dR2l7Yd/LCs6s4iyumVnIz1/dzPVnjGJ0TlrQJYmI9EpM3yl6NF+54jQS4o3/WFiuR+yKSMRQoB/B8IwUvnT5RF5eV8sfllcFXY6ISK8o0I/i42eXMHdMNl//8xq21+8PuhwRkeNSoB9FXJzx3Wun0+nOVx5bpakXERn0FOjHUJwzhDuvOI1XN9Tx0NJtx/8GEZEAKdCP4yNnjmbe+FzufrKcdTsbgy5HROSoFOjHERdnfO9D0xmanMinf7ecZt1wJCKDlAK9F/LTU/jh9TPYVNfMV/+0WvPpIjIoKdB76ZzxuXz+klIef6uaX7+xJehyRET+jgL9BHzu4lIunZTP3U+u4fWNdUGXIyJyCAX6CYiLM77/oRmMyU3jM79bwbY9LUGXJCJykAL9BKWnJPKzG+fQ2eV88tdlOkkqIoOGAv0kjMlN40cfnsX6mka+rJuORGSQUKCfpAsm5HH7ZRP5y6odPLZCL8QQkeAp0PvgUxeMY25JNl9bWK75dBEJnAK9D+LjjP+9bjoAX3xkJZ16F6mIBEiB3kejsodw94IpLKvcy3efXRd0OSISwxTo/eCaWUV8+MxifvryRp7QC6ZFJCAK9H7ytfdNYW5JNl9+dBVLNu0OuhwRiUEK9H6SlBDH/31sNkVZqdzyqzLeqdoXdEkiEmMU6P0oOy2J337iTDJTE/n4L5dSsasp6JJEJIYo0PtZQWYqv/3EmcSZ8dH7l+hyRhE5ZRToA2BMbhq/uWUuLe0dfOznS9jV2Bp0SSISAxToA2RSQQa/vHkuNQ1t3PjzpexrORB0SSIS5RToA2j26Czuu3E2m2qbufmBpXqQl4gMKAX6ADuvNI8f3jCTldvq+affLKetozPokkQkSvUq0M1svpmtM7MKM7vjCNs/ZWbvmNlKM3vNzCb3f6mRa/7UEXz72um8VlHH5x56i47OrqBLEpEodNxAN7N44F7gCmAycMMRAvtBd5/m7jOAbwPf6+9CI921s4v4j/dNZlF5DV957B269NwXEelnCb1oMxeocPdNAGb2MLAAWNPdwN0berRPA5RWR3DzuWNo2N/B959fT3JiHP+1YCpxcRZ0WSISJXoT6IXAth7LVcCZhzcys88AXwSSgIuPtCMzuxW4FaC4uPhEa40Kn7tkPG0dnfzk5Y0Y8F/vn4qZQl1E+q7fToq6+73uPg74CvDVo7S5z93nuPucvLy8/jp0RDEzvnT5RP75wnH8bslWvrNIT2gUkf7RmxF6NTCqx3JReN3RPAz8tC9FRTsz48uXT2Tf/gP85OWN5A5N5h/njQm6LBGJcL0ZoS8DSs1sjJklAdcDC3s2MLPSHovvBTb0X4nRycz4+oKpzJ8ygrufXKPH7opInx030N29A7gNWASsBR5x93Izu9vMrg43u83Mys1sJaF59I8PVMHRJD7O+MH1MzhzTDa3P/I2v3q9Ule/iMhJs6DeWD9nzhwvKysL5NiDTWPrAT770Fu8vK6Wc8fn8MPrZ5IzNDnoskRkEDKz5e4+50jbdKfoIJCeksgvbzqD/7lmGmWVe7n6x4sp367nqYvIiVGgDxJmxg1zi3n0U+fQ2eVc+9M3eGb1zqDLEpEIokAfZKYVZbLwtnOZOCKdT/12OT9+cQNBTYuJSGRRoA9C+RkpPHzrWXxgZiHffXY9t/yqTC/KEJHjUqAPUimJ8Xzvuul89b2TWLp5D1f96DVeXrcr6LJEZBBToA9iZsYnzhvLXz43j4LMFG5+YBn3vlShSxtF5IgU6BFgdE4af/z0Obzv9JF8Z9E6vv6XNZpXF5G/05tb/2UQGJKUwD3XzyA7LYlfLq5kc10z55XmcXpRJnNGZ+kBXyKiQI8kZsZdV02mIDOF+1/bzMvragG4bPJwvvGBaeSl62YkkVimO0UjlLtT19TOYyuq+N5z68lISeT3/3QW4/KGBl2aiAwg3SkahcyMvPRkPnXBOBbedi7gfORnS6jaq8sbRWKVAj0KnDYig9/ccibN7R18/BdL2dvcHnRJIhIABXqUmFSQwf03zmHb3v3c/MAyhbpIDFKgR5Ezx+bw4xtmsmZ7A+/5/ivc8/wGahpagy5LRE4RBXqUuWzKCP746XOYMjKT7z+/nnO/+SKf/t1yNtU2BV2aiAwwXeUSxSrrmnlw6VYeWrKVts4uvnBpKZ88byyJ8fo9LhKpdJVLjCrJTePfrpzEC7dfwCWn5fPtZ9ax4MeLeadKz1oXiUYK9BiQn5HCTz86m//76Gzqmtr4wE8W8+s3KvX4AJEoo0CPIfOnjuC5f7mA8yfkcdcT5dz+yNs0tB4IuiwR6ScK9BiTOSSR+2+cw79cOoHHV1Zz9n+/wP88vVaXOYpEAQV6DIqLMz5/aSl/vm0el0wazn1/3cT5336JH72wgZb2jqDLE5GTpKtchHU7G/nus+t4bk0NOWlJnDUuh1nFWfzDnCIyUhKDLk9EejjWVS4KdDlo+ZY9/GJxJauq6tm2Zz8ZKQl88ryx/OO8MaQl68GcIoOBAl1O2Orqffzg+fU8v3YXOWlJ3HxuCR89azTDhiQFXZpITFOgy0lbsXUv9zy/gVfW1zIkKZ7rzyjmE+eNYeSw1KBLE4lJCnTps7U7GvjZXzex8O3txJnx4TOL+fSF48jPSAm6NJGY0udAN7P5wD1APHC/u3/zsO1fBD4BdAC1wD+6+5Zj7VOBHpmq6/fzoxc28IflVRhw6aThfOrCccwYNSzo0kRiQp8C3czigfXAe4AqYBlwg7uv6dHmImCJu7eY2T8DF7r7h461XwV6ZNuyu5nfLdnKH8q2sbflAJdNDgX7rOKsoEsTiWp9fZbLXKDC3Te5ezvwMLCgZwN3f8ndu1+V8yZQ1JeCZfAbnRN6TsyrX7mY298zgTc27eaan7zOgnsX88TKaloPdAZdokjM6c21aIXAth7LVcCZx2h/C/D0kTaY2a3ArQDFxcW9LFEGs6HJCXz2klJunjeGx5ZX8cDrlXz+4ZWkJsZzwYQ8Lp86nItPG05mqq5nFxlo/XpxsZl9FJgDXHCk7e5+H3AfhKZc+vPYEqyhyQl8/JwSPnbWaN7YtJunV+/g2fIaninfSUKccfa4HC6bMoLLJw/XiVSRAdKbOfSzga+5++Xh5TsB3P1/Dmt3KfAj4AJ333W8A2sOPfp1dTkrq+pZVL6TZ8tr2FzXjBnMGZ3F/KkFXDF1hC5/FDlBfT0pmkDopOglQDWhk6IfdvfyHm1mAo8C8919Q2+KUqDHFndnw64mnn5nJ0+v3sG7Oxsxg8smD+eT541l9ugszCzoMkUGvf64bPFK4AeELlv8hbt/w8zuBsrcfaGZPQ9MA3aEv2Wru199rH0q0GPb5rpmHl2+jd++uZV9+w8wfdQwPnbWaC6dlK+7UUWOQTcWyaDV0t7BY8ur+Plrm6nc3UJ8nDFj1DDG5qbxsbNHc3rRsKBLFBlUFOgy6HV1Oauq9/Fs+U6WbN7D+ppGGls7mD9lBP96+QTG56cHXaLIoHCsQNcj9GRQiAuPzLvvOG1sPcDPX9vM/a9u5tk1O7n4tHyumVXExaflk5IYH2yxIoOURugyqO1pbuf+Vzfx2IoqahrayEhJ4L2nF3DppOFMGZnJ8IxknUyVmKIpF4l4nV3O6xvreHxFNc+U76SlPXQnampiPFNGZrBgZiFXTSsgK00nVCW6KdAlqrS0d1C+vYG1OxqorGvhtYpa1tc0kRhvXDgxnw/MLNTUjEQtzaFLVBmSlMAZJdmcUZINhK5xX7OjgT+9Vc0TK7fz3Joa0lMSuGhiPpdOHs5FE/NI16v0JAZohC5RpXtqZuHK7bz47i52N7eTkhjHe6eN5B/mFDF7dBaJ8Xo3ukQujdAlZsTHGeeV5nFeaR6dXc7KbXt5bEU1C1du57EVVaQmxjOzeBhnjslh/tQRTByhyyElemiELjGhpb2Dl9fVsnTzHpZu3sPanQ24w9i8NM4bn8ulk4dzRkm25t1l0NNJUZHD1Da28czqHTy3dhfLNu9h/4FOkuLjmFE8jHnjc5lXmsvphZkkaHpGBhkFusgxtB7oZHFFHW9u2s0bm3azuroBgPSUBM4Zl8O80jzmjc9lTG5awJWKaA5d5JhSEuO5ZNJwLpk0HAjdzLS4oo7XNtTxWkUdi8prACjNH8pFp+VzRkk2Z4/LYWiy/vnI4KIRusgxuDuVu1t4Zd0uninfyYot9bR3dpEYb8wqzmJmcRYZqQlMKshgeHoKZqH5+py0ZEo0opcBoCkXkX7S1tHJ8i17eWV9LYsr6nh3RyMdXUf+N3ThxDw+e3Eps0frxdnSfxToIgPkQGcXTa0dbKproqahDQNSk+J5p2ofv3y9kj3N7YzPH8rs4izG5KVRkjOE4RkppKckkJqUQE5akq6skROiQBcJQEt7B38oq+L5tTWs3dFIXVPb37WJM1gwo5DPXjyesXlDA6hSIo0CXWQQaGg9wNbdLdQ0tNLc3sn+9g7W7mjk4WVbaT3QxYUT87huziiKs4eQn55MztBk4uP0JEk5lAJdZBCrbWzjoaVb+c2bW6ht/NsoPs5gdE4aUwszmV6UyVljc5hckIEZemRwDFOgi0SA9o4uyrfvo6ahjdrGVmoa2tiwq5F3qvaxfV8rAGaQEGfMG5/L7ZdNZGphZsBVy6mm69BFIkBSQhwzi498RczOfa28samOTbXNtLR38tiKKq760WtcdXoBH5xVxKziLDKH6ImSsU4jdJEItG//Ae7760YeWFxJc/hlH4XDUjlrbA7nT8jlvNI8svWyj6ikKReRKNV6oJOyyr2s3r6Pd6r2sXhjHfUtBzCDyQUZTB81jNMLM5k7JpsxuWmae48CCnSRGNHZ5ayqquev6+tYsnk371Tvo7G1A4DhGcnMHp1FaX46o7KHUJw9hCkjM0jTIwwiiubQRWJEfJwxM/xIAijF3dlU18ySTXt4fWMd71Tv4+nVO+kex8UZjM8fSnpKIiMyUjh3fC7njs+hOHuIRvMRSCN0kRjTeqCTmoZWNtY28fa2fayqqmdvywF27NtPTUPossmirFTOHZfLBRPzGJObRlFWql7jN0hoykVEjsvd2VjbzOsb61hcUcfrG3cfnK4BSEuKZ1T2EGYWD6NwWCqTR2ZwzrhcPbrgFOvzlIuZzQfuAeKB+939m4dtPx/4AXA6cL27P9qnikXklDMzxucPZXz+UG48u4T2ji7W7mhg294Wqvbup6ahlQ01TTxbXsPu5nYgNGUzKnsIY3PTGJM7lI6uLnKHJvPe0wsYp0cZnHLHHaGbWTywHngPUAUsA25w9zU92pQAGcC/Agt7E+gaoYtErv3tnSyr3ENZ5R421jWzcVcTm+uaSYyPo6mtgziDiybmM3JYKkkJcSQlxDFxeDrnT9DllH3V1xH6XKDC3TeFd/YwsAA4GOjuXhne1tXnakVk0EtNiuf8CXmcPyHv4Dp3x8yoa2rjZ3/dxHNrali+dS8HOrpo6+g6+Jjh7LQkRmWFpmzOKMnmjJJsirJSdRK2H/Qm0AuBbT2Wq4AzT+ZgZnYrcCtAcXHxyexCRAap7kDOHZrMnVdO4s4rJx3c1tHZxdodjbyyfhfV9a1s3dPMk6t28NDSULSMyEjhjDHZTC/KJHdoMiOHpTKtMJPUJM3Pn4hTetmiu98H3AehKZdTeWwRCU5CfBzTijKZVvS3Z890djnraxpZVrmHZZV7WbZ5D39+e/vfvifOmFqYyZzRWeRnJNPlMD5vKHNKshg2RNM2R9KbQK8GRvVYLgqvExE5afFxxqSCDCYVZHDj2SW4O/UtB9jT0k5lXTNlW/ZSVrmH37y5hbaOv83mmsHMUcM4rzSPsXlppKckkJGSyOSRGQxJiu1ba3rT+2VAqZmNIRTk1wMfHtCqRCTmmBlZaUlkpSUxLm/owZd2d78VKj7eeHdHI4sr6nh53S7ueWHDId8fH2dMLshgVvEwZo3OYsrITMbkphEfZwfn96Ndr65DN7MrCV2WGA/8wt2/YWZ3A2XuvtDMzgAeB7KAVmCnu0851j51lYuI9MX+9k6q61tobuuktrGNldvqWb5lL29X1dMSfmBZckIcGamJ7G1uJzstibljsnnP5OHMGDUsYu+G1Y1FIhIzOjq7WF/TxNodDazd0UBjawfZQ5PYUb+fVzfUHbyGPiMlgdLh6WzZ3UJyQhx56cnkpydz/oQ8LpiQR+GwVOIG4Ruj9CwXEYkZCfFxTB6ZweSRGX+3rbPLWbO9gdXb97Gqah8ba5s4e1wOiXFGbVMb62oaeXZNDQApiXGMzR3KyGEpZKQkMrUwk/Mn5DEqO5XkhMF59Y1G6CIiYe7O2h2NrKqqZ8OuJip2NVHb2Mae5nZ2NrQebJeWFE9WWhI54Tn/YamJdDl0ujO9KJNzxuUyuSBjQEb4GqGLiPSCmR11dL91dwtvbtpNbVMo4Pc2t7OnJfTnptpm4uOMzi7nL6t2ADBsSCLTCjPJTE1kUkEGUwszmToyg5yhyQNWvwJdRKQXinOGUJwz5LjtahpaDz7cbMOuJip3h26i6jYyM4WvXHEaC2YU9nuNCnQRkX40PCOFa2YVcc2sooPr9u0/QPn2fayu3sfq6gby0gdmlK5AFxEZYJmpiZwzLpdzxuUO6HHiBnTvIiJyyijQRUSihAJdRCRKKNBFRKKEAl1EJEoo0EVEooQCXUQkSijQRUSiRGAP5zKzWmDLSX57LlDXj+VEAvU5NqjPsaEvfR7t7nlH2hBYoPeFmZUd7Wlj0Up9jg3qc2wYqD5rykVEJEoo0EVEokSkBvp9QRcQAPU5NqjPsWFA+hyRc+giIvL3InWELiIih1Ggi4hEiYgLdDObb2brzKzCzO4Iup7+Yma/MLNdZra6x7psM3vOzDaE/8wKrzcz+2H472CVmc0KrvKTZ2ajzOwlM1tjZuVm9vnw+qjtt5mlmNlSM3s73Of/DK8fY2ZLwn37vZklhdcnh5crwttLAu3ASTKzeDN7y8yeDC9HdX8BzKzSzN4xs5VmVhZeN6A/2xEV6GYWD9wLXAFMBm4ws8nBVtVvHgDmH7buDuAFdy8FXggvQ6j/peHPrcBPT1GN/a0DuN3dJwNnAZ8J//eM5n63ARe7+3RgBjDfzM4CvgV8393HA3uBW8LtbwH2htd/P9wuEn0eWNtjOdr72+0id5/R45rzgf3ZdveI+QBnA4t6LN8J3Bl0Xf3YvxJgdY/ldUBB+OsCYF346/8H3HCkdpH8AZ4A3hMr/QaGACuAMwndNZgQXn/w5xxYBJwd/joh3M6Crv0E+1kUDq+LgScBi+b+9uh3JZB72LoB/dmOqBE6UAhs67FcFV4XrYa7e/frwncCw8NfR93fQ/h/rWcCS4jyfoenH1YCu4DngI1Avbt3hJv07NfBPoe37wNyTmnBffcD4MtAV3g5h+jubzcHnjWz5WZ2a3jdgP5s6yXREcLd3cyi8hpTMxsKPAZ8wd0bzOzgtmjst7t3AjPMbBjwOHBasBUNHDO7Ctjl7svN7MKAyznV5rl7tZnlA8+Z2bs9Nw7Ez3akjdCrgVE9lovC66JVjZkVAIT/3BVeHzV/D2aWSCjMf+fufwyvjvp+A7h7PfASoSmHYWbWPcDq2a+DfQ5vzwR2n9pK++Rc4GozqwQeJjTtcg/R29+D3L06/OcuQr+45zLAP9uRFujLgNLwGfIk4HpgYcA1DaSFwMfDX3+c0Bxz9/obw2fGzwL29fjfuIhhoaH4z4G17v69Hpuitt9mlhcemWNmqYTOGawlFOzXhpsd3ufuv4trgRc9PMkaCdz9TncvcvcSQv9eX3T3jxCl/e1mZmlmlt79NXAZsJqB/tkO+sTBSZxouBJYT2je8d+Drqcf+/UQsAM4QGj+7BZCc4cvABuA54HscFsjdLXPRuAdYE7Q9Z9kn+cRmmdcBawMf66M5n4DpwNvhfu8GrgrvH4ssBSoAP4AJIfXp4SXK8Lbxwbdhz70/ULgyVjob7h/b4c/5d1ZNdA/27r1X0QkSkTalIuIiByFAl1EJEoo0EVEooQCXUQkSijQRUSihAJdRCRKKNBFRKLE/wdaPbQTz6zM4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 8')\n",
    "plt.plot(range(len(mlpClassifier8.loss_curve_)),mlpClassifier8.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4130eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix8 = metrics.confusion_matrix(y_test,y_pred8)\n",
    "sns.heatmap(pd.DataFrame(confMatrix8),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a27fed7",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c158b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63837118\n",
      "Iteration 2, loss = 0.61485584\n",
      "Iteration 3, loss = 0.59497575\n",
      "Iteration 4, loss = 0.57803542\n",
      "Iteration 5, loss = 0.56274258\n",
      "Iteration 6, loss = 0.54978820\n",
      "Iteration 7, loss = 0.53810396\n",
      "Iteration 8, loss = 0.52796557\n",
      "Iteration 9, loss = 0.51869406\n",
      "Iteration 10, loss = 0.51085741\n",
      "Iteration 11, loss = 0.50326898\n",
      "Iteration 12, loss = 0.49699277\n",
      "Iteration 13, loss = 0.49121515\n",
      "Iteration 14, loss = 0.48571616\n",
      "Iteration 15, loss = 0.48095659\n",
      "Iteration 16, loss = 0.47601943\n",
      "Iteration 17, loss = 0.47152075\n",
      "Iteration 18, loss = 0.46690501\n",
      "Iteration 19, loss = 0.46223107\n",
      "Iteration 20, loss = 0.45757267\n",
      "Iteration 21, loss = 0.45317530\n",
      "Iteration 22, loss = 0.44876030\n",
      "Iteration 23, loss = 0.44461829\n",
      "Iteration 24, loss = 0.44063107\n",
      "Iteration 25, loss = 0.43656671\n",
      "Iteration 26, loss = 0.43287694\n",
      "Iteration 27, loss = 0.42951171\n",
      "Iteration 28, loss = 0.42600997\n",
      "Iteration 29, loss = 0.42297505\n",
      "Iteration 30, loss = 0.42038960\n",
      "Iteration 31, loss = 0.41749683\n",
      "Iteration 32, loss = 0.41463263\n",
      "Iteration 33, loss = 0.41182889\n",
      "Iteration 34, loss = 0.40883069\n",
      "Iteration 35, loss = 0.40573625\n",
      "Iteration 36, loss = 0.40260144\n",
      "Iteration 37, loss = 0.39972128\n",
      "Iteration 38, loss = 0.39691889\n",
      "Iteration 39, loss = 0.39383087\n",
      "Iteration 40, loss = 0.39076203\n",
      "Iteration 41, loss = 0.38761891\n",
      "Iteration 42, loss = 0.38473826\n",
      "Iteration 43, loss = 0.38185499\n",
      "Iteration 44, loss = 0.37915177\n",
      "Iteration 45, loss = 0.37647897\n",
      "Iteration 46, loss = 0.37399295\n",
      "Iteration 47, loss = 0.37140080\n",
      "Iteration 48, loss = 0.36884047\n",
      "Iteration 49, loss = 0.36641080\n",
      "Iteration 50, loss = 0.36411334\n",
      "Iteration 51, loss = 0.36193315\n",
      "Iteration 52, loss = 0.35953565\n",
      "Iteration 53, loss = 0.35745664\n",
      "Iteration 54, loss = 0.35578999\n",
      "Iteration 55, loss = 0.35364320\n",
      "Iteration 56, loss = 0.35152241\n",
      "Iteration 57, loss = 0.34928765\n",
      "Iteration 58, loss = 0.34663205\n",
      "Iteration 59, loss = 0.34403799\n",
      "Iteration 60, loss = 0.34131950\n",
      "Iteration 61, loss = 0.33882031\n",
      "Iteration 62, loss = 0.33661328\n",
      "Iteration 63, loss = 0.33437729\n",
      "Iteration 64, loss = 0.33221762\n",
      "Iteration 65, loss = 0.33018670\n",
      "Iteration 66, loss = 0.32830030\n",
      "Iteration 67, loss = 0.32636086\n",
      "Iteration 68, loss = 0.32453965\n",
      "Iteration 69, loss = 0.32271811\n",
      "Iteration 70, loss = 0.32082771\n",
      "Iteration 71, loss = 0.31904043\n",
      "Iteration 72, loss = 0.31722466\n",
      "Iteration 73, loss = 0.31543196\n",
      "Iteration 74, loss = 0.31354161\n",
      "Iteration 75, loss = 0.31162899\n",
      "Iteration 76, loss = 0.30971850\n",
      "Iteration 77, loss = 0.30775799\n",
      "Iteration 78, loss = 0.30573367\n",
      "Iteration 79, loss = 0.30405070\n",
      "Iteration 80, loss = 0.30215154\n",
      "Iteration 81, loss = 0.30053396\n",
      "Iteration 82, loss = 0.29889707\n",
      "Iteration 83, loss = 0.29688561\n",
      "Iteration 84, loss = 0.29495107\n",
      "Iteration 85, loss = 0.29281532\n",
      "Iteration 86, loss = 0.29085711\n",
      "Iteration 87, loss = 0.28894406\n",
      "Iteration 88, loss = 0.28723778\n",
      "Iteration 89, loss = 0.28576991\n",
      "Iteration 90, loss = 0.28477408\n",
      "Iteration 91, loss = 0.28348259\n",
      "Iteration 92, loss = 0.28218014\n",
      "Iteration 93, loss = 0.28065135\n",
      "Iteration 94, loss = 0.27916946\n",
      "Iteration 95, loss = 0.27764211\n",
      "Iteration 96, loss = 0.27633817\n",
      "Iteration 97, loss = 0.27461352\n",
      "Iteration 98, loss = 0.27339812\n",
      "Iteration 99, loss = 0.27196193\n",
      "Iteration 100, loss = 0.27113475\n",
      "Iteration 101, loss = 0.26975882\n",
      "Iteration 102, loss = 0.26872652\n",
      "Iteration 103, loss = 0.26737732\n",
      "Iteration 104, loss = 0.26602899\n",
      "Iteration 105, loss = 0.26422341\n",
      "Iteration 106, loss = 0.26203728\n",
      "Iteration 107, loss = 0.26071053\n",
      "Iteration 108, loss = 0.25990081\n",
      "Iteration 109, loss = 0.25969182\n",
      "Iteration 110, loss = 0.25939583\n",
      "Iteration 111, loss = 0.25900400\n",
      "Iteration 112, loss = 0.25790297\n",
      "Iteration 113, loss = 0.25671754\n",
      "Iteration 114, loss = 0.25549178\n",
      "Iteration 115, loss = 0.25379382\n",
      "Iteration 116, loss = 0.25181705\n",
      "Iteration 117, loss = 0.24983206\n",
      "Iteration 118, loss = 0.24753588\n",
      "Iteration 119, loss = 0.24599062\n",
      "Iteration 120, loss = 0.24450114\n",
      "Iteration 121, loss = 0.24344226\n",
      "Iteration 122, loss = 0.24275976\n",
      "Iteration 123, loss = 0.24205664\n",
      "Iteration 124, loss = 0.24168379\n",
      "Iteration 125, loss = 0.24101028\n",
      "Iteration 126, loss = 0.23982819\n",
      "Iteration 127, loss = 0.23873598\n",
      "Iteration 128, loss = 0.23752856\n",
      "Iteration 129, loss = 0.23645795\n",
      "Iteration 130, loss = 0.23535778\n",
      "Iteration 131, loss = 0.23422613\n",
      "Iteration 132, loss = 0.23308288\n",
      "Iteration 133, loss = 0.23173472\n",
      "Iteration 134, loss = 0.23003522\n",
      "Iteration 135, loss = 0.22882122\n",
      "Iteration 136, loss = 0.22757840\n",
      "Iteration 137, loss = 0.22691900\n",
      "Iteration 138, loss = 0.22601453\n",
      "Iteration 139, loss = 0.22521126\n",
      "Iteration 140, loss = 0.22427024\n",
      "Iteration 141, loss = 0.22327160\n",
      "Iteration 142, loss = 0.22231814\n",
      "Iteration 143, loss = 0.22133592\n",
      "Iteration 144, loss = 0.22046373\n",
      "Iteration 145, loss = 0.22019008\n",
      "Iteration 146, loss = 0.22016362\n",
      "Iteration 147, loss = 0.21943343\n",
      "Iteration 148, loss = 0.21861822\n",
      "Iteration 149, loss = 0.21756926\n",
      "Iteration 150, loss = 0.21678330\n",
      "Iteration 151, loss = 0.21567580\n",
      "Iteration 152, loss = 0.21493955\n",
      "Iteration 153, loss = 0.21416861\n",
      "Iteration 154, loss = 0.21355778\n",
      "Iteration 155, loss = 0.21279314\n",
      "Iteration 156, loss = 0.21176946\n",
      "Iteration 157, loss = 0.21095556\n",
      "Iteration 158, loss = 0.21010195\n",
      "Iteration 159, loss = 0.20911517\n",
      "Iteration 160, loss = 0.20815652\n",
      "Iteration 161, loss = 0.20733879\n",
      "Iteration 162, loss = 0.20667479\n",
      "Iteration 163, loss = 0.20608147\n",
      "Iteration 164, loss = 0.20522109\n",
      "Iteration 165, loss = 0.20418887\n",
      "Iteration 166, loss = 0.20335799\n",
      "Iteration 167, loss = 0.20245956\n",
      "Iteration 168, loss = 0.20164195\n",
      "Iteration 169, loss = 0.20085688\n",
      "Iteration 170, loss = 0.20027193\n",
      "Iteration 171, loss = 0.19945171\n",
      "Iteration 172, loss = 0.19865541\n",
      "Iteration 173, loss = 0.19807010\n",
      "Iteration 174, loss = 0.19741980\n",
      "Iteration 175, loss = 0.19683950\n",
      "Iteration 176, loss = 0.19648748\n",
      "Iteration 177, loss = 0.19604985\n",
      "Iteration 178, loss = 0.19544403\n",
      "Iteration 179, loss = 0.19482734\n",
      "Iteration 180, loss = 0.19419285\n",
      "Iteration 181, loss = 0.19367290\n",
      "Iteration 182, loss = 0.19312518\n",
      "Iteration 183, loss = 0.19257055\n",
      "Iteration 184, loss = 0.19195399\n",
      "Iteration 185, loss = 0.19141265\n",
      "Iteration 186, loss = 0.19100201\n",
      "Iteration 187, loss = 0.19033200\n",
      "Iteration 188, loss = 0.18970842\n",
      "Iteration 189, loss = 0.18890827\n",
      "Iteration 190, loss = 0.18800722\n",
      "Iteration 191, loss = 0.18706076\n",
      "Iteration 192, loss = 0.18629161\n",
      "Iteration 193, loss = 0.18570302\n",
      "Iteration 194, loss = 0.18501385\n",
      "Iteration 195, loss = 0.18447546\n",
      "Iteration 196, loss = 0.18389193\n",
      "Iteration 197, loss = 0.18329934\n",
      "Iteration 198, loss = 0.18280180\n",
      "Iteration 199, loss = 0.18282278\n",
      "Iteration 200, loss = 0.18230436\n",
      "Iteration 201, loss = 0.18137618\n",
      "Iteration 202, loss = 0.18035707\n",
      "Iteration 203, loss = 0.17941706\n",
      "Iteration 204, loss = 0.17845274\n",
      "Iteration 205, loss = 0.17769847\n",
      "Iteration 206, loss = 0.17719356\n",
      "Iteration 207, loss = 0.17640228\n",
      "Iteration 208, loss = 0.17571752\n",
      "Iteration 209, loss = 0.17521561\n",
      "Iteration 210, loss = 0.17454499\n",
      "Iteration 211, loss = 0.17382962\n",
      "Iteration 212, loss = 0.17322395\n",
      "Iteration 213, loss = 0.17280607\n",
      "Iteration 214, loss = 0.17249093\n",
      "Iteration 215, loss = 0.17225148\n",
      "Iteration 216, loss = 0.17215259\n",
      "Iteration 217, loss = 0.17183528\n",
      "Iteration 218, loss = 0.17117850\n",
      "Iteration 219, loss = 0.17038006\n",
      "Iteration 220, loss = 0.16969816\n",
      "Iteration 221, loss = 0.16906800\n",
      "Iteration 222, loss = 0.16826073\n",
      "Iteration 223, loss = 0.16758469\n",
      "Iteration 224, loss = 0.16708670\n",
      "Iteration 225, loss = 0.16650927\n",
      "Iteration 226, loss = 0.16614059\n",
      "Iteration 227, loss = 0.16568293\n",
      "Iteration 228, loss = 0.16528600\n",
      "Iteration 229, loss = 0.16457543\n",
      "Iteration 230, loss = 0.16403037\n",
      "Iteration 231, loss = 0.16333717\n",
      "Iteration 232, loss = 0.16259335\n",
      "Iteration 233, loss = 0.16194779\n",
      "Iteration 234, loss = 0.16128990\n",
      "Iteration 235, loss = 0.16091692\n",
      "Iteration 236, loss = 0.16040983\n",
      "Iteration 237, loss = 0.16019260\n",
      "Iteration 238, loss = 0.15977858\n",
      "Iteration 239, loss = 0.15919252\n",
      "Iteration 240, loss = 0.15851469\n",
      "Iteration 241, loss = 0.15792193\n",
      "Iteration 242, loss = 0.15740308\n",
      "Iteration 243, loss = 0.15701764\n",
      "Iteration 244, loss = 0.15651539\n",
      "Iteration 245, loss = 0.15588903\n",
      "Iteration 246, loss = 0.15538929\n",
      "Iteration 247, loss = 0.15514152\n",
      "Iteration 248, loss = 0.15457999\n",
      "Iteration 249, loss = 0.15412814\n",
      "Iteration 250, loss = 0.15367145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier9 = MLPClassifier(max_iter=250,activation='relu', verbose=True, hidden_layer_sizes=(200,),random_state=1,learning_rate_init=0.001)\n",
    "mlpClassifier9.fit(X_train, y_train)\n",
    "y_pred9 = mlpClassifier9.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de7ada5",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d7a2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.79569892473118"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd86d6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fde36b20a00>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss Curve 9')\n",
    "plt.plot(range(len(mlpClassifier9.loss_curve_)),mlpClassifier9.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b80f7288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confMatrix9 = metrics.confusion_matrix(y_test,y_pred9)\n",
    "sns.heatmap(pd.DataFrame(confMatrix9),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5284bdf",
   "metadata": {},
   "source": [
    "## Creating the MLP Classifire #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0da60c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63936500\n",
      "Iteration 2, loss = 0.63830439\n",
      "Iteration 3, loss = 0.63664177\n",
      "Iteration 4, loss = 0.63441776\n",
      "Iteration 5, loss = 0.63180256\n",
      "Iteration 6, loss = 0.62895949\n",
      "Iteration 7, loss = 0.62593565\n",
      "Iteration 8, loss = 0.62284214\n",
      "Iteration 9, loss = 0.61965378\n",
      "Iteration 10, loss = 0.61643781\n",
      "Iteration 11, loss = 0.61316922\n",
      "Iteration 12, loss = 0.61001294\n",
      "Iteration 13, loss = 0.60699637\n",
      "Iteration 14, loss = 0.60403327\n",
      "Iteration 15, loss = 0.60123308\n",
      "Iteration 16, loss = 0.59841165\n",
      "Iteration 17, loss = 0.59576090\n",
      "Iteration 18, loss = 0.59314862\n",
      "Iteration 19, loss = 0.59060880\n",
      "Iteration 20, loss = 0.58812025\n",
      "Iteration 21, loss = 0.58572072\n",
      "Iteration 22, loss = 0.58339556\n",
      "Iteration 23, loss = 0.58118499\n",
      "Iteration 24, loss = 0.57907363\n",
      "Iteration 25, loss = 0.57707328\n",
      "Iteration 26, loss = 0.57516188\n",
      "Iteration 27, loss = 0.57333258\n",
      "Iteration 28, loss = 0.57155343\n",
      "Iteration 29, loss = 0.56984862\n",
      "Iteration 30, loss = 0.56819187\n",
      "Iteration 31, loss = 0.56649457\n",
      "Iteration 32, loss = 0.56480023\n",
      "Iteration 33, loss = 0.56309689\n",
      "Iteration 34, loss = 0.56136380\n",
      "Iteration 35, loss = 0.55971500\n",
      "Iteration 36, loss = 0.55812371\n",
      "Iteration 37, loss = 0.55665439\n",
      "Iteration 38, loss = 0.55522614\n",
      "Iteration 39, loss = 0.55376070\n",
      "Iteration 40, loss = 0.55228030\n",
      "Iteration 41, loss = 0.55077429\n",
      "Iteration 42, loss = 0.54933578\n",
      "Iteration 43, loss = 0.54790268\n",
      "Iteration 44, loss = 0.54650182\n",
      "Iteration 45, loss = 0.54509659\n",
      "Iteration 46, loss = 0.54370757\n",
      "Iteration 47, loss = 0.54235584\n",
      "Iteration 48, loss = 0.54096876\n",
      "Iteration 49, loss = 0.53962964\n",
      "Iteration 50, loss = 0.53835485\n",
      "Iteration 51, loss = 0.53708868\n",
      "Iteration 52, loss = 0.53588795\n",
      "Iteration 53, loss = 0.53468484\n",
      "Iteration 54, loss = 0.53353646\n",
      "Iteration 55, loss = 0.53237535\n",
      "Iteration 56, loss = 0.53125033\n",
      "Iteration 57, loss = 0.53011570\n",
      "Iteration 58, loss = 0.52899468\n",
      "Iteration 59, loss = 0.52790563\n",
      "Iteration 60, loss = 0.52679447\n",
      "Iteration 61, loss = 0.52571057\n",
      "Iteration 62, loss = 0.52463847\n",
      "Iteration 63, loss = 0.52360035\n",
      "Iteration 64, loss = 0.52257277\n",
      "Iteration 65, loss = 0.52156269\n",
      "Iteration 66, loss = 0.52057648\n",
      "Iteration 67, loss = 0.51959942\n",
      "Iteration 68, loss = 0.51865931\n",
      "Iteration 69, loss = 0.51770072\n",
      "Iteration 70, loss = 0.51676847\n",
      "Iteration 71, loss = 0.51583186\n",
      "Iteration 72, loss = 0.51488615\n",
      "Iteration 73, loss = 0.51398400\n",
      "Iteration 74, loss = 0.51308509\n",
      "Iteration 75, loss = 0.51222954\n",
      "Iteration 76, loss = 0.51137888\n",
      "Iteration 77, loss = 0.51053985\n",
      "Iteration 78, loss = 0.50970991\n",
      "Iteration 79, loss = 0.50890535\n",
      "Iteration 80, loss = 0.50809506\n",
      "Iteration 81, loss = 0.50727820\n",
      "Iteration 82, loss = 0.50645446\n",
      "Iteration 83, loss = 0.50560793\n",
      "Iteration 84, loss = 0.50478750\n",
      "Iteration 85, loss = 0.50395184\n",
      "Iteration 86, loss = 0.50315304\n",
      "Iteration 87, loss = 0.50235169\n",
      "Iteration 88, loss = 0.50157129\n",
      "Iteration 89, loss = 0.50080102\n",
      "Iteration 90, loss = 0.50004374\n",
      "Iteration 91, loss = 0.49930020\n",
      "Iteration 92, loss = 0.49856435\n",
      "Iteration 93, loss = 0.49783282\n",
      "Iteration 94, loss = 0.49709979\n",
      "Iteration 95, loss = 0.49638711\n",
      "Iteration 96, loss = 0.49567993\n",
      "Iteration 97, loss = 0.49501644\n",
      "Iteration 98, loss = 0.49438146\n",
      "Iteration 99, loss = 0.49376519\n",
      "Iteration 100, loss = 0.49316617\n",
      "Iteration 101, loss = 0.49257033\n",
      "Iteration 102, loss = 0.49198494\n",
      "Iteration 103, loss = 0.49142136\n",
      "Iteration 104, loss = 0.49084325\n",
      "Iteration 105, loss = 0.49023563\n",
      "Iteration 106, loss = 0.48959787\n",
      "Iteration 107, loss = 0.48895209\n",
      "Iteration 108, loss = 0.48833571\n",
      "Iteration 109, loss = 0.48767800\n",
      "Iteration 110, loss = 0.48706123\n",
      "Iteration 111, loss = 0.48642968\n",
      "Iteration 112, loss = 0.48582777\n",
      "Iteration 113, loss = 0.48525043\n",
      "Iteration 114, loss = 0.48470391\n",
      "Iteration 115, loss = 0.48415955\n",
      "Iteration 116, loss = 0.48359286\n",
      "Iteration 117, loss = 0.48304634\n",
      "Iteration 118, loss = 0.48250720\n",
      "Iteration 119, loss = 0.48196745\n",
      "Iteration 120, loss = 0.48143872\n",
      "Iteration 121, loss = 0.48089474\n",
      "Iteration 122, loss = 0.48037474\n",
      "Iteration 123, loss = 0.47985417\n",
      "Iteration 124, loss = 0.47935196\n",
      "Iteration 125, loss = 0.47882971\n",
      "Iteration 126, loss = 0.47830922\n",
      "Iteration 127, loss = 0.47778447\n",
      "Iteration 128, loss = 0.47727120\n",
      "Iteration 129, loss = 0.47675806\n",
      "Iteration 130, loss = 0.47624759\n",
      "Iteration 131, loss = 0.47574464\n",
      "Iteration 132, loss = 0.47523953\n",
      "Iteration 133, loss = 0.47473864\n",
      "Iteration 134, loss = 0.47424651\n",
      "Iteration 135, loss = 0.47375891\n",
      "Iteration 136, loss = 0.47325327\n",
      "Iteration 137, loss = 0.47274450\n",
      "Iteration 138, loss = 0.47225012\n",
      "Iteration 139, loss = 0.47175171\n",
      "Iteration 140, loss = 0.47127434\n",
      "Iteration 141, loss = 0.47077741\n",
      "Iteration 142, loss = 0.47028871\n",
      "Iteration 143, loss = 0.46979068\n",
      "Iteration 144, loss = 0.46934130\n",
      "Iteration 145, loss = 0.46891678\n",
      "Iteration 146, loss = 0.46845257\n",
      "Iteration 147, loss = 0.46795096\n",
      "Iteration 148, loss = 0.46746713\n",
      "Iteration 149, loss = 0.46697752\n",
      "Iteration 150, loss = 0.46648290\n",
      "Iteration 151, loss = 0.46598305\n",
      "Iteration 152, loss = 0.46550363\n",
      "Iteration 153, loss = 0.46502377\n",
      "Iteration 154, loss = 0.46456391\n",
      "Iteration 155, loss = 0.46410907\n",
      "Iteration 156, loss = 0.46363895\n",
      "Iteration 157, loss = 0.46318264\n",
      "Iteration 158, loss = 0.46271766\n",
      "Iteration 159, loss = 0.46223930\n",
      "Iteration 160, loss = 0.46178185\n",
      "Iteration 161, loss = 0.46130801\n",
      "Iteration 162, loss = 0.46085128\n",
      "Iteration 163, loss = 0.46040906\n",
      "Iteration 164, loss = 0.45995244\n",
      "Iteration 165, loss = 0.45951052\n",
      "Iteration 166, loss = 0.45908894\n",
      "Iteration 167, loss = 0.45867384\n",
      "Iteration 168, loss = 0.45823988\n",
      "Iteration 169, loss = 0.45781000\n",
      "Iteration 170, loss = 0.45737766\n",
      "Iteration 171, loss = 0.45693942\n",
      "Iteration 172, loss = 0.45650749\n",
      "Iteration 173, loss = 0.45607119\n",
      "Iteration 174, loss = 0.45564262\n",
      "Iteration 175, loss = 0.45521691\n",
      "Iteration 176, loss = 0.45481110\n",
      "Iteration 177, loss = 0.45439543\n",
      "Iteration 178, loss = 0.45397634\n",
      "Iteration 179, loss = 0.45357138\n",
      "Iteration 180, loss = 0.45317441\n",
      "Iteration 181, loss = 0.45278056\n",
      "Iteration 182, loss = 0.45238880\n",
      "Iteration 183, loss = 0.45199253\n",
      "Iteration 184, loss = 0.45159771\n",
      "Iteration 185, loss = 0.45120919\n",
      "Iteration 186, loss = 0.45082709\n",
      "Iteration 187, loss = 0.45043135\n",
      "Iteration 188, loss = 0.45002982\n",
      "Iteration 189, loss = 0.44960878\n",
      "Iteration 190, loss = 0.44921075\n",
      "Iteration 191, loss = 0.44879648\n",
      "Iteration 192, loss = 0.44839427\n",
      "Iteration 193, loss = 0.44799702\n",
      "Iteration 194, loss = 0.44758313\n",
      "Iteration 195, loss = 0.44719811\n",
      "Iteration 196, loss = 0.44679385\n",
      "Iteration 197, loss = 0.44640811\n",
      "Iteration 198, loss = 0.44599283\n",
      "Iteration 199, loss = 0.44561563\n",
      "Iteration 200, loss = 0.44518959\n",
      "Iteration 201, loss = 0.44479027\n",
      "Iteration 202, loss = 0.44443038\n",
      "Iteration 203, loss = 0.44403665\n",
      "Iteration 204, loss = 0.44364743\n",
      "Iteration 205, loss = 0.44328070\n",
      "Iteration 206, loss = 0.44292107\n",
      "Iteration 207, loss = 0.44253879\n",
      "Iteration 208, loss = 0.44218481\n",
      "Iteration 209, loss = 0.44181021\n",
      "Iteration 210, loss = 0.44144786\n",
      "Iteration 211, loss = 0.44109368\n",
      "Iteration 212, loss = 0.44075336\n",
      "Iteration 213, loss = 0.44040807\n",
      "Iteration 214, loss = 0.44007407\n",
      "Iteration 215, loss = 0.43973005\n",
      "Iteration 216, loss = 0.43938801\n",
      "Iteration 217, loss = 0.43905046\n",
      "Iteration 218, loss = 0.43870580\n",
      "Iteration 219, loss = 0.43838385\n",
      "Iteration 220, loss = 0.43804644\n",
      "Iteration 221, loss = 0.43769999\n",
      "Iteration 222, loss = 0.43733799\n",
      "Iteration 223, loss = 0.43698368\n",
      "Iteration 224, loss = 0.43664257\n",
      "Iteration 225, loss = 0.43628365\n",
      "Iteration 226, loss = 0.43593501\n",
      "Iteration 227, loss = 0.43557338\n",
      "Iteration 228, loss = 0.43521653\n",
      "Iteration 229, loss = 0.43485772\n",
      "Iteration 230, loss = 0.43450911\n",
      "Iteration 231, loss = 0.43416522\n",
      "Iteration 232, loss = 0.43381109\n",
      "Iteration 233, loss = 0.43346324\n",
      "Iteration 234, loss = 0.43312022\n",
      "Iteration 235, loss = 0.43279498\n",
      "Iteration 236, loss = 0.43245826\n",
      "Iteration 237, loss = 0.43213693\n",
      "Iteration 238, loss = 0.43181657\n",
      "Iteration 239, loss = 0.43149660\n",
      "Iteration 240, loss = 0.43116982\n",
      "Iteration 241, loss = 0.43084556\n",
      "Iteration 242, loss = 0.43050418\n",
      "Iteration 243, loss = 0.43015905\n",
      "Iteration 244, loss = 0.42981243\n",
      "Iteration 245, loss = 0.42947313\n",
      "Iteration 246, loss = 0.42912240\n",
      "Iteration 247, loss = 0.42879049\n",
      "Iteration 248, loss = 0.42843485\n",
      "Iteration 249, loss = 0.42809341\n",
      "Iteration 250, loss = 0.42775748\n",
      "Iteration 251, loss = 0.42742605\n",
      "Iteration 252, loss = 0.42709822\n",
      "Iteration 253, loss = 0.42676971\n",
      "Iteration 254, loss = 0.42645295\n",
      "Iteration 255, loss = 0.42612805\n",
      "Iteration 256, loss = 0.42581537\n",
      "Iteration 257, loss = 0.42548911\n",
      "Iteration 258, loss = 0.42518111\n",
      "Iteration 259, loss = 0.42485538\n",
      "Iteration 260, loss = 0.42451649\n",
      "Iteration 261, loss = 0.42417612\n",
      "Iteration 262, loss = 0.42385977\n",
      "Iteration 263, loss = 0.42350527\n",
      "Iteration 264, loss = 0.42316430\n",
      "Iteration 265, loss = 0.42282910\n",
      "Iteration 266, loss = 0.42249423\n",
      "Iteration 267, loss = 0.42216784\n",
      "Iteration 268, loss = 0.42184526\n",
      "Iteration 269, loss = 0.42153297\n",
      "Iteration 270, loss = 0.42121654\n",
      "Iteration 271, loss = 0.42091072\n",
      "Iteration 272, loss = 0.42062511\n",
      "Iteration 273, loss = 0.42032058\n",
      "Iteration 274, loss = 0.42004593\n",
      "Iteration 275, loss = 0.41974890\n",
      "Iteration 276, loss = 0.41943902\n",
      "Iteration 277, loss = 0.41914682\n",
      "Iteration 278, loss = 0.41885194\n",
      "Iteration 279, loss = 0.41854599\n",
      "Iteration 280, loss = 0.41823663\n",
      "Iteration 281, loss = 0.41793271\n",
      "Iteration 282, loss = 0.41761669\n",
      "Iteration 283, loss = 0.41731792\n",
      "Iteration 284, loss = 0.41700674\n",
      "Iteration 285, loss = 0.41670442\n",
      "Iteration 286, loss = 0.41640125\n",
      "Iteration 287, loss = 0.41608871\n",
      "Iteration 288, loss = 0.41576421\n",
      "Iteration 289, loss = 0.41543565\n",
      "Iteration 290, loss = 0.41513110\n",
      "Iteration 291, loss = 0.41484132\n",
      "Iteration 292, loss = 0.41454310\n",
      "Iteration 293, loss = 0.41424769\n",
      "Iteration 294, loss = 0.41400195\n",
      "Iteration 295, loss = 0.41367821\n",
      "Iteration 296, loss = 0.41338537\n",
      "Iteration 297, loss = 0.41309541\n",
      "Iteration 298, loss = 0.41282030\n",
      "Iteration 299, loss = 0.41252592\n",
      "Iteration 300, loss = 0.41226242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlpClassifier10 = MLPClassifier(max_iter=300,activation='relu', verbose=True, hidden_layer_sizes=(200,),random_state=1,learning_rate_init=0.001,solver='sgd')\n",
    "mlpClassifier10.fit(X_train, y_train)\n",
    "y_pred10 = mlpClassifier10.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "353f5aa2",
   "metadata": {},
   "source": [
    "## Analyzing the Classifier #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b048772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.49462365591397"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(y_test, y_pred10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
